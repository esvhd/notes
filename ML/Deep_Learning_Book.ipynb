{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Learning Notes\n",
    "\n",
    "by zwl\n",
    "\n",
    "Creative Commons License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential Formulae\n",
    "\n",
    "Definitions first:\n",
    "\n",
    "### Logistic Sigmoid\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11aeb4400>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkMpdRCQL/X/1v3/KLjr2TgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11adc0748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "n = np.linspace(-10, 10, 50)\n",
    "out = sigmoid(n)\n",
    "\n",
    "plt.plot(n, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softplus\n",
    "\n",
    "$$ \\zeta(x) = \\log(1 + e^x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11abea2b0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VNX9//HXSUIIJOxLAAUBQcSNJShqtRWX1t3WVsXd\nVkWxKrR+W7dW++tibau1WrfaurPEvVr3LWitW0kIa9h3ZIcEkhCSzHx+f8xgY0zI7HeW9/PxmEdm\n7tx75z1nMp/cnHvvuc7MEBGR1JfldQAREYkNFXQRkTShgi4ikiZU0EVE0oQKuohImlBBFxFJEyro\nIiJpQgVdRCRNqKCLiKSJnES+WM+ePW3gwIERLVtTU0N+fn5sA8WAcoVHucKjXOFJ1lwQXbbS0tIt\nZtarzRnNLGG3oqIii1RJSUnEy8aTcoVHucKjXOFJ1lxm0WUDZloINVZdLiIiaUIFXUQkTaigi4ik\nCRV0EZE0oYIuIpIm2izozrnHnHObnHPzmkzr7px7xzm3JPizW3xjiohIW0LZQn8COLnZtJuA98xs\nKPBe8LGIiHiozYJuZh8C25pNPgt4Mnj/SeC7Mc4lIpIW6hp8/OqV+eysj//lPp2FcE1R59xA4FUz\nOyT4uNLMugbvO2D7nsctLDsBmABQWFhYVFxcHFHQ6upqCgoKIlo2npQrPMoVHuUKTzLmenTubj5a\n18g1BxuH948s27hx40rNbEybM4Zy9hEwEJjX5HFls+e3h7IenSmaOMoVHuUKj3KFpvjzVbbfja/a\n3W8tTOozRTc65/oCBH9uinA9IiJpad66Kn758nyOHdqTSScekJDXjLSgvwJcGrx/KfBybOKIiKS+\nql0NXDO1jB75ufzlvJFkZ7mEvG4ohy1OBz4Bhjnn1jrnLgfuBE5yzi0BTgw+FhHJeH6/ccOzs/mi\nchf3XzCaHgXtE/babQ6fa2bnt/LUCTHOIiKS8v724XLerdjI7WccRNF+iT1FR2eKiojEyCfLtvKn\ntxZy2mF9uezogQl/fRV0EZEY2LSjjuumz2JQz3z+8P3DCBzRnVgJvWKRiEg6avD5uXbaLGp2NzLt\nyrEUtPemtKqgi4hE6U9vLeLzldu4d/xIDijs5FkOdbmIiEThzXkbeOTD5Vx85H6cNXIfT7OooIuI\nRGjFlhp+9txsRvTvyi9OH+51HBV0EZFI7Kr3MXFKKdnZjgcuGEX7nGyvI6kPXUQkXGbGL/45j0Ub\nd/L4ZYezb7eOXkcCtIUuIhK24v+u4YWytVx//FCOG9bb6zhfUkEXEQnDvHVV3P5KYNCt608Y6nWc\nr1BBFxEJUVVtA1dPKaVnfi73jh+VsEG3QqU+dBGREPj9xk+fLWfjjjqeveoouufneh3pa7SFLiIS\ngoc+WMZ7Czfxi9MOYtSAxA66FSoVdBGRNny8bAt3v72IM0b045Kj9vM6TqtU0EVE9mJDVR3XT5/F\n4F4F3Hn2oZ4MuhUq9aGLiLQiMOhWGbX1PoonjCbfo0G3QpXc6UREPHTnGwuZuWo7950/iiG9vRt0\nK1TqchERacHrc9fz6EcruOzogZw5op/XcUKigi4i0szyzdX8/Pk5jOzflVtO9X7QrVCpoIuINFFb\n38jEKWXk5mTx4IWjyc1JnTKpPnQRkSAz49aX5rF4006e+tER9OvawetIYUmdPz0iInE29bPVvDRr\nHZNPOIBjh/byOk7YVNBFRIA5ayv59b8W8K0DenHd8UO8jhMRFXQRyXjba+qZOKWMXp3a85fzRpKV\nZINuhUp96CKS0fx+4yfPlrNpZx3PXX003ZJw0K1QaQtdRDLaAyVLmbFoM7edcTAj+3f1Ok5UVNBF\nJGP9e8lm/vzuYr47sh8XjR3gdZyoqaCLSEb6onIXk4rLGdq7gDuSfNCtUKmgi0jGqW/08+NpZexu\n8PHQRUV0zE2P3Ynp8S5ERMJwx+sVzFpdyQMXjGb/XgVex4mZqLbQnXM/cc7Nd87Nc85Nd87lxSqY\niEg8/Gv2Fzzx8Up++I2BnHZYX6/jxFTEBd05tw9wPTDGzA4BsoHxsQomIhJrSzdVc9MLcxg9oCs3\nn5I6g26FKto+9Bygg3MuB+gIfBF9JBGR2KvZ3cjEKaW0b5fNAyk26FaoIn5HZrYOuAtYDawHqszs\n7VgFExGJFTPj5hfnsnRzNfeNH0XfLqk16FaonJlFtqBz3YAXgPOASuA54Hkzm9JsvgnABIDCwsKi\n4uLiiF6vurqagoLk23mhXOFRrvAoV3hay/XuqgamVNRz9tB2nLm/N2eCRtNm48aNKzWzMW3OaGYR\n3YBzgEebPL4EeHBvyxQVFVmkSkpKIl42npQrPMoVHuUKT0u5ylZtsyG3vGaXPfaZ+Xz+xIcKiqbN\ngJkWQl2OphNpNXCkc66jCxyRfwJQEcX6RERialtNPT+eWkZh5zzuSeFBt0IVTR/6Z8DzQBkwN7iu\nR2KUS0QkKj6/MfmZcrZU1/PghaPp2jF1B90KVVQnFpnZ7cDtMcoiIhIzf31/CR8u3swd3zuUw/ZN\n7UG3QpV+x+2ISMb7YPFm7n1vCWeP3ofzj+jvdZyEUUEXkbSyrnIXk4tnMaywE7/7bnoMuhUqFXQR\nSRsNfuOaqWU0+IwHLxxNh9xsryMllAbnEpG0Ubywntlrann4otEMTqNBt0KlLXQRSQsvl6/jvdWN\nXHHMIE4+JL0G3QqVCrqIpLwlG3dy0wtzOaBbFjeecqDXcTyjLhcRSWnVuxu5ekop+e1zmDgii3bZ\nmbudmrnvXERSnplx0wtzWLGlhr+eP4pueZld0jL73YtISnvi45W8Omc9P/vOgRy1fw+v43hOBV1E\nUlLpqu387rUKThxeyNXfGux1nKSggi4iKWdr9W6unVZGv64duPvcERl18tDeaKeoiKQUn9+YVFzO\n1pp6Xpx4NF06tPM6UtLQFrqIpJR7313MR0u38JuzDuaQfbp4HSepqKCLSMooWbiJ+95fyrlj9uW8\nwwd4HSfpqKCLSEpYs62Wyc+Uc1Dfzvz6rEO8jpOUVNBFJOnVNfi4ZmoZfjMeumg0ee0ya9CtUGmn\nqIgkvV+/uoC566p45OIi9uuR73WcpKUtdBFJai+WrWXaZ6u56luD+fbBfbyOk9RU0EUkaS3csINb\nXprL2EHd+dm3h3kdJ+mpoItIUtpZ18DEKWV0zmvHXy8YRU4GD7oVKvWhi0jSMTN+9twcVm+rZdoV\nY+ndKc/rSClBf/JEJOk8+tEK3py/gZtOPpCxgzXoVqhU0EUkqXy+Yhu/f2MhJx/chyuOHeR1nJSi\ngi4iSWPTzjqunVZG/24d+OM5h2nQrTCpD11EkkKjz8/102exo66BJ390BJ3zNOhWuFTQRSQp3P3O\nYj5dvo27zxnB8L6dvY6TktTlIiKee2fBRh6asYwLxg7g+0X7eh0nZamgi4inVm2t4afPlnPoPl24\n7fSDvI6T0lTQRcQzdQ0+rp5SRpZzPHihBt2KlvrQRcQzt708j4r1O3j8ssPp372j13FSnrbQRcQT\nz/x3Nc/OXMt1xw9h3IG9vY6TFqIq6M65rs65551zC51zFc65o2IVTETS17x1Vfzy5fkcM6Qnk088\nwOs4aSPaLpd7gTfN7AfOuVxA/zOJyF5V1TYwcWopPfJzuXf8SLKzdPJQrERc0J1zXYBvApcBmFk9\nUB+bWCKSjvx+44bnytlQVcczVx1Fj4L2XkdKK87MIlvQuZHAI8ACYARQCkwys5pm800AJgAUFhYW\nFRcXR/R61dXVFBQURLRsPClXeJQrPOmW69Vl9Ty/pIELh+dy0n6xPxM0WdsLoss2bty4UjMb0+aM\nZhbRDRgDNAJjg4/vBX6zt2WKioosUiUlJREvG0/KFR7lCk865frPks026KZX7dppZeb3+2MfypK3\nvcyiywbMtBDqcjQ7RdcCa83ss+Dj54HRUaxPRNLUhqo6rps+i8G9Crjz7EM16FacRFzQzWwDsMY5\nt+e6UCcQ6H4REflSfaOfa6aWsqvBx8MXjSa/vU5/iZdoW/Y6YGrwCJflwA+jjyQi6eT3b1RQtrqS\nv54/iiG9O3kdJ61FVdDNrJxAX7qIyNe8OucLHv/PSn74jYGcMaKf13HSns4UFZG4WLppJzc+P4ei\n/bpx8ynDvY6TEVTQRSTmanY3cvWUMvLaZfPABaPJzVGpSQTtnRCRmDIzbn5xLss3VzPl8rH06ZLn\ndaSMoT+bIhJTT32yildmf8EN3x7G0UN6eh0no6igi0jMlK7azm9fW8CJw3sz8Vv7ex0n46igi0hM\nbK3ezY+nltG3SwfuPmckWRp0K+HUhy4iUfP5jeuLZ7G9tp4XrzmaLh1jP06LtE0FXUSids87i/nP\n0q388QeHcXC/Ll7HyVjqchGRqLxXsZH7S5Zy/hH9OXdMf6/jZDQVdBGJ2OqttfzkmXIO2aczt59x\nsNdxMp4KuohEpK7Bx9VTSnHO8dCFReS1y/Y6UsZTH7qIROS2l+exYP0OHr/scPp319Unk4EKuoiE\n7YO1DTw7by3XHT+EcQf29jqOBKnLRUTCMndtFU8vqOfYoT2ZfOIBXseRJlTQRSRklbX1TJxaSudc\nx73jR5Gtk4eSigq6iITE7zcmP1POxh11XDuyPd3zc72OJM2ooItISP76/lJmLNrMbWcczOCuOqIl\nGamgi0ibPli8mb+8t5izR+3DRWMHeB1HWqGCLiJ7tXZ7LZOKZzGssBO/+96hOKd+82Slgi4irapr\n8HHN1DJ8PuOhi4rokKuulmSm49BFpFX/718LmLO2ir9dXMSgnvlex5E2aAtdRFr03Mw1TP98NROP\n25/vHNzH6zgSAhV0Efmaeeuq+MU/53H0/j244SSdPJQqVNBF5CuqahuYOLWU7vm53Hf+KHKyVSZS\nhfrQReRLgZOHZrGhqo5nrzqKngXtvY4kYdCfXhH50v0lSykJnjw0akA3r+NImFTQRQSAGYs2cc+7\nOnkolamgiwhrttUy+ZlynTyU4lTQRTJcXYOPiVNL8fmNh3XyUErTTlGRDGZm/PKf85i3bgePXjqG\ngTp5KKVFvYXunMt2zs1yzr0ai0AikjjTP1/Dc6Vruf74IZwwvNDrOBKlWHS5TAIqYrAeEUmg8jWV\n/OqV+XzzgF5M0pWH0kJUBd05ty9wGvCP2MQRkUTYWr2ba6aU0rtze+4bP1JXHkoT0W6h/wX4OeCP\nQRYRSYBGn5/rps9iS009D19URNeOuvJQunBmFtmCzp0OnGpm1zjnjgP+z8xOb2G+CcAEgMLCwqLi\n4uKIXq+6upqCgoKIlo0n5QqPcoUnHrmeXVTP6ysauPyQXI7dt13S5IqFZM0F0WUbN25cqZmNaXNG\nM4voBvweWAusBDYAtcCUvS1TVFRkkSopKYl42XhSrvAoV3hineuNuV/Yfje+aje9MCeq9WRKe8VS\nNNmAmRZCXY64y8XMbjazfc1sIDAeeN/MLop0fSISX0s3VXPDs7MZsW8XfnXmQV7HkTjQiUUiGWBn\nXQNXPT2TvHbZPHRREe1zdPJQOorJiUVmNgOYEYt1iUhsmRn/99xsVm6tZcrlY+nXtYPXkSROtIUu\nkuYe+mAZb83fyM2nHMhR+/fwOo7EkQq6SBr7cPFm7nprEWeM6MflxwzyOo7EmQq6SJpas62W64tn\nMbR3J/7wfY2gmAlU0EXS0K56H1c9HRhB8W8XF9ExV+PwZQJ9yiJpxsy49aW5LFi/g8cu0wiKmURb\n6CJp5omPV/LirHVMPnEoxx+oERQziQq6SBr5ZNlWfvtaBScdVMj1xw/1Oo4kmAq6SJpYV7mLH08r\nY2CPjvz53BFkaQTFjKOCLpIG6hp8XP10KfWNfh65ZAyd8iIbdEtSm3aKiqQ4M+OWl+Yyd10Vf79k\nDPv3Ss7RBiX+tIUukuKe/HglL5YFdoKedJB2gmYyFXSRFPbp8q385rUKThyunaCigi6SstZur+XH\nU8vYr0dH/nyedoKKCrpISqqtb2TCU6XU+/z8/ZIxdNZOUEE7RUVSjpnxs+fnULFhB49ddrh2gsqX\ntIUukmIenLGM1+as58aTD2TcsN5ex5EkooIukkLeXbCRu95exFkj+3HVNwd7HUeSjAq6SIpYsnEn\nk58p5+B+nfnD9w/TcLjyNSroIimgqraBK58KXBP0kYvHkNdO1wSVr1NBF0lyjT4/104vY13lLh6+\naLSuCSqt0lEuIknut69V8O8lW7jz7EMZM7C713EkiamgiySx91c38NSClVx+zCDGHzHA6ziS5NTl\nIpKkPlqyhSkV9Ywb1otbTh3udRxJASroIklo+eZqrplaSt98x33njyJbp/VLCNTlIpJkKmvrufzJ\nmeRkZzF5dDuNbS4h0xa6SBJp8Pm5ZmoZ67bv4m8XF9Gro76iEjr9togkCTPj9lfm8/Gyrdxx9qEc\nriNaJEwq6CJJ4tGPVjDts9Vc/a39+UHRvl7HkRSkgi6SBN6Yu57fvV7BKYf04effGeZ1HElRKugi\nHitdtZ3Jz5Qzsn9X7jlvpC5UIRFTQRfx0MotNVz51Ez6dMnjH5dojBaJTsQF3TnX3zlX4pxb4Jyb\n75ybFMtgIulue009P3ziv/jNePyyw+lR0N7rSJLiojkOvRG4wczKnHOdgFLn3DtmtiBG2UTSVl2D\njyufmsm6yl1Mu2Isg3XVIYmBiLfQzWy9mZUF7+8EKoB9YhVMJF35/cYNz81m5qrt3HPuSA24JTHj\nzCz6lTg3EPgQOMTMdjR7bgIwAaCwsLCouLg4oteorq6moCD5tmKUKzyZnsvMKF5Uz1srGzl3WDtO\nHZSbFLnCpVzhiybbuHHjSs1sTJszmllUN6AAKAXObmveoqIii1RJSUnEy8aTcoUn03M9ULLE9rvx\nVbv95Xnm9/vbnD/T2ytcyZrLLLpswEwLoR5HdZSLc64d8AIw1cxejGZdIulu+uer+eObgeuB3nb6\nQbqEnMRcNEe5OOBRoMLM/hy7SCLp542567n1pbkcN6wXd50zQseaS1xEs4X+DeBi4HjnXHnwdmqM\ncomkjf8s3cKk4nJGDejGQxcW0S5bp39IfER82KKZfQRoM0NkL2avqWTCUzMZ1DOfxy49nA65OnFI\n4kebCiJxsnRTNZc9/jndC3J56vIj6NJR45pLfKmgi8TB8s3VXPiPT8nOyuLpH42lsHOe15EkA6ig\ni8TYii01nP/3T2n0GVOvGMvAnvleR5IMoYIuEkMrttQw/pFPaPQZ0648kmF9OnkdSTKIrikqEiMr\nt9Rw/iOf0uAzpquYiwe0hS4SAyu31DD+kU+p9/mZduVYFXPxhAq6SJRWBvvMdzf6mHrFWA7s09nr\nSJKh1OUiEoWFG3Zw6WOfU9/oZ9qVRzK8r4q5eEdb6CIR+mz5Vs55+BMApk9QMRfvaQtdJAJvzF3P\npGfK6d+tA09dPpZ9unbwOpKICrpIuJ7+ZCW3vTKfUf278uilh9Mtf+9jmoskigq6SIjMjLvfXsz9\nJUs54cDe3H/BaI3NIklFBV0kBPWNfn7xz7k8O3Mt543pz+++dwg5GjVRkowKukgbNlTVcc3UUspW\nV3Ld8UP46UkH6OIUkpRU0EX24pNlW7luehm19T7uv2AUpx/Wz+tIIq1SQRdpgZnx938v5w9vLmJg\nj45Mv/JIhhbq7E9JbiroIs3srGvg58/P4Y15Gzj10D788QcjKGivr4okP/2WijQxd20Vk56Zxaqt\ntdx66nCuOHaQ+sslZaigiwD1PuPONxby938vp0d+LlMuH8tR+/fwOpZIWFTQJeN9tnwrv/zPLjbW\nLuO8Mf255bThdOmgy8VJ6lFBl4y1s66BP765iKc/XUWvDo6pV4zlG0N6eh1LJGIq6JJx/H7j9Xnr\nueO1CtbvqOPyYwZxRN5GFXNJeSrokjHMjPcXbuLutxezYP0OhhV24v4LRzN6QDdmzNjkdTyRqKmg\nS0b4eOkW7np7EWWrKxnQvSP3nDeCM0fsQ3aWjmCR9KGCLmnLzPh0+Tb++v4SPl62lT6d87jje4dy\nzph9aadxWCQNqaBL2tlWU8+LZWuZ9vlqlm+uoUd+Lr88/SAuHDuAvHYaHVHSlwq6pAUz47MV25j2\n2WrenLeBep+fov26cdc5Qzjt0L4a5lYyggq6pKz6Rj+fr9jGews38l7FJlZvq6VzXg4XjB3A+UcM\nYFgfjb0imUUFXVLKpp11zFi0mfcrNvHR0i1U724kNyeLb+zfg0knDOW0w/qqW0Uylgq6JK26Bh/z\nv9hB+ZpKytdUMntNJau31QLQp3MeZ47sx/HDenP0kB50zNWvskhU3wLn3MnAvUA28A8zuzMmqSSj\n7Kr3sXJrDcs317BiSzXLt9SwZGM1Fet30Og3APp2yWNk/65cMHYAxw7tyUF9O2vQLJFmIi7ozrls\n4AHgJGAt8F/n3CtmtiBW4SS1Nfr87KhrpLK2ni3V9WzYUcfHKxr4qHoBG3fuZmNVHesqd7GuctdX\nluvbJY/BvfK58puDGdm/KyP7d6Wwc55H70IkdUSzhX4EsNTMlgM454qBswAV9CRjZvgNfH4L3Mzw\n+YxGvx+fGY2+wK3e56fB5//K/d2NfuoafOxu9LM7+LOuwUdtvY+a+kZqdwd+1uxupLbeR9WuBipr\nG6isrWdHXWOLefKWr6JP5zx6d87jiEHdGdQzn8G98hnUM3BT94lIZKL55uwDrGnyeC0wNro4Lbvv\nvSUUf1xLx7IP4rH6L5lZy9NbXQBqa2vpOHNGq/M3Xad9Oa3pvPbltD3T9yxje6ZjmEGg98GC0/5X\nqP3BCX4LrM3nN3w+P7z9Oj4zWnlbUcvNySI/N5v89jnk5+bQsX023fNzGdwzn64dc+nSoR3dOraj\na8dcuufn0qdLHkvmzOTUE49Td4lIHMR9U8g5NwGYAFBYWMiMGTPCXsf2Lxoo7OAnJ2tX2zNHKdwy\n4+voJyen7uvraWVF7mt3wAUf7Jnk3Fdz7HnsgjPtuf/l9C+nBe5kuSwaG3y0z22Hc5AFZLnAfNkO\nspwL/gzcsh3kZEF2lvvffQc5WY7cLGiXDe2yHO2a3G+fHXj+f/zBW8NX33ADUAX+KvjiC7DdNXzw\nQXz/MEeiuro6ot/NeFOu8CRrLkhQNjOL6AYcBbzV5PHNwM17W6aoqMgiVVJSEvGy8aRc4VGu8ChX\neJI1l1l02YCZFkJdjmZAi/8CQ51zg5xzucB44JUo/76IiEiEIu5yMbNG59y1wFsEDlt8zMzmxyyZ\niIiEJao+dDN7HXg9RllERCQKGkNURCRNqKCLiKQJFXQRkTShgi4ikiZU0EVE0oSzeJ0X3tKLObcZ\nWBXh4j2BLTGMEyvKFR7lCo9yhSdZc0F02fYzs15tzZTQgh4N59xMMxvjdY7mlCs8yhUe5QpPsuaC\nxGRTl4uISJpQQRcRSROpVNAf8TpAK5QrPMoVHuUKT7LmggRkS5k+dBER2btU2kIXEZG9SKqC7pw7\nxzk33znnd86Nafbczc65pc65Rc6577SyfHfn3DvOuSXBn93ikPEZ51x58LbSOVfeynwrnXNzg/PN\njHWOFl7vV865dU2yndrKfCcH23Cpc+6mBOT6k3NuoXNujnPuJedc11bmS0h7tfX+XcB9wefnOOdG\nxytLk9fs75wrcc4tCP7+T2phnuOcc1VNPt/b4p0r+Lp7/Vw8aq9hTdqh3Dm3wzk3udk8CWkv59xj\nzrlNzrl5TaaFVIfi8l0MZdD0RN2A4cAwYAYwpsn0g4DZQHtgELAMyG5h+T8CNwXv3wT8Ic557wZu\na+W5lUDPBLbdr4D/a2Oe7GDbDQZyg216UJxzfRvICd7/Q2ufSSLaK5T3D5wKvEHgIlBHAp8l4LPr\nC4wO3u8ELG4h13HAq4n6fQr1c/GivVr4TDcQOE474e0FfBMYDcxrMq3NOhSv72JSbaGbWYWZLWrh\nqbOAYjPbbWYrgKUELlLd0nxPBu8/CXw3PkkDWybAucD0eL1GHHx5YW8zqwf2XNg7bszsbTPbc7Xo\nT4F94/l6bQjl/Z8FPGUBnwJdnXN94xnKzNabWVnw/k6ggsA1e1NBwturmROAZWYW6QmLUTGzD4Ft\nzSaHUofi8l1MqoK+Fy1dkLqlX/hCM1sfvL8BKIxjpmOBjWa2pJXnDXjXOVcavK5qIlwX/Lf3sVb+\nzQu1HePlRwS25lqSiPYK5f172kbOuYHAKOCzFp4+Ovj5vuGcOzhBkdr6XLz+nRpP6xtVXrQXhFaH\n4tJucb9IdHPOuXeBPi08dauZvRyr1zEzc85FdAhPiBnPZ+9b58eY2TrnXG/gHefcwuBf84jtLRfw\nEPAbAl/A3xDoDvpRNK8Xi1x72ss5dyvQCExtZTUxb69U45wrAF4AJpvZjmZPlwEDzKw6uH/kn8DQ\nBMRK2s/FBS59eSaB6xk351V7fUU0dSgSCS/oZnZiBIutA/o3ebxvcFpzG51zfc1sffDfvk3xyOic\nywHOBor2so51wZ+bnHMvEfgXK6ovQqht55z7O/BqC0+F2o4xzeWcuww4HTjBgh2ILawj5u3VglDe\nf1zaqC3OuXYEivlUM3ux+fNNC7yZve6ce9A519PM4jpuSQifiyftFXQKUGZmG5s/4VV7BYVSh+LS\nbqnS5fIKMN451945N4jAX9rPW5nv0uD9S4GYbfE3cyKw0MzWtvSkcy7fOddpz30COwbntTRvrDTr\nt/xeK6+X8At7O+dOBn4OnGlmta3Mk6j2CuX9vwJcEjx640igqsm/z3ER3B/zKFBhZn9uZZ4+wflw\nzh1B4Lu7Nc65QvlcEt5eTbT6X7IX7dVEKHUoPt/FeO8FDudGoBCtBXYDG4G3mjx3K4G9wouAU5pM\n/wfBI2KAHsB7wBLgXaB7nHI+AVzdbFo/4PXg/cEE9lrPBuYT6HqId9s9DcwF5gR/Mfo2zxV8fCqB\noyiWJSjXUgJ9heXB28NetldL7x+4es/nSeBojQeCz8+lydFWccx0DIGusjlN2unUZrmuDbbNbAI7\nl49OQK4WPxev2yv4uvkECnSXJtMS3l4E/qCsBxqCtevy1upQIr6LOlNURCRNpEqXi4iItEEFXUQk\nTaigi4g1UGjkAAAAJElEQVSkCRV0EZE0oYIuIpImVNBFRNKECrqISJpQQRcRSRP/H70pGi1jWO94\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aef2080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "n = np.linspace(-10, 10, 50)\n",
    "out = softplus(n)\n",
    "\n",
    "plt.plot(n, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Properties of Sigmoid & Softplus functions\n",
    "Derivations for Page 67 formulae. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.33. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(x) =& \\frac{e^x}{e^x} \\times \\frac{1}{1 + e^{-x}} \\\\\n",
    "=& \\frac{e^x}{e^x(1 + e^{-x})} \\\\\n",
    "=& \\frac{e^x}{e^x + 1}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.34. \n",
    "\n",
    "Let $y = e^{-x}$, $z = 1 + y$, then we have $\\sigma(x) = z^{-1} $.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{dx}\\sigma(x) =& \\frac{d\\sigma}{dz} \\times \\frac{dz}{dy} \\times \\frac{dy}{dx} \\\\\n",
    "=& -z^{-2} \\times 1 \\times -e^{-x} \\\\\n",
    "=& -(1 + e^{-1})^{-2} \\times -e^{-x} \\\\\n",
    "=& \\frac{e^{-x}}{(1 + e^{-1})^{2}} \\\\\n",
    "=& \\frac{1}{1 + e^{-1}} \\times \\frac{e^{-x}}{1 + e^{-1}} \\\\\n",
    "=& \\sigma(x) (1-\\sigma(x))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.35.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1 - \\sigma(x) =& 1 - \\frac{1}{1 + e^{-x}}\\\\\n",
    "=& \\frac{1 + e^{-x} - 1}{1 + e^{-x}} \\\\\n",
    "=& \\frac{e^{-x}}{1 + e^{-x}} \\\\\n",
    "=& \\frac{1}{e^x + 1} \\\\\n",
    "=& \\sigma(-x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.36.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log(\\sigma(x)) &= \\log\\big(\\frac{1}{1 + e^{-x}}\\big) \\\\\n",
    "&= \\log(1) - \\log(1 + e^{-x}) \\\\\n",
    "&= -\\log(1 + e^{-x}) \\\\\n",
    "&= -\\zeta(-x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.37.\n",
    "\n",
    "Let $u = 1 + e^x$, hence $\\zeta(x) = \\log(u)$, then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{dx}\\zeta(x) =& \\frac{d\\zeta}{du} \\times \\frac{du}{dx} \\\\\n",
    "&= \\frac{1}{u} \\times e^x \\\\\n",
    "&= \\frac{e^x}{1 + e^x} \\\\\n",
    "&= \\frac{1}{e^{-x} + 1} \\\\\n",
    "&= \\sigma(x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.38.\n",
    "\n",
    "**Logit function**, for $ \\forall x \\in (0, 1)$:\n",
    "\n",
    "$$ \\sigma^{-1}(x) = \\log\\bigg(\\frac{x}{1-x}\\bigg) $$\n",
    "\n",
    "Here the power of -1 does not mean reciprical, but the **inverse**. Eg. given $\\sigma(x)$, find $x$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}} \\\\\n",
    "1 + e^{-x} &= \\frac{1}{\\sigma(x)} \\\\\n",
    "e^{-x} &= \\frac{1}{\\sigma(x)} - 1 \\\\\n",
    "-x &= \\log\\bigg(\\frac{1-\\sigma(x)}{\\sigma(x)}\\bigg) \\\\\n",
    "x &= -\\log\\bigg(\\frac{1-\\sigma(x)}{\\sigma(x)}\\bigg) \\\\\n",
    "x &= \\log\\bigg(\\frac{\\sigma(x)}{1-\\sigma(x)}\\bigg)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40546510810816422"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "def logit(x):\n",
    "    return np.log(x / (1.-x))\n",
    "\n",
    "logit(.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59999999999999998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(logit(.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.39.\n",
    "\n",
    "Inverse of $\\zeta(x)$, let $u=\\zeta(x)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u &= \\log(1 + e^x) \\\\\n",
    "e^u &= 1 + e^x \\\\\n",
    "e^x &= e^u - 1 \\\\\n",
    "\\forall x &> 0 \\text{, take log on both sides} \\\\\n",
    "x &= \\log\\big( e^u -1 \\big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.40.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\int^{x}_{-\\infty} \\sigma(y)dy &= \\int^{x}_{-\\infty} \\frac{1}{1 + e^{-y}} \\\\\n",
    "&= \\log \\big \\lvert 1 + e^{-y} \\big \\rvert + y \\\\\n",
    "&= \\log \\big ( \\frac{e^y + 1}{e^y} \\big ) + y \\\\\n",
    "&= \\log(e^y + 1) - \\log(e^y) + y \\\\\n",
    "&= \\log(e^y + 1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Key here is the integration part. Results can be checked with `SymPy`. Or, to reverse that back:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{dx}\\big(\\log(1 + e^{-x}) + x\\big) &= \\frac{1}{1+e^{-x}} \\times -e^{-x} + 1 \\\\\n",
    "&= \\frac{1 + e^{-x} - e^{-x}}{1 + e^{-x}} \\\\\n",
    "&= \\sigma(x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sympy as spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y + log(1 + exp(-y))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = spy.Symbol('y')\n",
    "\n",
    "spy.integrate(1 / (1 + spy.exp(-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exp(-y)/(1 + exp(-y))**2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy.diff(1 / (1 + spy.exp(-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1 - exp(-y)/(1 + exp(-y))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy.diff(y + spy.log(1 + spy.exp(-y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.41\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\zeta(x)-\\zeta(-x) &= \\log(1 + e^x) - \\log(1 + e^{-x}) \\\\\n",
    "&= \\log\\bigg(\\frac{1 + e^x}{1 + e^{-x}}\\bigg) \\\\\n",
    "&= \\log\\bigg(\\frac{e^{-x}(1 + e^x)}{e^{-x}(1 + e^{-x})}\\bigg) \\\\\n",
    "&= \\log\\bigg(\\frac{e^{-x} + 1}{e^{-x}(1 + e^{-x})}\\bigg) \\\\\n",
    "&= \\log\\bigg(\\frac{1}{e^{-x}}\\bigg) \\\\\n",
    "&= \\log(e^x) \\\\\n",
    "&= x\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Softmax Function, p78\n",
    "\n",
    "$$ softmax(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n} \\exp(x_j)} $$\n",
    "\n",
    "Overflow when $x_i$ is very large, underflow when $x_i$ is very negative. Solution is to use $softmax(z)$, where:\n",
    "$z = x - max_i x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Poor Conditioning\n",
    "\n",
    "Given function $f(x) = A^{-1}x. When $A \\in \\mathbb{R}^{n\\times n} has an **eigenvalue decomposition**, its **condition number** is:\n",
    "\n",
    "$$ \\max_{i,j} \\bigg \\lvert \\frac{\\lambda_i}{\\lambda_j} \\bigg \\rvert $$\n",
    "\n",
    "I.e. Ratio of largest and smallest eigenvalues.\n",
    "\n",
    "Poor conditioning makes choosing a good optimization step size difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hessian Matrix & Min/Max/Saddle Points\n",
    "\n",
    "At a **critical point** where $\\triangledown_x f(x) = 0$:\n",
    "\n",
    "* **Local minimum** if the Hessian matrix is **positive definite** (i.e. all of its eigenvalues are positive).\n",
    "* **local maximum** if the Hessian matrix is **negative definite** (i.e. all of its eigenvalues are negative).\n",
    "* **Inconclusive** if all non-zero eigenvalues have the same sign but **at least one eigenvalue is zero**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Chapter 5\n",
    "\n",
    "**Representational Capacity**, p110, The model specifis which family of functions the learning algorithm can choose from when varying the parameters in order to reduce a training objective. \n",
    "\n",
    "**Effective Capacity** can be limited by imperfections of the optimization algorithm, which means it may be less than the representational capacity.\n",
    "\n",
    "## Vapnik-Chervonenkis (VC) Dimension, p111\n",
    "\n",
    "VC dimension measres the capacity of a binary classifier. Defined as being the lagest possible value of $m$ for which there exists a training set of $m$ different $x$ points that the classifier can label arbitrarily.\n",
    "\n",
    "The most important results in statistical learning theory show that the discrepancy between the trainnig error and generalization error is bounded from above by a quantity that grows as the model capacity grows but shrinks as the number of traning examples increase. \n",
    "\n",
    "However, this is rarely used in practice when working with deep learning algos. Because:\n",
    "* it can be quite difficult to determine the capacity of deal learning algos.\n",
    "* The problem of determining the capacity of deep learning models is especially difficult because the effective capacity is limited by the capabilities of the optimization algorithm. \n",
    "* we have little theoretical understanding of the general non-convex optimization problems involved in deep learning.\n",
    "\n",
    "Typically training error decreases until it asymptotes to the minimum possible error value as model capacity increases (assuming the error measure has a minimum value); generalization error typically has a U-shaped curve as a function of model capacity.\n",
    "\n",
    "**No Free Lunch theorem**: averaged over all possible data-generating distributions, every classification algothrim has the same error rate when classifying previously unobserved points. \n",
    "\n",
    "**Goal** is to understand what kind of distributions are relevant to the real world that an AI agent experiences, and what kind of machine learning algorithem perform well on data drawn from the kinds of data-generating distributions we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Estimator, Bias and Variance\n",
    "\n",
    "Sample variance, unbiased sample variance, both underestimates the true standard deviation but are still used in practice. Unbiased sample variance is less of an underestimate. For large sample size $m$, the approximation is quite reasonable.\n",
    "\n",
    "## Maximum Likelihood Estimation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_{ML} &= \\underset{\\theta}{\\operatorname{arg max}} p_{model}(\\mathbb{X}; \\theta) \\\\\n",
    "&= \\underset{\\theta}{\\operatorname{arg max}}\\Pi_{i=1}^{m}p_{model}(x^{(i)}; \\theta) \\\\\n",
    "&= \\underset{\\theta}{\\operatorname{arg max}}\\sum_{i=1}^m \\log p_{model}(x^{(i)}; \\theta) \\\\\n",
    "D_{KL}(\\hat{p}_{data} \\lVert p_{model}) &= \\mathbb{E}_{x \\sim \\hat{p}_{data}} \\bigg[ \\log \\hat{p}_{data}(x) - \\log p_{model}(x)\\bigg] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore minimizing KL divergence corresponds exactly to minimzing the cross-entropy between the distributions.\n",
    "\n",
    "**Appropriate Conditions** for maximum likelihood estimator to have the property of **consistency**:\n",
    "\n",
    "* The true distribution $p_{data}$ must lie within the model family $p_{model}(.;\\theta)$, otherwise, no estimator can recover $p_{data}$.\n",
    "\n",
    "* The true distribution $p_{data}$ must corresond to example one value of $\\theta$. Otherwise maximum likelihood can recover the correct $p_{data}$ but will not be able to determine which value of $\\theta$ was used by the data-generating process.\n",
    "\n",
    "For large sample size $m$, the **Cramer-Rao lower bound** shows that no consistent estimator has a lower Mean Square Error than the maximum likelihood estimator.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Linear Regression as Maximum Likelihood, p130\n",
    "\n",
    "Gaussian PDF:\n",
    "\n",
    "$$ PDF(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\bigg(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\bigg) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Conditional log-likilihood derivation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^{m}\\log p(y^{(i)} \\mid x^{(i)};\\theta) \\\\\n",
    "=&\\sum_{i=1}^{m}\\log\\bigg[\\big(2\\pi\\sigma^2\\big)^{-1/2}\\exp\\bigg(-\\frac{\\lVert\\hat{y}^{(i)}-y^{(i)}\\rVert^2}{2\\sigma^2}\\bigg)\\bigg] \\\\\n",
    "=& \\sum_{i=1}^{m}\\bigg\\{-\\frac{1}{2}\\log\\big(2\\pi\\sigma^2\\big) + \\log\\bigg[\\exp\\bigg(-\\frac{\\lVert\\hat{y}^{(i)}-y^{(i)}\\rVert^2}{2\\sigma^2}\\bigg)\\bigg]\\bigg\\} \\\\\n",
    "=&\\sum_{i=1}^{m}\\bigg(-\\frac{1}{2}\\log\\big(2\\pi\\sigma^2\\big)\\bigg) - \\sum_{i=1}^{m}\\frac{\\lVert\\hat{y}^{(i)}-y^{(i)}\\rVert^2}{2\\sigma^2} \\\\\n",
    "=& -\\frac{m}{2}\\log\\big( 2\\pi \\big) - m\\log\\sigma - \\sum_{i=1}^{m}\\frac{\\lVert\\hat{y}^{(i)}-y^{(i)}\\rVert^2}{2\\sigma^2} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bayesian Regression, p134\n",
    "\n",
    "Likelihood:\n",
    "\n",
    "$$ p(y \\mid X, w) \\propto \\exp\\bigg[-\\frac{1}{2}\\big(y-Xw\\big)^T \\big(y-Xw\\big)\\bigg] $$\n",
    "\n",
    "Prior:\n",
    "\n",
    "$$ p(w) \\propto \\exp\\bigg[ -\\frac{1}{2}(w-\\mu_0)^T\\Lambda^{-1}_{0}(w-\\mu_0) \\bigg] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Posterior:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(w \\mid X, y) &\\propto p(y \\mid X, w)p(w) \\\\\n",
    "&\\propto \\exp\\bigg[-\\frac{1}{2}\\big(y-Xw\\big)^T \\big(y-Xw\\big)\\bigg] \\exp\\bigg[ -\\frac{1}{2}(w-\\mu_0)^T\\Lambda^{-1}_{0}(w-\\mu_0)\\bigg] \\\\\n",
    "&\\propto \\exp\\bigg\\{ -\\frac{1}{2} \\bigg[ y^Ty - y^TXw - w^T X^T y + w^T X^T X w  + w^T \\Lambda^{-1}_{0}w - w^T\\Lambda^{-1}_{0}\\mu_0 -\\mu^T_0 \\Lambda^{-1}_{0}w + \\mu^T_0 \\Lambda^{-1}_{0}\\mu_0 \\bigg] \\bigg\\} \\\\\n",
    "\\because &\\text{ } y^TXw = w^T X^T y,\\text{ } \\mu^T_0 \\Lambda^{-1}_{0} w = w^T \\Lambda^{-1}_{0}\\mu_0 \\\\\n",
    "\\because &\\text{ terms without } w \\text{ can be ignored} \\\\\n",
    "&\\propto \\exp\\bigg\\{ -\\frac{1}{2} \\bigg[-2y^TXw + w^T X^T X w  + w^T \\Lambda^{-1}_{0}w - 2\\mu^T_0 \\Lambda^{-1}_{0}w \\bigg] \\bigg\\} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "By setting:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Lambda_m &= \\big(X^T X + \\Lambda^{-1}_0\\big)^{-1} \\\\\n",
    "\\mu_m &= \\Lambda_m \\big(X^T y + \\Lambda^{-1}_0 \\mu_0\\big) \\\\\n",
    "& \\therefore \\\\\n",
    "\\mu^T_m &= \\big(X^T y + \\Lambda^{-1}_0\\mu_0\\big)^T \\Lambda^T_m  \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Also note $\\Lambda^T_m = \\Lambda_m$ as $\\Lambda_m = diag(\\lambda_m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Rewrite posterior:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(w \\mid X, y) &\\propto \\exp\\bigg(-\\frac{1}{2}\\big(w - \\mu_m\\big)^T \\Lambda^{-1}_m \\big(w - \\mu_m\\big)\\bigg) \\\\\n",
    "&\\propto \\exp\\bigg\\{-\\frac{1}{2}\\bigg[ w^T\\big(X^T X + \\Lambda^{-1}_0\\big)w - w^T\\Lambda^{-1}_m\\Lambda_m \\big(X^T y + \\Lambda^{-1}_0\\mu_0 \\big) -\\big(X^T y + \\Lambda^{-1}_0\\mu_0 \\big)^T\\Lambda^T_m\\Lambda^{-1}_m w \\bigg]\\bigg\\} \\\\\n",
    "&\\propto \\exp\\bigg\\{-\\frac{1}{2}\\bigg[ w^T X^T X w + w^T \\Lambda^{-1}_0 w - w^T X^T y - w^T \\Lambda^{-1}_0\\mu_0 - y^T Xw - \\mu^T_0 \\Lambda^{-1}_0 w \\bigg]\\bigg\\} \\\\\n",
    "&\\propto \\exp\\bigg\\{-\\frac{1}{2}\\bigg[ w^T X^T X w + w^T \\Lambda^{-1}_0 w - 2 y^T Xw - 2 \\mu^T_0 \\Lambda^{-1}_0 w \\bigg]\\bigg\\}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we set $\\mu_0 = 0$, $\\Lambda_0 = \\frac{1}{\\alpha} I$, flat prior for covariance:\n",
    "\n",
    "$$ p(w \\mid X, y) \\propto \\exp\\bigg\\{-\\frac{1}{2}\\bigg[ w^T X^T X w + w^T \\alpha I w - 2 y^T X w \\bigg]\\bigg\\} $$\n",
    "\n",
    "This is the form of **ridge regression**, weight decay term $\\alpha w^T w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Supervised Learning Algorithms\n",
    "\n",
    "### K-Nearest Neighbour\n",
    "* k-nearest neighbour can achieve high capacity. Enables it to obtain high accuracy given a large training set.\n",
    "* this comes with computational costs\n",
    "* may generalize very badly given a small training set. \n",
    "* It cannot learn that one feature is more discriminative than another.\n",
    "* Cannot solve a problem where $X\\in\\mathbb{R}^{100}$ but only $x_1$ is relavent to the output. \n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "It struggles to solve some problems that are easy even for logistic regression, E.g. a 2-class problem and the positive class occurs whenever $x_2 > x_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Unsupervised Learning Algorithms\n",
    "\n",
    "Most comment representations:\n",
    "1. lower-dimensional representation,\n",
    "2. sparse representation (increase dimension),\n",
    "3. independent representation.\n",
    "\n",
    "### PCA\n",
    "\n",
    "Given design matrix $X$, centered with $\\mathbb{E}(x)=0$.\n",
    "\n",
    "Unbiased Sample Covariance associated with X is:\n",
    "\n",
    "$$ Var(x) = \\frac{1}{m-1}X^T X $$\n",
    "\n",
    "PCA finds a representation (through orghogonal, linear transformations) $z=W^T x$ where $Var(z)$ is diagonal (implies zero covariance of $z$).\n",
    "\n",
    "Two ways to find the Principal Components, $W$:\n",
    "\n",
    "* With Eigenvalue Decomposition: $X^T X = W \\Lambda W^T$\n",
    "\n",
    "* With SVD: $X=U \\Sigma W^T$, the **principal components** are the right singular vectors of $X$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^T X &= (U \\Sigma W^T)^T U \\Sigma W^T \\\\\n",
    "&= W\\Sigma^T U^T U \\Sigma W^T \\\\\n",
    "\\because U \\text{ is orthogonal, } U^T U &= I \\\\\n",
    "X^T X &= W \\Sigma^2 W^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can show $Var(z)$ is diagonal:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z &= XW \\\\\n",
    "Var(z) &= \\frac{1}{m-1} Z^T Z \\\\\n",
    "&= \\frac{1}{m-1} (XW)^T X W \\\\\n",
    "&= \\frac{1}{m-1} W^T X^T X W \\\\\n",
    "\\because X^T X &= W \\Sigma^2 W^T \\\\\n",
    "Var(z) &= W^T W \\Sigma^2 W^T W \\\\\n",
    "\\because W^T W = I \\\\\n",
    "Var(z) &= \\Sigma^2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SGD\n",
    "\n",
    "Uses **minibatch** of $m'$, typically $m'$ ranges from 1 to a few 100s. Crucially, $m'$ is usually held fixed as the training set size $m$ grows.\n",
    "\n",
    "The number of updates required to reach convergence usually increases with training set size. However, as $m$ approaches infinity, the model will eventually converge to its best possible test error before SGD has sampled every sample in the training set. \n",
    "\n",
    "## ML Algo Building Blocks:\n",
    "\n",
    "Four pieces:\n",
    "1. dataset\n",
    "2. cost function\n",
    "3. optimization procedure\n",
    "4. model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Constancy & Smoothness Regularization\n",
    "\n",
    "**Smoothness prior** or **local constancy prior** states that the function we learn should not change very much within a small region.\n",
    "\n",
    "Relying on this assumption alone fails to solve many deep learning problems. k-means and decision treess all suffer from this problem.\n",
    "\n",
    "In general, to distinguish $O(k)$ regions in input space, all these methods require $O(k)$ examples. Typically, there are $O(k)$ parameters, with $O(1)$ parameters associated with each of the $O(k)$ regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning\n",
    "\n",
    "In machine learning, manifold is loosely used to designate a connected set of points that can be approximated well by considering only a small number of degrees of freedom, or dimensions, embedded in a higher-dimensional space. Each direction corresponds to a direction of variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Chapter 6\n",
    "\n",
    "## Gradient Based Learning\n",
    "\n",
    "For **feedforward neural nets**, it is important to initialize all weights to **small random numbers**. The biases may be initialized to **zero or to small positive values**. p172.\n",
    "\n",
    "Unfortunately mean square error and mean absolute error often lead to poor results when used with gradient-based optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Output Unit for Bernoulli Output Distribution\n",
    "\n",
    "Sigmoid output unit is defined by:\n",
    "\n",
    "$$ \\hat{y} = \\sigma\\big(w^T h + b\\big) $$\n",
    "\n",
    "where $\\sigma$ is the logisitc sigmod function, $\\hat{y}$ is a scaler. In this case set $z = w^T h + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Units for Multinoulli Output Distribution\n",
    "\n",
    "In this case the model output $\\hat{y}$ is a vector, of the following properties:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\forall i, y_i &\\in [0, 1] \\\\\n",
    "\\sum_i y &= 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Softmax Function\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\text{softmax}(z)_i &= \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)} \\\\\n",
    "\\log \\text{ softmax}(z)_i &= z_i - \\log\\sum_j \\exp(z_j) \\\\\n",
    "\\text{softmax}(z) &= \\text{softmax}(z - max_i z_i) \\text{, this is numerically stable}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$z$ is defined as:\n",
    "\n",
    "$$ z = W^T h + b $$\n",
    "\n",
    "where $W$ is a **matrix**, as oppose to $w$ being a **vector** in the Bernooulli case.\n",
    "\n",
    "Note that log-softmax **cannot saturate**.\n",
    "\n",
    "The log-likelihood term $\\log\\sum_j \\exp(z_j)$ can be roughly approximated as $\\max_j z_j$. p180. Therefore the intuition is that the negative log-likelihood cost funciton always strongly penalizes the most active incorrect prediction. \n",
    "\n",
    "Softmax function can saturate when the difference between input values become extreme. p180. Numerically stable softmax uses the above $\\max_j z_j$ approximation. If the cost function is not designed to undo the effects of softmax, it would also saturate when softmax saturates, in which case the gradient vanishes.\n",
    "\n",
    "In simple form, have $n$ outputs is an **overparameterization**, since $\\hat{y}$ sums to 1, we only need to know $n-1$ values. In practice, overparameterization rarely causes much difference and is simpler to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logsoftmax(z, i):\n",
    "    log_total = np.log(np.sum(np.exp(z)))\n",
    "    return z[i] - log_total\n",
    "\n",
    "def softmax(z, i):\n",
    "    '''\n",
    "    Numerically stable softmax.\n",
    "    '''\n",
    "    max_z = np.max(z)\n",
    "    z_mod = z - max_z\n",
    "    top = np.exp(z_mod[i])\n",
    "    bottom = np.sum(np.exp(z_mod))\n",
    "    return top / bottom\n",
    "\n",
    "def softmax_unstable(z, i):\n",
    "    top = np.exp(z[i])\n",
    "    bottom = np.sum(np.exp(z))\n",
    "    return top / bottom\n",
    "\n",
    "z = np.zeros(10)\n",
    "z[0] = 10000\n",
    "z[1] = 1\n",
    "\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zwl/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:16: RuntimeWarning: overflow encountered in exp\n",
      "/Users/zwl/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:17: RuntimeWarning: overflow encountered in exp\n",
      "/Users/zwl/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# this version has overflow problem\n",
    "print(softmax_unstable(z, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# this version performs well.\n",
    "print(softmax(z, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
