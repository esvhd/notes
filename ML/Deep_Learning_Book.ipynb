{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Learning Notes\n",
    "\n",
    "by zwl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Essential Formulae\n",
    "\n",
    "Definitions first:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\sigma(x) =& \\frac{1}{1 + e^{-x}} \\\\\n",
    "\\zeta(x) =& \\log(1 + e^x) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Properties of Sigmoid & Softplus functions\n",
    "Derivations for Page 67 formulae. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.33. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(x) =& \\frac{e^x}{e^x} \\times \\frac{1}{1 + e^{-x}} \\\\\n",
    "=& \\frac{e^x}{e^x(1 + e^{-x})} \\\\\n",
    "=& \\frac{e^x}{e^x + 1}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.34. \n",
    "\n",
    "Let $y = e^{-x}$, $z = 1 + y$, then we have $\\sigma(x) = z^{-1} $.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{dx}\\sigma(x) =& \\frac{d\\sigma}{dz} \\times \\frac{dz}{dy} \\times \\frac{dy}{dx} \\\\\n",
    "=& -z^{-2} \\times 1 \\times -e^{-x} \\\\\n",
    "=& -(1 + e^{-1})^{-2} \\times -e^{-x} \\\\\n",
    "=& \\frac{e^{-x}}{(1 + e^{-1})^{2}} \\\\\n",
    "=& \\frac{1}{1 + e^{-1}} \\times \\frac{e^{-x}}{1 + e^{-1}} \\\\\n",
    "=& \\sigma(x) (1-\\sigma(x))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.35.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1 - \\sigma(x) =& 1 - \\frac{1}{1 + e^{-x}}\\\\\n",
    "=& \\frac{1 + e^{-x} - 1}{1 + e^{-x}} \\\\\n",
    "=& \\frac{e^{-x}}{1 + e^{-x}} \\\\\n",
    "=& \\frac{1}{e^x + 1} \\\\\n",
    "=& \\sigma(-x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.36.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log(\\sigma(x)) &= \\log\\big(\\frac{1}{1 + e^{-x}}\\big) \\\\\n",
    "&= \\log(1) - \\log(1 + e^{-x}) \\\\\n",
    "&= -\\log(1 + e^{-x}) \\\\\n",
    "&= -\\zeta(-x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.37.\n",
    "\n",
    "Let $u = 1 + e^x$, hence $\\zeta(x) = \\log(u)$, then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{dx}\\zeta(x) =& \\frac{d\\zeta}{du} \\times \\frac{du}{dx} \\\\\n",
    "&= \\frac{1}{u} \\times e^x \\\\\n",
    "&= \\frac{e^x}{1 + e^x} \\\\\n",
    "&= \\frac{1}{e^{-x} + 1} \\\\\n",
    "&= \\sigma(x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.38.\n",
    "\n",
    "**Logit function**, for $ \\forall x \\in (0, 1)$:\n",
    "\n",
    "$$ \\sigma^{-1}(x) = \\log\\bigg(\\frac{x}{1-x}\\bigg) $$\n",
    "\n",
    "Here the power of -1 does not mean reciprical, but the **inverse**. Eg. given $\\sigma(x)$, find $x$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}} \\\\\n",
    "1 + e^{-x} &= \\frac{1}{\\sigma(x)} \\\\\n",
    "e^{-x} &= \\frac{1}{\\sigma(x)} - 1 \\\\\n",
    "-x &= \\log\\bigg(\\frac{1-\\sigma(x)}{\\sigma(x)}\\bigg) \\\\\n",
    "x &= -\\log\\bigg(\\frac{1-\\sigma(x)}{\\sigma(x)}\\bigg) \\\\\n",
    "x &= \\log\\bigg(\\frac{\\sigma(x)}{1-\\sigma(x)}\\bigg)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40546510810816422"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "def logit(x):\n",
    "    return np.log(x / (1.-x))\n",
    "\n",
    "logit(.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59999999999999998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(logit(.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.39.\n",
    "\n",
    "Inverse of $\\zeta(x)$, let $u=\\zeta(x)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u &= \\log(1 + e^x) \\\\\n",
    "e^u &= 1 + e^x \\\\\n",
    "e^x &= e^u - 1 \\\\\n",
    "\\forall x &> 0 \\text{, take log on both sides} \\\\\n",
    "x &= \\log\\big( e^u -1 \\big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.40.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\int^{x}_{-\\infty} \\sigma(y)dy &= \\int^{x}_{-\\infty} \\frac{1}{1 + e^{-y}} \\\\\n",
    "&= \\log \\big \\lvert 1 + e^{-y} \\big \\rvert + y \\\\\n",
    "&= \\log \\big ( \\frac{e^y + 1}{e^y} \\big ) + y \\\\\n",
    "&= \\log(e^y + 1) - \\log(e^y) + y \\\\\n",
    "&= \\log(e^y + 1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Key here is the integration part. Results can be checked with `SymPy`. Or, to reverse that back:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{dx}\\big(\\log(1 + e^{-x}) + x\\big) &= \\frac{1}{1+e^{-x}} \\times -e^{-x} + 1 \\\\\n",
    "&= \\frac{1 + e^{-x} - e^{-x}}{1 + e^{-x}} \\\\\n",
    "&= \\sigma(x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sympy as spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y + log(1 + exp(-y))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = spy.Symbol('y')\n",
    "\n",
    "spy.integrate(1 / (1 + spy.exp(-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exp(-y)/(1 + exp(-y))**2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy.diff(1 / (1 + spy.exp(-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1 - exp(-y)/(1 + exp(-y))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy.diff(y + spy.log(1 + spy.exp(-y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.41\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\zeta(x)-\\zeta(-x) &= \\log(1 + e^x) - \\log(1 + e^{-x}) \\\\\n",
    "&= \\log\\bigg(\\frac{1 + e^x}{1 + e^{-x}}\\bigg) \\\\\n",
    "&= \\log\\bigg(\\frac{e^{-x}(1 + e^x)}{e^{-x}(1 + e^{-x})}\\bigg) \\\\\n",
    "&= \\log\\bigg(\\frac{e^{-x} + 1}{e^{-x}(1 + e^{-x})}\\bigg) \\\\\n",
    "&= \\log\\bigg(\\frac{1}{e^{-x}}\\bigg) \\\\\n",
    "&= \\log(e^x) \\\\\n",
    "&= x\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Softmax Function, p78\n",
    "\n",
    "$$ softmax(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^{n} \\exp(x_j)} $$\n",
    "\n",
    "Overflow when $x_i$ is very large, underflow when $x_i$ is very negative. Solution is to use $softmax(z)$, where:\n",
    "$z = x - max_i x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Poor Conditioning\n",
    "\n",
    "Given function $f(x) = A^{-1}x. When $A \\in \\mathbb{R}^{n\\times n} has an **eigenvalue decomposition**, its **condition number** is:\n",
    "\n",
    "$$ \\max_{i,j} \\bigg \\lvert \\frac{\\lambda_i}{\\lambda_j} \\bigg \\rvert $$\n",
    "\n",
    "I.e. Ratio of largest and smallest eigenvalues.\n",
    "\n",
    "Poor conditioning makes choosing a good optimization step size difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hessian Matrix & Min/Max/Saddle Points\n",
    "\n",
    "At a **critical point** where $\\triangledown_x f(x) = 0$:\n",
    "\n",
    "* **Local minimum** if the Hessian matrix is **positive definite** (i.e. all of its eigenvalues are positive).\n",
    "* **local maximum** if the Hessian matrix is **negative definite** (i.e. all of its eigenvalues are negative).\n",
    "* **Inconclusive** if all non-zero eigenvalues have the same sign but **at least one eigenvalue is zero**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Chapter 5\n",
    "\n",
    "**Representational Capacity**, p110, The model specifis which family of functions the learning algorithm can choose from when varying the parameters in order to reduce a training objective. \n",
    "\n",
    "**Effective Capacity** can be limited by imperfections of the optimization algorithm, which means it may be less than the representational capacity.\n",
    "\n",
    "## Vapnik-Chervonenkis (VC) Dimension, p111\n",
    "\n",
    "VC dimension measres the capacity of a binary classifier. Defined as being the lagest possible value of $m$ for which there exists a training set of $m$ different $x$ points that the classifier can label arbitrarily.\n",
    "\n",
    "The most important results in statistical learning theory show that the discrepancy between the trainnig error and generalization error is bounded from above by a quantity that grows as the model capacity grows but shrinks as the number of traning examples increase. \n",
    "\n",
    "However, this is rarely used in practice when working with deep learning algos. Because:\n",
    "* it can be quite difficult to determine the capacity of deal learning algos.\n",
    "* The problem of determining the capacity of deep learning models is especially difficult because the effective capacity is limited by the capabilities of the optimization algorithm. \n",
    "* we have little theoretical understanding of the general non-convex optimization problems involved in deep learning.\n",
    "\n",
    "Typically training error decreases until it asymptotes to the minimum possible error value as model capacity increases (assuming the error measure has a minimum value); generalization error typically has a U-shaped curve as a function of model capacity.\n",
    "\n",
    "**No Free Lunch theorem**: averaged over all possible data-generating distributions, every classification algothrim has the same error rate when classifying previously unobserved points. \n",
    "\n",
    "**Goal** is to understand what kind of distributions are relevant to the real world that an AI agent experiences, and what kind of machine learning algorithem perform well on data drawn from the kinds of data-generating distributions we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator, Bias and Variance\n",
    "\n",
    "Sample variance, unbiased sample variance, both underestimates the true standard deviation but are still used in practice. Unbiased sample variance is less of an underestimate. For large sample size $m$, the approximation is quite reasonable.\n",
    "\n",
    "## Maximum Likelihood Estimation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_{ML} &= \\underset{\\theta}{\\operatorname{arg max}} p_{model}(\\mathbb{X}; \\theta) \\\\\n",
    "&= \\underset{\\theta}{\\operatorname{arg max}}\\Pi_{i=1}^{m}p_{model}(x^{(i)}; \\theta) \\\\\n",
    "&= \\underset{\\theta}{\\operatorname{arg max}}\\sum_{i=1}^m \\log p_{model}(x^{(i)}; \\theta) \\\\\n",
    "D_{KL}(\\hat{p}_{data} \\lVert p_{model}) &= \\mathbb{E}_{x \\sim \\hat{p}_{data}} \\bigg[ \\log \\hat{p}_{data}(x) - \\log p_{model}(x)\\bigg] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore minimizing KL divergence corresponds exactly to minimzing the cross-entropy between the distributions.\n",
    "\n",
    "**Appropriate Conditions** for maximum likelihood estimator to have the property of **consistency**:\n",
    "\n",
    "* The true distribution $p_{data}$ must lie within the model family $p_{model}(.;\\theta)$, otherwise, no estimator can recover $p_{data}$.\n",
    "\n",
    "* The true distribution $p_{data}$ must corresond to example one value of $\\theta$. Otherwise maximum likelihood can recover the correct $p_{data}$ but will not be able to determine which value of $\\theta$ was used by the data-generating process.\n",
    "\n",
    "For large sample size $m$, the **Cramer-Rao lower bound** shows that no consistent estimator has a lower Mean Square Error than the maximum likelihood estimator.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Algorithms\n",
    "\n",
    "### K-Nearest Neighbour\n",
    "* k-nearest neighbour can achieve high capacity. Enables it to obtain high accuracy given a large training set.\n",
    "* this comes with computational costs\n",
    "* may generalize very badly given a small training set. \n",
    "* It cannot learn that one feature is more discriminative than another.\n",
    "* Cannot solve a problem where $X\\in\\mathbb{R}^{100}$ but only $x_1$ is relavent to the output. \n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "It struggles to solve some problems that are easy even for logistic regression, E.g. a 2-class problem and the positive class occurs whenever $x_2 > x_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Algorithms\n",
    "\n",
    "Most comment representations:\n",
    "1. lower-dimensional representation,\n",
    "2. sparse representation (increase dimension),\n",
    "3. independent representation.\n",
    "\n",
    "### PCA\n",
    "\n",
    "Given design matrix $X$, centered with $\\mathbb{E}(x)=0$.\n",
    "\n",
    "Unbiased Sample Covariance associated with X is:\n",
    "\n",
    "$$ Var(x) = \\frac{1}{m-1}X^T X $$\n",
    "\n",
    "PCA finds a representation (through orghogonal, linear transformations) $z=W^T x$ where $Var(z)$ is diagonal (implies zero covariance of $z$).\n",
    "\n",
    "Two ways to find the Principal Components, $W$:\n",
    "\n",
    "* With Eigenvalue Decomposition: $X^T X = W \\Lambda W^T$\n",
    "\n",
    "* With SVD: $X=U \\Sigma W^T$, the **principal components** are the right singular vectors of $X$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^T X &= (U \\Sigma W^T)^T U \\Sigma W^T \\\\\n",
    "&= W\\Sigma^T U^T U \\Sigma W^T \\\\\n",
    "\\because U \\text{ is orthogonal, } U^T U &= I \\\\\n",
    "X^T X &= W \\Sigma^2 W^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can show $Var(z)$ is diagonal:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z &= XW \\\\\n",
    "Var(z) &= \\frac{1}{m-1} Z^T Z \\\\\n",
    "&= \\frac{1}{m-1} (XW)^T X W \\\\\n",
    "&= \\frac{1}{m-1} W^T X^T X W \\\\\n",
    "\\because X^T X &= W \\Sigma^2 W^T \\\\\n",
    "Var(z) &= W^T W \\Sigma^2 W^T W \\\\\n",
    "\\because W^T W = I \\\\\n",
    "Var(z) &= \\Sigma^2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "\n",
    "Uses **minibatch** of $m'$, typically $m'$ ranges from 1 to a few 100s. Crucially, $m'$ is usually held fixed as the training set size $m$ grows.\n",
    "\n",
    "The number of updates required to reach convergence usually increases with training set size. However, as $m$ approaches infinity, the model will eventually converge to its best possible test error before SGD has sampled every sample in the training set. \n",
    "\n",
    "## ML Algo Building Blocks:\n",
    "\n",
    "Four pieces:\n",
    "1. dataset\n",
    "2. cost function\n",
    "3. optimization procedure\n",
    "4. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
