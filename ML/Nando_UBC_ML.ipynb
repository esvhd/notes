{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Nadno de Fritas UBC ML Lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lecture Videos: https://www.youtube.com/playlist?list=PLE6Wd9FR--Ecf_5nCbnSQMHqORpiChfJf\n",
    "\n",
    "Website: http://www.cs.ubc.ca/~nando/340-2012/lectures.php\n",
    "\n",
    "Google Group: https://groups.google.com/forum/#!forum/cpsc340-2012\n",
    "\n",
    "Latex symbols: https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols\n",
    "\n",
    "Equations: https://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lecture Notes 10\n",
    "\n",
    "## Question on slide 9, $P(M=1 \\mid T=0)$?\n",
    "\n",
    "To continue from **slide 5**, the probabilities are:\n",
    "\n",
    "$P( T \\mid F=1, S=0, \\beta_{2} ) = \\beta^1_2 (1-\\beta_{2})^1$\n",
    "\n",
    "$P( T \\mid F=0, S=1, \\beta_{3} ) = \\beta^1_3 (1-\\beta_{3})^0$\n",
    "\n",
    "$P( T \\mid F=1, S=1, \\beta_{4} ) = \\beta^1_4 (1-\\beta_{4})^0$\n",
    "\n",
    "With **Maximum Likelihood**, I get:\n",
    "\n",
    "$\\beta_{2,ML} = \\frac{1}{2}$\n",
    "\n",
    "$\\beta_{3,ML} = \\frac{1}{1} = 1$\n",
    "\n",
    "$\\beta_{4,ML} = \\frac{1}{1} = 1$\n",
    "\n",
    "Also to complete **slide 6**: \n",
    "\n",
    "$\\gamma_{2,ML} = \\frac{3}{4}$\n",
    "\n",
    "Then we have all the numbers we need to complete the tables on **slide 4**.\n",
    "\n",
    "Assuming all priors are Beta(1,1), with **posterior mean**, E.g.:\n",
    "\n",
    "$P(\\theta) = Beta(1,1) \\propto \\theta^{(1-1)}(1-\\theta)^{(1-1)}$?\n",
    "\n",
    "Since all priors are Beta(1,1), we get the following, with probablities on slide 5:\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "P(\\theta|M) = P(M|\\theta)P(\\theta) \\propto \\theta^4(1-\\theta)^1\\theta^0(1-\\theta)^0 \\Rightarrow \\mathbb{E}(\\theta|M)= \\frac{5}{5+2}=\\frac{5}{7}\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "And therefore:\n",
    "\n",
    "$\\mathbb{E}(\\alpha \\mid S)=\\frac{3}{4}$\n",
    "\n",
    "$\\mathbb{E}(\\gamma_1 \\mid F,M=0)=\\frac{1}/{1+2}=\\frac{1}{3}$\n",
    "\n",
    "$\\mathbb{E}(\\gamma_2 \\mid F,M=1)=\\frac{4}/{4+2}=\\frac{2}{3}$\n",
    "\n",
    "$\\mathbb{E}(\\beta_1 \\mid T,F=0,S=0)=\\frac{1}{1+2}=\\frac{1}{3}$\n",
    "\n",
    "$\\mathbb{E}(\\beta_2 \\mid T,F=1,S=0)=\\frac{2}{2+2}=\\frac{1}{2}$\n",
    "\n",
    "$\\mathbb{E}(\\beta_3 \\mid T,F=0,S=1)=\\frac{2}{2+1}=\\frac{2}{3}$\n",
    "\n",
    "$\\mathbb{E}(\\beta_4 \\mid T,F=1,S=1)=\\frac{2}{2+1}=\\frac{2}{3}$\n",
    "\n",
    "Now we have all parameters needed for filling the tables on **slide 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Matrix Algebra & Calculus\n",
    "\n",
    "http://www.statpower.net/Content/312/Handout/Matrix.pdf\n",
    "\n",
    "\n",
    "## Properties of Transposition\n",
    "\n",
    "1. $(A^{'})^{'} = A$\n",
    "2. $(cA)^{'} = cA^{'}$\n",
    "3. $(A + B)^{'} = A^{'} + B^{'}$\n",
    "4. $(AB)^{'} = B^{'}A^{'}$\n",
    "5. $A^{'}B = B^{'}A$\n",
    "6. $(X^{-1})^{'} = (X^{'})^{-1}$\n",
    "\n",
    "## Other Properties\n",
    "\n",
    "* If $x^{-1} == x^{T}$ then $x$ is **orthgonal**.\n",
    "\n",
    "## Calculus\n",
    "\n",
    "Proof: http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf\n",
    "\n",
    "| y | $\\frac{\\partial{y}}{\\partial{x}}$ |\n",
    "|:---:|:---------------:| \n",
    "| $Ax$ | $A^{T}$ |\n",
    "| $x^{T}A$ | $A$ |\n",
    "| $x^{T}x$ | $2x$ |\n",
    "| $x^{T}Ax$ | $Ax + A^{T}x$, if $A$ is **symmetric**, $2Ax$ |\n",
    "\n",
    "| $\\alpha$ | $\\frac{\\partial{\\alpha}}{\\partial{x}}$ | $\\frac{\\partial{\\alpha}}{\\partial{y}}$ |\n",
    "|:---: |: --- :|: --- :|\n",
    "| $y^{T}Ax$ | $y^{T}A$ | $x^{T}A^{T}$ |\n",
    "\n",
    "| $\\alpha$  | $\\frac{\\partial{\\alpha}}{\\partial{z}}$ can't figout out how to make it wider|\n",
    "|:---: |: --- :|\n",
    "| $y^{T}x$ | $x^{T}\\frac{\\partial{y}}{\\partial{z}} + y^{T}\\frac{\\partial{x}}{\\partial{z}}$ |\n",
    "| $y^{T}Ax$ | $x^{T}A^{T}\\frac{\\partial{y}}{\\partial{z}} + y^{T}A\\frac{\\partial{x}}{\\partial{z}}$ |\n",
    "| $x^{T}x$ | $2x^{T}\\frac{\\partial{x}}{\\partial{z}}$ |\n",
    "| $x^{T}Ax$ | $x^{T}(A + A^{T})\\frac{\\partial{x}}{\\partial{z}}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Trace of Matrices\n",
    "\n",
    "Kevin Murphy's book, p101\n",
    "\n",
    "Trace of matrix $A$ is the sum of its diagonal elements.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "tr(A) =& \\sum_{i} A_{ii} \\\\\n",
    "tr(ABC) =& tr(CAB) = tr(BCA) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Trace trick** last equation above is called the **cyclic permutation property** of the trace operator. Using this we derive the trace trick, which reorders the scaler inner product $x^TAx$:\n",
    "\n",
    "$$x^TAx = tr(x^TAx) = tr(xx^TA) = tr(Axx^T) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lecture 19\n",
    "\n",
    "## Linear Regression - Maximum Likelihood Matrix Maths\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ L(\\theta) & = [y - x \\theta]^{T} [y - x \\theta ] \\\\\n",
    "\\ &= [y^{T} - (x\\theta)^{T}][y - x\\theta] \\\\\n",
    "\\ & = [y^{T} - \\theta^{T}x^{T}][y - x\\theta] \\\\\n",
    "\\ & = y^{T}y - y^{T}x\\theta - \\theta^{T}x^{T}y + \\theta^{T}x^{T}x\\theta \\\\\n",
    "\\ & = y^{T}y - 2y^{T}x\\theta +\\theta^{T}x^{T}x\\theta \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that $y^{T}x\\theta == \\theta^{T}x^{T}y$. See numerical example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = np.matrix([[2], [3]])\n",
    "x = np.matrix([[3, 5], [6, 8]])\n",
    "theta = np.matrix([[5], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3, 5],\n",
       "        [6, 8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ True]], dtype=bool)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(y.T @ x @ theta - theta.T @ x.T @ y, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set $\\frac{\\partial{L(\\theta)}}{\\partial{\\theta}} = 0$ for maximum likelihood gives:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\frac{\\partial{L(\\theta)}}{\\partial{\\theta}} = -2y^{T}x + 2x^{T}x\\theta &= 0\\\\\n",
    "\\ -2y^{T}x + 2x^{T}x\\theta &= 0 \\\\\n",
    "\\ \\hat{\\theta} &= (x^{T}x)^{-1}x^{T}y \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Probability Distributions\n",
    "\n",
    "## Normal Distribution\n",
    "\n",
    "Python: `scipy.stats.norm`\n",
    "\n",
    "Given a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\ PDF(x) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\bigg(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\bigg) \\\\\n",
    "\\ CDF(x) &= \\frac{1}{2}\\big[1 + erf\\big(\\frac{x - \\mu}{\\sigma\\sqrt{2}}\\big)\\big] \\\\\n",
    "\\ erf(x) &= \\frac{1}{\\sqrt{\\pi}}\\int_{-x}^{x}e^{-t^2}dt = \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{x}e^{-t^2}dt\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Control of Precision**: when we need to evaluate the PDF many times, we set $\\beta = \\frac{1}{\\sigma^2}$:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\ PDF(x) &= \\sqrt{\\frac{\\beta}{2\\pi}} \\exp\\bigg(-\\frac{1}{2}\\beta(x-\\mu)^2\\bigg) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Beta Distribution\n",
    "\n",
    "https://en.wikipedia.org/wiki/Beta_distribution\n",
    "\n",
    "Python: `scipy.stats.beta`\n",
    "\n",
    "In Bayesian inference, the beta distribution is the **conjugate prior** probability distribution for the **Bernoulli, binomial, negative binomial and geometric distributions**. \n",
    "\n",
    "Given $Beta(\\alpha, \\beta)$:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\ \\Gamma(x) &= (n - 1)! \\, \\forall n \\in \\text{positive integers, i.e. }\\mathbb{N}^+ \\\\\n",
    "\\ \\Gamma(z) &= \\int_{0}^{\\infty}x^{z-1}e^{-x}dx \\\\\n",
    "\\ B(\\alpha, \\beta) &= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\\\\n",
    "\\ PDF(x) &= \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)} \\\\\n",
    "\\ CDF(x) &= I_x(\\alpha, \\beta) = \\frac{B(x; \\alpha,\\beta)}{B(\\alpha, \\beta)} \\\n",
    " = \\frac{\\int_{0}^{\\infty}t^{\\alpha-1}(1-t)^{\\beta-1}dt}{B(\\alpha, \\beta)} \\\\\n",
    "\\ \\mathbb{E}[Beta(\\alpha, \\beta)] &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\n",
    "\\ var(Beta(\\alpha, \\beta) &= \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bernoulli Distribution\n",
    "\n",
    "See **Nando's lecture notes 19** for a good discussion on its application.\n",
    "\n",
    "Given $ 0 < p < 1, p \\in \\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ PDF(p,k) &= p^k(1-p)^{1-k}, &k \\in \\{0, 1\\} \\\\\n",
    "\\ CDF(p,k) &= \n",
    "\\begin{cases}\n",
    "0 &k < 0 \\\\\n",
    "1-p &0 \\leq k < 1 \\\\\n",
    "1 &k \\geq 1 \n",
    "\\end{cases} \\\\\n",
    "\\ Mean = p \\\\\n",
    "\\ Variance = p(1-p)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Binomial Distribution\n",
    "\n",
    "Given $B(n, p), n \\in \\mathbb{N}_0, p \\in [0, 1]$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "PDF &= \\binom{n}{k} p^k(1-p)^{n-k} \\\\\n",
    "CDF &= \\sum_{i=0}^{\\lfloor k \\rfloor} \\binom{n}{i} p^i(1-p)^{n-i} = I_{1-p}(n-k, 1+k) \\\\\n",
    "Mean &= np \\\\\n",
    "Variance &= np(1-p)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lecture Notes 15\n",
    "\n",
    "## Multivariate Normal Distribution\n",
    "\n",
    "Let $y \\in \\mathbb{R}^{n\\times1}$, then the PDF of a n-dimensional multivariable normal distribution is (only when $\\Sigma$ is **positive-definite**):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "pdf(y) &= (2\\pi)^{-n/2}\\det(\\Sigma)^{-1/2}\\exp\\bigg[-\\frac{1}{2}(y-\\mu)^T\\Sigma^{-1}(y-\\mu)\\bigg] \\\\\n",
    "\\mu &= \\mathbb{E}(y) \\\\\n",
    "\\Sigma &= \\mathbb{E}[(y-\\mu)^T(y-\\mu)] = \\text{covariance} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Mahalanobis distance** measures the distance betwwen a data vector $x$ and the mean vector $\\mu$, It is the expression inside the exponential in the multivariate normal PDF formula.\n",
    "\n",
    "**Control of Precision**: when we need to evaluate the PDF many times, we need to invert $\\Sigma$ first and assign as $\\beta$: See Deep Learning (Goodfellow) p63.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "pdf(y) &= (2\\pi)^{-n/2}\\det(\\Sigma)^{1/2}\\exp\\bigg[-\\frac{1}{2}(y-\\mu)^T\\beta(y-\\mu)\\bigg] \\\\\n",
    "\\mu &= \\mathbb{E}(y) \\\\\n",
    "\\Sigma &= \\mathbb{E}[(y-\\mu)^T(y-\\mu)] = \\text{covariance} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Eigenvector & Eigenvalues\n",
    "\n",
    "Deep Learning (Goodfellow)\n",
    "\n",
    "An **eigenvector** of a **square** matrix $A$ is a nonzero vector $v$ such that:\n",
    "$$ Av = \\lambda v $$\n",
    "Where $\\lambda$ is a constant scaler, known as the **eigenvalue** of this corresponding eigenvector.\n",
    "\n",
    "Concatenating all the eigenvectors to form a matrix $V$ with one eigenvector per column, and eigenvalues into a diagonal matrix:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V =& \\big[v^{(1)},\\dots,v^{(n)}\\big] \\\\\n",
    "D =& \\text{diag}([\\lambda_1, \\dots, \\lambda_n])\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We have the **Eigen-decomposition**:\n",
    "\n",
    "$$ A = V D V^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Properties of Eigen-decomposition\n",
    "\n",
    "$V$ above is an **orthogonal matrix** of eigenvectors, satisfying $V^{-1} = V^T$ and $V^T V=I$\n",
    "\n",
    "$D$ is a diagonal, square matrix of eigenvalues\n",
    "\n",
    "Note the inverse of a diagonal matrix exists only if every diagonal entry is nonzero. In this case:\n",
    "$$ diag(v)^{-1} = [1/v_1, \\dots, 1/v_n]^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Linear Dependence / Singular Matrix\n",
    "\n",
    "A set of vectors is **linearly independent** if no vector in the set is a linear combination of the other vectors.\n",
    "\n",
    "A **square** matrix with linearly dependent columns is know as **singular**.\n",
    "\n",
    "For square matrices the left inverse and right inverse are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Matrix Fractional Power\n",
    "\n",
    "http://math.stackexchange.com/questions/732511/fractional-power-of-matrix \n",
    "\n",
    "If a matrix is diagonalizable, first diagonalize it (**eigen-decomposition**), apply the power then convert back.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A &= VDV^{-1} \\\\\n",
    "A^n &= VD^nV^{-1} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "See **power of matrices** section at https://en.wikipedia.org/wiki/Matrix_multiplication\n",
    "\n",
    "**For a diagonal matrix $A$, $A^n$ is just raising the diagonal elements of $A$ to the power of $n$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SVD\n",
    "\n",
    "Deep Learning (Goodfellow)\n",
    "\n",
    "Assume matrix $A$ is $m \\times n$, SVD is\n",
    "\n",
    "$$ A = UDV^{T} $$\n",
    "\n",
    "where:\n",
    "\n",
    "1. $U$ is $m \\times m$, orthogonal, columns are **left-singular vectors** of $A$\n",
    "2. $D$ is $m \\times n$, diagonal matrix of **singular values** of $A$\n",
    "3. $V$ is $n \\times n$, orthogonal, columns are **right-singular vectors** of $A$\n",
    "\n",
    "### Other Properties\n",
    "\n",
    "1. $U$ are the eigenvectors of $AA^T$\n",
    "2. $V$ are the eigenvectors of $A^T A$\n",
    "3. the nonzero singular values of $A$ are the square roots of eigenvalues of $A^T A$, same is true for $AA^T$\n",
    "\n",
    "### Moore-Penrose Pseudoinverse\n",
    "\n",
    "Solving equations: $Ax = y$, given $A$ is $n \\times m$.\n",
    "\n",
    "1. $n > m$ - possible that there is no solution\n",
    "2. $n < m$ - could be multiple solutions\n",
    "\n",
    "Pseudoinverse definition:\n",
    "\n",
    "$$ A^+ = VD^+U^T $$\n",
    "\n",
    "where $U$, $D$, $V$ are coming from the SVD of $A$, the pseudoinverse $D^+$ of diagonal matrix $D$ is obtained by taking the reciprocal of its nonzero elements then taking the transpose of the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lecture 20 (Notes PDF L16b)\n",
    "\n",
    "Ridge regression is the mean of Bayesian regression (posterior mean), in special case where prior mean == 0.\n",
    "\n",
    "Ridge regression maths essentially gets rid of the small eigenvalues.\n",
    "\n",
    "## Bayesian Linear Regression\n",
    "\n",
    "The likelihood is Gaussian, $\\mathcal{N}(y\\mid X\\theta, \\sigma^{2}I_d)$.\n",
    "\n",
    "The conjugate prior is also Gaussian, $\\mathcal{N}(\\theta \\mid \\theta_{0}, V_{0})$\n",
    "\n",
    "With Bayes rule, the posterior is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ p(\\theta \\mid X, y, \\sigma^{2}) &\\propto \\mathcal{N}(\\theta \\mid \\theta_{0}, V_{0}) \\mathcal{N}(y \\mid X\\theta, \\sigma^{2}I) = \\mathcal{N}(\\theta | \\theta_{n}, V_{n}) \\\\\n",
    "\\ \\theta_{n} &= V_{n}V_{0}^{-1}\\theta_{0} + \\frac{1}{\\sigma^2}V_{n}X^{T}y \\\\\n",
    "\\ V_{n}^{-1} &= V_{0}^{-1} + \\frac{1}{\\sigma^2}X^{T}X \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## Slide title Bayesian versus ML plugin prediction, minute 35:00\n",
    "\n",
    "For the Bayesian, the posterior is the new prior, $\\mathcal{N}(\\theta \\mid \\theta_{n}, V_{n})$. \n",
    "\n",
    "In the special case where:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\theta_0 &= 0 \\\\\n",
    "\\ V_0 &= \\tau^2 I_d \\\\\n",
    "\\ \\text{define: } \\lambda &= \\frac{\\sigma^2}{\\tau^2} \\\\\n",
    "\\ \\text{Posterior mean: } \\theta_{n} &= (X^{T}X + \\lambda I_d)^{-1}X^{T}y \\\\\n",
    "\\ \\text{Posterior variance: } V_{n} &= \\sigma^{2}(X^{T}X + \\lambda I_d)^{-1} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Bayesian linear regression produces difference variance to Maximum Likelihood method. \n",
    "\n",
    "Implication is that Bayesian predictions have **lower** variance in places where it has seen data, but **higher** variance where it has not seen data before. This is shown by the variance term for Bayesian prediction, which has a data depended term associated with $V_{n}$, which is essentially the **inverse of data matrix**.\n",
    "\n",
    "Bayesian, given $D = (X, y)$ is the data matrix:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ P(y|x_{*}, D, \\sigma^{2}) &= \\mathcal{N}(y \\mid x_{*}^{T}\\theta_{n}, \\sigma^{2} + x_{*}^{T}V_{n}x_{*}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Also the posterial is proper if $n > k$ where $n$ is the number of rows and $k$ the number of features, and $rank(X) == k$. Ref: https://www.youtube.com/watch?v=d1iIUtnDngg, min 4:17\n",
    "\n",
    "Maximum Likelihood:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ P(y|x_{*}, D, \\sigma^{2}) &= \\mathcal{N}(y \\mid x_{*}^{T}\\theta_{ML}, \\sigma^{2}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "This means that Bayesian believes there are many possible values of $\\theta$, they are weighted by their probabilities. \n",
    "\n",
    "When a new **data point** $x_{*}$ comes in, it is evaluated with all possible $\\theta$'s, then weighted by their probabilities again. Posterior $\\propto$ Prior * Likelihood.\n",
    "\n",
    "For Maximum Likelihood, the posterior is a delta function that takes a value at $\\theta_{ML}$, i.e. there is only one solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lecture 21\n",
    "\n",
    "## Sub-differentiation\n",
    "\n",
    "This is the treatment of taking derivative of absolute functions. The idea is to consider all possible slopes (derivatives), and define the solution as a set. Example below. This is also why for **Lasso** some parameters will be **zero**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ J(\\theta) &= \\delta^{2}|\\theta| \\\\\n",
    "\\ \\frac{\\partial{J}}{\\partial{\\theta}} &= \n",
    "\\begin{cases}\n",
    "-1, \\text{if $\\theta < 0$} \\\\\n",
    "\\lbrack -1, 1 \\rbrack, \\text{if $\\theta = 0$} \\\\\n",
    "1, \\text{if $\\theta > 0$}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Coordinate Descent Algorithm for Sparse Prediction\n",
    "\n",
    "In the derivation of derivative of the objective function, for a vector $x$, the notation $x_{i-j}$ refers to all the elements of $x$ except the $j^{th}$ element. Similarly, $x_{j}$ refers to only the $j^{th}$ element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Regression / Stats Notes\n",
    "\n",
    "For OLS regression problems, assume model $y = X\\beta + \\varepsilon$, we have solution for $\\beta$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\hat{\\beta} &= (X^{T}X)^{-1}X^{T}y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let $\\hat{y}$ be the model predicted value of $y$.\n",
    "\n",
    "## Regression Metrics\n",
    "\n",
    "### **RSS**, Residual Sum of Squares, or sometime known as Sum of Square Erros, **SSE**\n",
    "\n",
    "$$RSS = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n}e_i^{2}$$\n",
    "\n",
    "### **MSE**, Mean Sequare Error, or **RSE**, Residual Standard Error\n",
    "\n",
    "For $X$ in the dimention of $(n, p)$, i.e. $n$ observations, $p$ features (degree of freedom = $p + 1$, assuming there is an intercept):\n",
    "\n",
    "$$MSE = \\frac{RSS}{n - p - 1}$$\n",
    "\n",
    "$$RSE = RMSE = \\sqrt{MSE}$$\n",
    "\n",
    "**R code**, degree of freedum is given by: `summary(model)$sigma`\n",
    "\n",
    "### **TSS**, Total Sum of Squares\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i$$\n",
    "\n",
    "$$TSS = \\sum_{i=1}^{n}(y_i - \\bar{y}_i)^2$$\n",
    "\n",
    "### $R^2$, Adjusted $R^2$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ R^2 &= \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS} \\\\\n",
    "\\ \\\\\n",
    "\\ Adjusted.R^2 &= 1 - \\frac{RSS/(n-p-1)}{TSS/(n-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### F-test of significance of all parameters. ISLR, p77\n",
    "\n",
    "Look up for p-value in F-table for degree of freedum of $(p, n-p-1)$ (columns, rows). \n",
    "\n",
    "NULL hypothesis $H_0$ is that all parameters are zero.\n",
    "\n",
    "$$F = \\frac{(TSS-RSS) / p}{RSS / (n-p-1)}$$\n",
    "\n",
    "### F-test of two models\n",
    "\n",
    "Consider a large model of $p$ features, and a smaller model of a subset of $p$ features, say $q$ features. F-test can be used to see which model is better. \n",
    "\n",
    "NULL hypothesis $H_0$ is that the smaller model is better. \n",
    "\n",
    "$$F = \\frac{(RSS_q - RSS_p)/q}{RSS_p / (n-p-1)}$$\n",
    "\n",
    "\n",
    "### $R^2$ and Correlation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ R^2 = \n",
    "\\begin{cases}\n",
    "p = 1, \\text{corr($y$, $x$)^2} \\\\\n",
    "p > 1, \\text{corr($y$, $\\hat{y}$)^2} \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model selection Metrics for p > 1\n",
    "\n",
    "Model selection should be done using metrics on **test data, not training data**.\n",
    "\n",
    "### $C_{p}$, choose model with lowest value\n",
    "\n",
    "$$C_{p} = \\frac{1}{n}(RSS + 2p\\hat{\\sigma}^2)$$\n",
    "\n",
    "Where $\\hat{\\sigma}^2$ is the estimated residual variance.\n",
    "\n",
    "\n",
    "### AIC, lower value better\n",
    "\n",
    "$$AIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS + 2p\\hat{\\sigma}^2)$$\n",
    "\n",
    "### BIC, places heavier penalty on models with higher dimensions. \n",
    "\n",
    "$$BIC = \\frac{1}{n}(RSS + \\log{(n)}p\\hat{\\sigma}^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##  Information Theory and Deviance\n",
    "\n",
    "(Statistical Rethinking)\n",
    "\n",
    "**Information Entropy**: for $n$ possible events and each event $i$ has probability $p_i$, then the unique measure of uncertainty is:\n",
    "\n",
    "$$ H(p) = -\\mathbb{E}[log(p_i)] = - \\sum_{i=1}^{n}p_{i} log(p_{i}) $$\n",
    "\n",
    "**Kullback-Leibler (K-L) Divergence**: the divergence is the average difference in log probability between the target ($p$) and model ($q$). Where $p$ and $q$ are parameters of the **true model** and our estimated model respectively. In otherwords, it is defined as the **additional** entropy induced by using $q$. \n",
    "\n",
    "$$ D_{KL} = \\sum p_{i}[log(p_i) - log(q_i)] = \\sum_{i} p_i log(\\frac{p_i}{q_i}) $$\n",
    "\n",
    "**Cross Entropy**: Using a probability distribution $q$ to predict events from another distribution $p$:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\ H(p, q) &= - \\sum_{i}p_i log(q_i) \\\\\n",
    "\\ D_{KL}(p, q) &= H(p, q) - H(p) \\\\\n",
    "\\ &= - \\sum_{i=1}^{n}p_{i} log(q_{i}) - (- \\sum_{i=1}^{n}p_{i} log(p_{i})) \\\\\n",
    "\\ &= - \\sum_{i=1}^{n}p_{i} (log(q_{i}) - log(p_{i}))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "However, in real world, the true model, $p$, is usually unknown. Therefore **Deviance** is defined as:\n",
    "\n",
    "$$ D(q) = - 2\\sum_{i} log(q_i) $$\n",
    "\n",
    "`R` function `-2 * logLik()` \n",
    "\n",
    "**AIC** provides an estimate of average out-of-sample deviance:\n",
    "\n",
    "$$ AIC = D_{train} + 2p $$\n",
    "\n",
    "where $p$ is the number of free parameters to be estimated in the model.\n",
    "\n",
    "** Deviance Information Criterion (DIC) ** \n",
    "\n",
    "** Widely Applicable Information Criterion **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Accurancy of Sample Mean $\\hat{\\mu}$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\hat{SE}(\\hat{\\mu}) &= \\frac{\\sigma^{2}}{n} \\\\\n",
    "\\ \\hat{\\mu} &= \\frac{1}{n}\\sum_{i=1}^{n}x_i \\\\\n",
    "\\ \\sigma^2 &= variance(x) \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hierarchical Principal, ISLR, p89\n",
    "\n",
    "If we include an interaction in a model, we should also include the main effects, even if the p-value associated with their coefficient are not significant. E.g. if $X_1 \\times X_2$ seems important, the model should include both $X_1$ and $X_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Confidence Intervals, p = 1\n",
    "\n",
    "CI - Confidience Interval of model prediction **on average**, where $\\alpha$ is the probability, e.g. 95%:\n",
    "\n",
    "$$CI=\\hat{y}_n \\pm t_{(\\alpha/2, n-p-1)} \\times \\sqrt{MSE * (\\frac{1}{n} + \\frac{(x_h - \\bar{x})^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2})}$$\n",
    "\n",
    "$t_{(\\alpha/2, n-p-1)}$ is based on Normal distrubtion table.\n",
    "\n",
    "Conditions for CI\n",
    "\n",
    "1. $x_h$ is within the range of training data, i.e. within scope of the model\n",
    "2. **LINE**: Linear, Independent errors, normal errors, equal variance. Still works if error is approximately Normal. Or, if $n$ is large, error can deviate substantially from normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prediction Interval, p = 1\n",
    "\n",
    "PI - Prediction Interval, for point predictions, where $\\alpha$ is the probability.\n",
    "\n",
    "$$PI = \\hat{y}_h \\pm t_{(\\alpha/2, n-p-1)} \\times \\sqrt{MSE \\times (1 + \\frac{1}{n} + \\frac{(x_h - \\bar{x})^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2})}$$\n",
    "\n",
    "$t_{(\\alpha/2, n-p-1)}$ is based on Normal distrubtion table.\n",
    "\n",
    "Conditions for PI\n",
    "\n",
    "1. $x_h$ is within the range of training data, i.e. within scope of the model\n",
    "2. **LINE**: Linear, Independent errors, normal errors, equal variance. Strongly depends on errors being normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Confidence & Prediction Intervals where p > 1\n",
    "\n",
    "Reference:\n",
    "\n",
    "* https://onlinecourses.science.psu.edu/stat501/node/314\n",
    "\n",
    "* https://onlinecourses.science.psu.edu/stat501/node/315\n",
    "\n",
    "Let $X_n = (1, x_{n,1}, x_{n, 2}, \\ldots, x_{n, p})^T$, $X_h$ is an observation.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ CI &= \\hat{y}_h \\pm t_{(\\alpha/2, n-p-1)} \\times SE(\\hat{y}_h) \\\\\n",
    "\\ \\\\\n",
    "\\ PI &= \\hat{y}_h \\pm t_{(\\alpha/2, n-p-1)} \\times \\sqrt{MSE + SE(\\hat{y}_h)^2} \\\\\n",
    "\\ \\\\\n",
    "\\ SE(\\hat{y}_h)^2 &= MSE \\times X_{h}^T(X^{T}X)^{-1}X_{h} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Python \n",
    "http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLSResults.html\n",
    "\n",
    "Confidence Interval: ```OLSResults.conf_int()```\n",
    "\n",
    "Prediction Interval: \n",
    "http://markthegraph.blogspot.de/2015/05/using-python-statsmodels-for-ols-linear.html\n",
    "\n",
    "```{python}\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "prstd, iv_l, iv_u = wls_prediction_std(re)\n",
    "```\n",
    "\n",
    "See source:\n",
    "https://github.com/statsmodels/statsmodels/blob/master/statsmodels/sandbox/regression/predstd.py\n",
    "\n",
    "### R\n",
    "\n",
    "`predict` or `predict.lm`. check out `help(predict.lm)` and see `internal` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Variance of $\\hat{\\beta}$\n",
    "\n",
    "Let:\n",
    "\n",
    "$$ \\sigma^2 = MSE = RSE^2 $$\n",
    "\n",
    "### I.I.D Errors\n",
    "\n",
    "Based on OLS model formula above, subsituting in for $y$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\hat{\\beta} & = (X^{T}X)^{-1}X^{T}y \\\\\n",
    "\\ &= (X^{T}X)^{-1}X^{T}(X\\beta + \\varepsilon) \\\\\n",
    "\\ &= (X^{T}X)^{-1}X^{T}X\\beta + (X^{T}X)^{-1}X^{T}\\varepsilon \\\\\n",
    "\\ &= \\beta + (X^{T}X)^{-1}X^{T}\\varepsilon\\\\\n",
    "\\ \\\\\n",
    "\\ var(\\hat{\\beta}) &= \\mathbb{E}[(X^{T}X)^{-1}X^{T}\\varepsilon\\varepsilon^{T}X(X^{T}X)^{-1}] \\\\\n",
    "\\ &= (X^{T}X)^{-1}\\mathbb{E}[X^{T}\\varepsilon\\varepsilon^{T}X](X^{T}X)^{-1}\\\\\n",
    "\\ &= (X^{T}X)^{-1}X^{T}\\mathbb{E}[\\varepsilon\\varepsilon^{T}]X(X^{T}X)^{-1}\\\\\n",
    "\\ \\because \\varepsilon &\\sim \\mathcal{N}(0, \\sigma^2) \\\\\n",
    "\\ \\therefore \\mathbb{E}[\\varepsilon\\varepsilon^{T} \\mid X] &= \\Omega = \\sigma^2 I \\\\\n",
    "\\ &= (X^{T}X)^{-1}X^{T}\\Omega X(X^{T}X)^{-1}\\\\\n",
    "\\ &= \\sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1}\\\\\n",
    "\\ var(\\hat{\\beta}) &= \\sigma^{2}(X^{T}X)^{-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### I.N.I.D Errors (Heteroskedasticity)\n",
    "\n",
    "To deal with uncorrelationed residuals\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ var(\\hat{\\beta}) &= (X^{T}X)^{-1}X^{T}X\\mathbb{E}[\\varepsilon\\varepsilon^{T}](X^{T}X)^{-1}\\\\\n",
    "\\ \\because \\mathbb{E}[\\varepsilon\\varepsilon^{T}] &= \\sigma^{2}\\Omega \\\\\n",
    "\\ var(\\hat{\\beta}) &= (X^{T}X)^{-1}X^{T}\\sigma^{2}\\Omega X(X^{T}X)^{-1}\\\\\n",
    "\\ var(\\hat{\\beta}) &= \\sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1}\\\\\n",
    "\\ \\therefore \\hat{\\beta} &\\sim \\mathcal{N}(\\beta, \\sigma^{2}(X^{T}X)^{-1}X^{T}\\Omega X(X^{T}X)^{-1})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### N.I.N.I.D Errors (HAC)\n",
    "\n",
    "To deal with correlated residuals, use Newey West. See video below for details and Greene's book, p517 - p518.\n",
    "\n",
    "https://www.youtube.com/watch?v=HznGehi6xNQ \n",
    "\n",
    "**Newey West essentially has a modified / weighted variance-covariance matrix $\\Omega_{NW}$.**\n",
    "\n",
    "Essentially applying weights $\\omega$ diagonally to $\\Omega$, example:\n",
    "\n",
    "$$\n",
    "\\Omega_{NW} = \n",
    " \\begin{pmatrix}\n",
    "  e_{1}e_{1} & \\omega_{1}e_{1}e_{2} & \\omega_{2}e_{1}e_{3} & 0 & \\cdots & 0 \\\\\n",
    "  \\omega_{1}e_{1}e_{2} & e_{2}e_{2} & \\omega_{1}e_{2}e_{3} & \\omega_{2}e_{2}e_{4} & \\cdots & 0 \\\\\n",
    "  \\omega{2}e_{1}e_{3} & \\omega_{1}e_{2}e_{3} & e_{3}e_{3} & \\omega_{1}e_{3}e_{4} & \\cdots & 0 \\\\\n",
    "  0 & \\omega_{2}e_{2}e_{4} & \\omega_{1}e_{3}e_{4} & e_{4}e_{4} & \\cdots & 0 \\\\\n",
    "  \\vdots  & \\vdots  & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "  0 & 0 & 0 & \\cdots &\\omega_{1}e_{n-1}e_{n} & e_{n}e_{n}\n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "With $0 < \\omega_1 < \\omega_2 < \\cdots < \\omega_n < 1$\n",
    "\n",
    "\n",
    "### Newey West \n",
    "\n",
    "http://stackoverflow.com/questions/23420454/newey-west-standard-errors-for-ols-in-python\n",
    "\n",
    "In `R`, `NeweyWest()` in the `sandwich` package use Newey West 1994 paper to automatically select the lag. \n",
    "\n",
    "In `Python`, currently there isn't a way to automatically select the lag. Some python packages, such as `arch`, use a default value of $4(n/100)^{2/9}$ where $n$ is the length of data, i.e. nobs.\n",
    "\n",
    "**`R` code to test $\\hat{\\beta} == 0$ with Newey West vcov matrix for correlated residuals**:\n",
    "\n",
    "```{R}\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "\n",
    "lm.fit <- lm(y~x)\n",
    "coeftest(lm.fit, vcov. = NeweyWest)\n",
    "\n",
    "```\n",
    "\n",
    "Or manually:\n",
    "\n",
    "```{R}\n",
    "lm.fit <- lm(y~x)\n",
    "# IID case\n",
    "# std_err <- sqrt(diag(vcov(lm.fit)))\n",
    "std_err <- sqrt(diag(NeweyWest(lm.fit)))\n",
    "tstat <- coef(lm.fit) / std_err\n",
    "p_vals <- pt(abs(tstat), df=df.residuals(lm.fit), lower.tail=FALSE)\n",
    "```\n",
    "\n",
    "** `R` code to get $var(\\hat{\\beta})$ **:\n",
    "\n",
    "```\n",
    "var_beta <- vcov(lm.fit)\n",
    "# or\n",
    "x <- lm.fit.matrix(~V1+V2, data=df)\n",
    "var_beta <- summary(lm.fit)$sigma^2 * solve(t(x) %*% x)\n",
    "```\n",
    "\n",
    "**`Python` code for Newey West**:\n",
    "\n",
    "```{python}\n",
    "ols = sm.ols(...).fit(cov_type='HAC',cov_kwds={'maxlags':1})\n",
    "ols.summary()\n",
    "# or\n",
    "ols = sm.ols(...).fit()\n",
    "ols2 = ols.get_robustcov_results(cov_type='HAC',maxlags=1)\n",
    "ols2.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Formulation $y = X\\beta + \\lambda I \\beta^{T}\\beta + \\varepsilon$.\n",
    "\n",
    "OLS should not be used when $n < p$, i.e. when nobs < no. of features.\n",
    "\n",
    "Generally when the relationship is close to linear, least square methods may have **high variance and low bias**. **Ridge regression outperforms when OLS estimates have high variance, or when $p > n$ as OLS cannot be used.**\n",
    "\n",
    "Unlike OLS, whose coefficents are **scale equivariant**, ridge regression coefficients can vary significantly when variables are scaled. **Therefore, it is best to apply ridge regresion after standardising the predictors to have standard deviation of 1**, see ISRL p217:\n",
    "\n",
    "$$ \\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_{ij}-\\bar{x}_{j})}}$$\n",
    "\n",
    "Derivation of $\\hat{\\beta}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ RSS &= (y - X\\beta)^{T}(y - X\\beta) + \\lambda I \\beta^{T} \\beta \\\\\n",
    "\\ &= y^{T}y - y^{T}X\\beta - \\beta^{T}X^{T}y + \\beta^{T}X^{T}X\\beta + \\lambda I \\beta^{T} \\beta \\\\\n",
    "\\frac{\\partial{RSS}}{\\partial{\\beta}} &= 0 - y^{T}X - X^{T}y + 2X^{T}X\\beta + 2\\lambda I \\beta  = 0\\\\\n",
    "\\therefore (2X^{T}X + 2\\lambda I)\\beta &= 2X^{T}y \\\\\n",
    "\\ (X^{T}X + \\lambda I)\\beta &= X^{T}y \\\\\n",
    "\\ \\beta &= (X^{T}X + \\lambda I)^{-1}X^{T}y \\\\\n",
    "\\ \\\\\n",
    "\\ var(\\hat{\\beta}) & = \\sigma^{2}\\mathbb{W}(X^{T}X)\\mathbb{W} \\\\\n",
    "\\ \\mathbb{W} &= (X^{T}X + \\lambda I)^{-1} \\\\\n",
    "\\ Bias(\\hat{\\beta}) &= -\\lambda \\mathbb{W}\\beta \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Variance formula & useful links: \n",
    "\n",
    "http://web.as.uky.edu/statistics/users/pbreheny/764-F11/notes/9-1.pdf\n",
    "\n",
    "http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf\n",
    "\n",
    "https://onlinecourses.science.psu.edu/stat857/node/155\n",
    "\n",
    "### Confidence & Prediction Intervals\n",
    "\n",
    "See:\n",
    "\n",
    "http://stats.stackexchange.com/questions/13329/calculate-prediction-interval-for-ridge-regression\n",
    "\n",
    "http://stackoverflow.com/questions/39750965/confidence-intervals-for-ridge-regression\n",
    "\n",
    "Due to the ridge being a biased estimator, Bootstrapping seems to be the best way to estimate prediction intervals here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ridge vs Laso, ISRL p224\n",
    "\n",
    "One does not dominate the other, use CV in practice to see which works better, depends on the true relationship in data.\n",
    "\n",
    "Ridge shrinks parameters by the same proportion.\n",
    "\n",
    "Lasso srhinks each OLS parameter by $\\lambda / 2$. Therefore any parameter less than $\\lambda / 2$ is shrunken to zero. This is known as **soft thresholding**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Linear Regression Diagnostics in `R`\n",
    "\n",
    "## LINE Conditions\n",
    "\n",
    "Reference: **R in Action**, summary of functions on p187.\n",
    "\n",
    "In `R`, you can examine **LINE** with `plot()`:\n",
    "\n",
    "```{R}\n",
    "fit <- lm(weight ~ height, data=women)\n",
    "par(mfrow=c(2, 2))\n",
    "plo(fit)\n",
    "```\n",
    "\n",
    "Many functions in `R`'s `car` package can help:\n",
    "\n",
    "* **L**: Linearity. Look at **Risidual vs Fitted values** plot, residual should be random, should not exhibit patterns.\n",
    "    * **Component plus residual plots** (aka partial residual plots), `crPlots()`.\n",
    "    * solution: transform **feature variables**, try `car::boxTidwell()` suggested power transforms. check for p-values for the variable to see if transform is needed.\n",
    "\n",
    "* **I**: Independence of errors. Can't tell from the plots. \n",
    "    * `durbinWatsonTest()`\n",
    "\n",
    "* **N**: Normality. Look at Q-Q plot. \n",
    "    * `qqPlot()`, plots the studentized residuals (aka studentized deleted residuals or jackknifed residuals) against a t-distribution with `n-p-2` degrees of freedom. (n - nobs, p - no. of features excluding intercept).\n",
    "    * `residplot()` in **R in Action** on page 189.\n",
    "    * solution: Transform **feature variables**. Use `car::powerTransform()`, check p-value for lambda(1) which has no transform to see if a transform is in fact needed.\n",
    "    \n",
    "* **E**: Equal variance / Homoscedasticity. Scale-location plot, sqrt(standardized residual) vs fitted values. \n",
    "    * `ncvTest()`, NULL hypothesis is constant variance. A significant result would support heterscedasticity. \n",
    "    * `spreadLevelPlot()`\n",
    "    * Counter-measure is to transform the **response** with `log()` or `sqrt()`.\n",
    "* **Global Test** package `gvlma`\n",
    "\n",
    "## Multicollinearity\n",
    "\n",
    "Use **Variance Inflation Factor**, `vif()` function in `car` package. General rules is $\\sqrt{vif} > 2$ indicates a multicollinearity problem (R in Action).\n",
    "\n",
    "See ISRL p101-102:\n",
    "\n",
    "VIF is the ratio of the variance $\\hat{\\beta}_j$ when fiting the full model divided by the variance of $\\hat{\\beta}_j$ if fit on its own. Minimum of VIF is 1. **VIF > 5 or 10 indicates problems.**\n",
    "\n",
    "$$ VIF(\\hat{\\beta}_{j}) = \\frac{1}{1 - R^{2}_{X_{j} \\mid X_{-j}}} $$\n",
    "\n",
    "$R^{2}_{X_{j} \\mid X_{-j}}$ is the $R^2$ from a regression of $X_j$ onto all other features. if $R^{2}_{X_{j} \\mid X_{-j}}$ is close to 1, collinearity is present. \n",
    "\n",
    "Two ways to correct for Multicollinearity:\n",
    "* drop one of the problematic features\n",
    "* combine the collinear features into one single feature.\n",
    "\n",
    "## Unusual Observations\n",
    "\n",
    "### Outliers\n",
    "\n",
    "`outlierTest()` - Bonferroni adjusted p-value for the largest absolute studentized residual. **The test needs to be repeated if the largest data point is removed to check for other outliers.**\n",
    "\n",
    "### High Leverage Points\n",
    "\n",
    "These are observations with unusual combination of predictor values, i.e. outliers with regards to other predictors.\n",
    "\n",
    "Compute **leverage statistics (aka. hat statistics)**, `hatvalues()` (**R in Action** p195, also see code plot that uses `identify()` function). \n",
    "\n",
    "In general a hat statistics greater than 0.2 or 0.3 should be examined.\n",
    "\n",
    "Formula from **An Introductino to Statistical Learning with Applications in R, (ISLR)**, p98:\n",
    "\n",
    "$$ h_i = \\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum_{i^{'}=1}^{n}(x_{i^{'}} - \\bar{x})^2} $$\n",
    "\n",
    "$1/n < h_i < 1$, average leverage of all observations is $(p+1)/n$. Any observations with leverage statistics greatly exceeds the average should be checked.\n",
    "\n",
    "In matrix form this is $H = X(X^{'}X)^{-1}X^{'}$, where $X$ is the design matrix, $h_{ii} = H_{ii}$ for $i^{th}$ data point.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Leverage_(statistics)\n",
    "\n",
    "### Influential Observations\n",
    "\n",
    "These have disproportional impact on the values of the model parameters. Identified by:\n",
    "* Cook's distance (D stat) greater than $4/(n-p-1)$. In `R`: `plot(fit, which=4, cook.level=cutoff)`\n",
    "* Added variable plots, `avPlots(fit, ask=FALSE, id.method='identify')` in `car`\n",
    "\n",
    "**An outlier that also has high leverage is particularly dangerous.** See **ISLR** p99.\n",
    "\n",
    "### Influence Plot\n",
    "\n",
    "Combines all the above into `car`'s `influencePlot(fit, id.method='identify')`.\n",
    "\n",
    "## Model Selection \n",
    "\n",
    "* Stepwise regression, `MASS::stepAIC()`\n",
    "* All subsets regression, `leaps::regsubsets()`\n",
    "* Cross validation, `bootstrap::crossval()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Statistical Tests\n",
    "\n",
    "ANOVA analysis is sensitive to outliers. Run `outlierTest()` before the analysis.\n",
    "\n",
    "### Equality of Variances\n",
    "\n",
    "* Barlett's test: `barlett.test()`\n",
    "* Fligner-Killeen: `fligner.test()`\n",
    "* Brown-Frosythe test: `HH::hov()`\n",
    "\n",
    "\n",
    "\n",
    "## Power Analysis\n",
    "\n",
    "**Power** is defined as: $1 - Prob(\\text{Type II Error})$, i.e. it is the probability of finding evidence to reject the NULL hypothesis (finding an effect is there).\n",
    "\n",
    "`pwr` package provides a list of power analysis tests. List of functions in **R in Action** page 242.\n",
    "\n",
    "Useful ones:\n",
    "\n",
    "* t-test: `pwr.t.test()`\n",
    "* correlation: `pwr.r.test()` \n",
    "* linear models: `pwr.f2.test()`\n",
    "\n",
    "# Permutation Test & Bootstrap\n",
    "\n",
    "Packages: \n",
    "\n",
    "* `coin` \n",
    "* `lmPerm`\n",
    "* `logregperm` permutation test for logistic regression\n",
    "* `glmperm` for GLM\n",
    "* `boot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# SVM\n",
    "\n",
    "**SVM tends to do better then logistic regression when the classes are well separated. In the more overlapping regimes, logistics regression is often prefered.** ISRL, p357\n",
    "\n",
    "A great set of tutorials on SVM maths can be found here: \n",
    "\n",
    "http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/\n",
    "\n",
    "## Basic Vector Maths\n",
    "\n",
    "Given a vector $v$ of $p$ dimension, $v=(v_1, v_2, \\ldots, v_p)$ its **magnitude or length or norm** is defined as: \n",
    "\n",
    "$$ \\lVert v \\rVert = \\sqrt{v^2} = \\sqrt{\\sum_{i=1}^{p}(v_i^2)} $$\n",
    "\n",
    "The **direction vector** of the $v$ is given by:\n",
    "\n",
    "$$u = \\frac{v}{\\lVert v \\rVert} = (\\frac{v_1}{\\lVert v \\rVert}, \\frac{v_2}{\\lVert v \\rVert}, \\ldots, \\frac{v_p}{\\lVert v \\rVert})$$ \n",
    "\n",
    "This is can also be derived from trigomitry, where for example in a 2-dimensional vector setting: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\cos(\\theta) &= \\frac{v_1}{\\lVert v \\rVert} \\\\\n",
    "\\ \\sin(\\theta) &= \\frac{v_2}{\\lVert v \\rVert} = \\cos(90^\\circ - \\theta) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Addition and Subtraction\n",
    "\n",
    "Subtraction is **not commutative**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ v + u &= (v_1 + u_1, v_2 + u_2, \\ldots, v_p + u_p) \\\\\n",
    "\\ v - u &= (v_1 - u_1, v_2 - u_2, \\ldots, v_p - u_p), \\text{vector points to }v \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Dot Product / Inner Product / Scaler Product are the same thing\n",
    "\n",
    "Given vector $x$ and $y$, and $\\theta$ is the angle between them:\n",
    "\n",
    "$$x \\cdot y = \\lVert x \\rVert \\lVert y \\rVert \\cos(\\theta) = \\sum_{i=1}^{p}(x_i y_i)$$\n",
    "\n",
    "### Orthogonal Projection\n",
    "\n",
    "Given vector $x$ and $y$, and $\\theta$ is the angle between them, let the projection of $x$ onto $y$ be $z$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\cos(\\theta) &= \\frac{\\lVert z \\rVert}{\\lVert x \\rVert}\\\\\n",
    "\\ \\lVert z \\rVert &= \\lVert x \\rVert \\cos(\\theta) \\\\\n",
    "\\ \\cos(\\theta) &= \\frac{x \\cdot y}{\\lVert x \\rVert \\lVert y \\rVert} \\\\\n",
    "\\ \\therefore \\lVert z \\rVert &= \\lVert x \\rVert \\frac{x \\cdot y}{\\lVert x \\rVert \\lVert y \\rVert} \\\\\n",
    "\\ \\lVert z \\rVert &= \\frac{x \\cdot y}{\\lVert y \\rVert} \\\\\n",
    "\\ \\text{Define direction of }$y$\\text{, } u &= \\frac{y}{\\lVert y \\rVert} \\\\\n",
    "\\ therefore \\lVert z \\rVert &= u \\cdot x \\\\\n",
    "\\ \\because & u \\text{ and } y \\text{ are in the same direction} \\\\\n",
    "\\ u &= \\frac{z}{\\lVert z \\rVert} \\\\\n",
    "\\ z &= \\lVert z \\rVert u = u \\cdot x \\cdot u \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "With orthogonal projection, we can caluclate the distance from $x$ to the line that goes through $y$ as $\\lVert x-z \\rVert$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperplane\n",
    "\n",
    "A hyperplane is defined as follows, with $w$ being weights:\n",
    "\n",
    "$$ w^T x = 0 $$\n",
    "\n",
    "An important property is that the vector $w$ is **perpendicular to the hyperplane**. \n",
    "\n",
    "### Distance of a point to a hyperplane\n",
    "\n",
    "Let point $A$ be a point outside the hyperplane $H_0$ defined by $w^T x = 0$. We can use trigonometry to find the perpendicular distance from $A$ to $H_0$.\n",
    "\n",
    "In principle, to do so we need to project vector $A$ onto $w$, call it $p$, and then find the magnitude of $p$. \n",
    "\n",
    "We know that $p = \\frac{y}{\\lVert y \\rVert} \\cdot A \\cdot \\frac{y}{\\lVert y \\rVert}$. Therefore, we also konw $\\lVert p \\rVert$.\n",
    "\n",
    "### Margin of the hyperplane, m\n",
    "\n",
    "$$ margin = 2\\lVert p \\rVert $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "URL: http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/\n",
    "\n",
    "We want to find the hyperplane $H: w^T x + b = 0$ with maximum margin $m/2$ that separates a dataset $\\mathcal{D} = \\{ (x_i, y_i) \\mid x_i \\in \\mathbb{R}^p, y_i \\in \\{-1, +1\\}\\}_{i=1}^{n}$, provided that $\\mathcal{D}$ is linearly separable. \n",
    "\n",
    "Let $H_0$ and $H_1$ be two hyperplanes on each side of $H$, with **margin** distance of $m$ each. Defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ H_0: w^T x + b &= -1 \\\\\n",
    "\\ H_1: w^T x + b &= 1 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**The problem is how to find H while maximizing $m$.** Since $w$ is perpendicular to $H, H_0, H_1$, let $u$ be the unit vector:\n",
    "\n",
    "$$ u = \\frac{w}{\\lVert w \\rVert} $$\n",
    "\n",
    "Let $k$ be the vector that represent $m$, we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ k &= m \\cdot u = m \\frac{w}{\\lVert w \\rVert} \\\\\n",
    "\\ \\lVert k \\rVert &= m \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "First, we have to find $m$. Let $x_0$ be a point on $H_0$. We can find a point $z_0$ on $H_1$, defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ z_0 &= x_0 + k \\\\\n",
    "\\ \\therefore w \\cdot x_0 + b &= -1 \\\\\n",
    "\\ \\text{and } w \\cdot z_0 + b &= 1 \\\\\n",
    "\\ w \\cdot (x_0 + k) + b &= 1 \\\\\n",
    "\\ w \\cdot (x_0 + m \\frac{w}{\\lVert w \\rVert}) + b &= 1 \\\\\n",
    "\\ w \\cdot x_0 + m \\frac{w \\cdot w}{\\lVert w \\rVert} + b &= 1 \\\\\n",
    "\\ w \\cdot x_0 + m \\frac{\\lVert w \\rVert^2}{\\lVert w \\rVert} + b &= 1 \\\\\n",
    "\\ w \\cdot x_0 + m \\lVert w \\rVert + b &= 1 \\\\\n",
    "\\ w \\cdot x_0 + b &= 1 - m \\lVert w \\rVert \\\\\n",
    "\\ -1 &= 1 - m \\lVert w \\rVert \\\\\n",
    "\\ m &= \\frac{2}{\\lVert w \\rVert}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, maximizing margin $m$ is the same as minimizing $w$. The SVC problem boils down to the optimization problem of the following:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{minimize }}& \\lVert w \\rVert \\\\\n",
    "& \\text{subject to: }& y_i(w \\cdot x_i + b) \\geq 1 \\\\\n",
    "& &\\forall i = 1, 2 \\ldots, n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Maths Concepts for Optimizations\n",
    "\n",
    "Reference: http://www.svm-tutorial.com/2016/09/unconstrained-minimization/ \n",
    "\n",
    "#### Positive Definite Matrix\n",
    "\n",
    "The following statements are equivalent:\n",
    "\n",
    "* The symmetric matrix $A$ is positive definite.\n",
    "* All eigenvalues of $A$ are positive.\n",
    "* All the leading principal minors of $A$ are positive.\n",
    "* There exists nonsingular square matrix $B$ such that $A=B^TB$\n",
    "\n",
    "Source: http://www.math.ucsd.edu/~njw/Teaching/Math271C/Lecture_03.pdf\n",
    "\n",
    "#### Positive Semi-definite Matrix\n",
    "\n",
    "* The symmetric matrix $A$ is positive semi-definite.\n",
    "* All eigenvalues of $A$ are non-negative.\n",
    "* All the leading principal minors of $A$ are non-negative.\n",
    "* There exists nonsingular square matrix $B$ such that $A=B^TB$\n",
    "\n",
    "Source: http://www.math.ucsd.edu/~njw/Teaching/Math271C/Lecture_03.pdf\n",
    "\n",
    "#### Principal minor\n",
    "\n",
    "A minor of matrix $A$ of order $k$ is principal if it is obtained by deleting $nk$ rows and the $nk$ columns with the same numbers.\n",
    "\n",
    "#### Leading principal minor\n",
    "\n",
    "The **leading principal minor** of matrix $A$ of order $k$ is the minor of order $k$ obtained by deleting the last $nk$ rows and columns, where $n$ is the dimension of a symmetric matrix.\n",
    "\n",
    "\n",
    "#### Convex function \n",
    "\n",
    "https://en.wikipedia.org/wiki/Convex_function \n",
    "\n",
    "More generally, a continuous, twice differentiable function of several variables is convex on a convex set **if and only if its Hessian matrix is positive semidefinite on the interior of the convex set**.\n",
    "\n",
    "If we want to check if a function is convex, one easy way is to check if the Hessian matrix is positive semi-definite.\n",
    "\n",
    "More on this: http://www.math.cmu.edu/~ploh/docs/math/mop2013/convexity-soln.pdf\n",
    "\n",
    "#### Lagrange multipliers\n",
    "\n",
    "In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints. (Wikipedia)\n",
    "\n",
    "**Lagrange multipliers only work with equality constraints**.\n",
    "\n",
    "Problem construct:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{x, y}{\\text{minimize }}& f(x,y) \\\\\n",
    "& \\text{subject to: }& g_1(x,y) = 0 \\\\\n",
    "& & g_2(x,y) = 0 \\\\\n",
    "& & \\ldots \\\\\n",
    "& & g_n(x,y) = 0 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Lagrange found that the minimum of $f(x,y)$ under the constraint $g(x,y)=0$ is obtained **when their gradients point in the same direction** (e.g. on a contour plot for 3D problems).\n",
    "\n",
    "Define **Lagrangian function**, with $\\lambda$ being the **Lagrange multiplier**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(x,y,\\lambda) &= f(x,y) - \\sum_{i=1}^n\\lambda_i g_i(x,y) \\\\\n",
    "\\ \\text{and the gradient } \\partial{\\mathcal{L}(x,y,\\lambda)} &= \\partial{f(x,y)} - \\sum_{i=1}^n\\lambda_i \\partial{g_i(x,y)}\\\\\n",
    "\\ \\text{Solve for } \\partial{\\mathcal{L}(x,y,\\lambda)} &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By solving for this we will find the solutions for $x, y, \\lambda_i$ at the same time by solveing simultaneous equations from above.\n",
    "\n",
    "More on equality and inequality constraints: http://www.engr.mun.ca/~baxter/Publications/LagrangeForSVMs.pdf\n",
    "\n",
    "To adapte for **inequality** constraints, the problem is formed in the same ways with additional rules applied as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ g(x,y) \\geq 0 &\\Rightarrow \\lambda \\geq 0 \\\\\n",
    "\\ g(x,y) \\leq 0 &\\Rightarrow \\lambda \\leq 0 \\\\\n",
    "\\ g(x,y) = 0 &\\Rightarrow \\lambda \\text{ is unconstrainted} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Support Vector Classifier with Overlaping Classes\n",
    "\n",
    "Maths notes: http://www.tristanfletcher.co.uk/SVM%20Explained.pdf\n",
    "\n",
    "**Based on ESL print 10, chapter 12.**\n",
    "\n",
    "**ESL** defines the margin as $M=\\frac{m}{2} = \\frac{1}{\\lVert w \\rVert}$, where $m$ is the margin definition from the notes aboce.\n",
    "\n",
    "To allow for overlapping classes, ESL introduces **slack variables**:\n",
    "\n",
    "$$ \\xi = \\big(\\xi_1, \\xi_2, \\ldots, \\xi_N \\big) $$\n",
    "\n",
    "The constraints of the previous optimization problem are modified:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{minimize }}& \\lVert w \\rVert \\\\\n",
    "& \\text{subject to: }& y_i(w \\cdot x_i + b) \\geq M(1-\\xi_i) \\\\\n",
    "& & \\xi_i \\geq 0 \\\\\n",
    "& &\\forall i = 1, 2 \\ldots, N \\\\\n",
    "& &\\sum_{i=1}^{N}\\xi_i \\leq \\text{constant (C)} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Computationally this is the same as (p420):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{minimize }}& \\frac{1}{2}\\lVert w \\rVert + C\\sum_{i=1}^{N}\\xi_i \\\\\n",
    "& \\text{subject to: }& y_i(w \\cdot x_i + b) \\geq (1-\\xi_i) \\\\\n",
    "& & \\xi_i \\geq 0 \\\\\n",
    "& &\\forall i = 1, 2 \\ldots, N\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(w) &= \\frac{1}{2}\\lVert w \\rVert + C\\sum_{i=1}^{N}\\xi_i \\\\\n",
    "g(w, y, x, \\xi) &= y(w \\cdot x + b) - (1-\\xi) \\\\\n",
    "h(\\xi) &= \\xi\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore the Lagrange (primal) function is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_P &= f(w) - \\sum_{i=1}^{N}\\alpha_i g(w, y_i, x_i, \\xi_i) - \\sum_{i=1}^{N} \\mu_i h(\\xi_i) \\\\\n",
    "& \\text{where } \\alpha_i \\text{ and } \\mu_i \\text{ are Lagrange multipliers} \\\\\n",
    "L_P &= \\frac{1}{2}\\lVert w \\rVert + C\\sum_{i=1}^{N}\\xi_i\n",
    "- \\sum_{i=1}^{N}\\alpha_i \\big[ y_i(w \\cdot x_i + b) - (1-\\xi_i) \\big] \n",
    "- \\sum_{i=1}^{N}\\mu_i \\xi_i \\\\\n",
    "\\frac{\\partial{L_P}}{\\partial{w}} &= w -\\sum_{i=1}^{N}\\alpha_i y_i x_i = 0 \\Rightarrow\n",
    "w = \\sum_{i=1}^{N}\\alpha_i y_i x_i \\\\\n",
    "\\frac{\\partial{L_P}}{\\partial{b}} &= \\sum_{i=1}^{N}\\alpha_i y_i = 0 \\\\\n",
    "\\frac{\\partial{L_P}}{\\partial{\\xi_i}} &= C - \\sum_{i=1}^{N}\\alpha_i - \\sum_{i=1}^{N}\\mu_i = 0 \n",
    "\\Rightarrow \\alpha_i = C - \\mu_i, \\forall i \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Substituting $w, \\sum_{i=1}^{N}\\alpha_i y_i = 0, \\alpha_i = C - \\mu_i$ into $L_P$, we have the **Lagrangian (Wolfe) dual objective function**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_D &= \\frac{1}{2}\\bigg[\\sum_{i=1}^{N}\\alpha_i y_i x_i \\bigg]^2 + C\\sum_{i=1}^{N}\\xi_i\n",
    "- \\sum_{i=1}^{N}\\alpha_i \\bigg[ y_i (\\sum_{j=1}^{N}\\alpha_j y_j x_i^T x_j + b) - (1 - \\xi_i)\\bigg]\n",
    "- \\sum_{i=1}^{N}\\mu_i \\xi_i\\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \n",
    "+ C\\sum_{i=1}^{N}\\xi_i\n",
    "- \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \n",
    "+ \\sum_{i=1}^{N} \\alpha_i y_i b + \\sum_{i=1}^{N} \\alpha_i (1 - \\xi_i)\n",
    "- \\sum_{i=1}^{N}\\mu_i \\xi_i\\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \n",
    "+ C\\sum_{i=1}^{N}\\xi_i\n",
    "- \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \n",
    "+ 0 + \\sum_{i=1}^{N} \\alpha_i -\\sum_{i=1}^{N} \\alpha_i \\xi_i\n",
    "- \\sum_{i=1}^{N}\\mu_i \\xi_i\\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \n",
    "+ \\sum_{i=1}^{N} \\alpha_i - \\bigg[\\sum_{i=1}^{N} (\\alpha_i - C + \\mu_i) \\xi_i \\bigg]\\\\\n",
    "&= \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j - 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We maximize $L_D$ subject to the following Lagrange and **Karush-Kuhn-Tucker** [(3)-(5)] constraints:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 \\leq \\alpha_i \\leq C & \\,\\,\\, (1)\\\\\n",
    "\\sum_{i=1}^{N} \\alpha_i y_i = 0 & \\,\\,\\, (2)\\\\\n",
    "\\alpha_i y_i(w \\cdot x_i + b) - (1-\\xi_i) = 0 & \\,\\,\\, (3)\\\\\n",
    "\\mu_i \\xi_i = 0 & \\,\\,\\, (4)\\\\\n",
    "y_i(w \\cdot x_i + b) - (1-\\xi_i) \\geq 0 & \\,\\,\\, (5)\\\\\n",
    "\\forall i = 1, \\ldots, N\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Kernels\n",
    "\n",
    "We can rewrite $L_D$ with a **kernel function $K$**: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_D &= \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\\\\\n",
    "\\text{where: } K(x_i, x_j) &= \\langle h(x_i), h(x_j) \\rangle \n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "**Three popular kernal functions**:\n",
    "\n",
    "* d-degree polynominal \n",
    "* Radial basis\n",
    "* Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Finding $\\alpha_i$ and $w$\n",
    "\n",
    "From this optimization we can find $alpha_i$, and therefore we can find $w$ with **non-zero** coefficients $\\hat{\\alpha}_i$, whose indices $i$'s indicate **support vectors**, due to (3) constraint above.\n",
    "\n",
    "### Finding $b$\n",
    "\n",
    "For some of the support vectors, they will lie exaclty on the edge of the margin ($\\xi_i = 0$). Based on $\\alpha_i = C - \\mu_i, \\forall i$ and constraint (4), these **margin points** will have $ 0 < \\hat{\\alpha}_i < C$. \n",
    "\n",
    "We can use any of these points with constraint (5) above to solve for $b$. **In practice, we use all of these points to calculate $b$ and take the average for numerical stability.**\n",
    "\n",
    "Note that the remaining support vectors ($\\xi_i > 0$) have $\\hat{\\alpha}_i = C$.\n",
    "\n",
    "Let $S$ represent the margin points, \n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_s \\big[y_s (w \\cdot x_s + b)\\big] &= 1 \\times y_s \\\\\n",
    "y_s^2 w \\cdot x_s + y_s^2 b &= y_s \\\\\n",
    "\\because \\, y_s^2 &= 1 \\\\\n",
    "w \\cdot x_s + b &= y_s \\\\\n",
    "b &= y_s - w \\cdot x_s \\\\\n",
    "\\therefore \\, \\hat{b} &= \\frac{1}{N_S}\\sum_{i=1}^{N_S}(y_s - w \\cdot x_s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Prediction\n",
    "\n",
    "For any new point $x'$, $y' = sign(w \\cdot x' + b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SVM Regression\n",
    "\n",
    "Similar to the classification case, **but need to introduce a tolerance band**, $\\epsilon$. Penalty is allocated to points where $y_i - \\hat{y}_i > \\epsilon$, using slack variable, either of $\\{\\xi^+, \\xi^-\\}$. \n",
    "\n",
    "ESL introduces two penalty functions: 1) standard $V_\\epsilon$ and 2) Huber $V_H$. **Huber is compared with robust regression**, as it assigns less penalty to large outliers, i.e. linear rather than quadratic.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_\\epsilon(r) &= \\begin{cases}\n",
    "0 & if \\mid r\\mid < \\epsilon \\\\\n",
    "\\mid r \\mid - \\epsilon & otherwise\n",
    "\\end{cases} \\\\\n",
    "V_H(r) &= \\begin{cases}\n",
    "r^2 / 2 & if \\mid r \\mid \\leq c \\\\\n",
    "c\\mid r \\mid - c^2 / 2 & \\mid r \\mid > c\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\epsilon$ is another tuning parameter that can be chosen by CV?\n",
    "\n",
    "ESL mentioned that if we **scaled the response $r$**, preset values for $c$ and $\\epsilon$ can be choosen, while $lambda$ is chosen with CV. (E.g. c = 1.345 achieves 95% efficiency for the Gaussian.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
