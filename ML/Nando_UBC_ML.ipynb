{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nadno de Fritas UBC ML Lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture Videos: https://www.youtube.com/playlist?list=PLE6Wd9FR--Ecf_5nCbnSQMHqORpiChfJf\n",
    "\n",
    "Website: http://www.cs.ubc.ca/~nando/340-2012/lectures.php\n",
    "\n",
    "Google Group: https://groups.google.com/forum/#!forum/cpsc340-2012\n",
    "\n",
    "Latex symbols: https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols\n",
    "\n",
    "Equations: https://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Notes 10\n",
    "\n",
    "## Question on slide 9, $P(M=1 \\mid T=0)$?\n",
    "\n",
    "To continue from **slide 5**, the probabilities are:\n",
    "\n",
    "$P( T \\mid F=1, S=0, \\beta_{2} ) = \\beta^1_2 (1-\\beta_{2})^1$\n",
    "\n",
    "$P( T \\mid F=0, S=1, \\beta_{3} ) = \\beta^1_3 (1-\\beta_{3})^0$\n",
    "\n",
    "$P( T \\mid F=1, S=1, \\beta_{4} ) = \\beta^1_4 (1-\\beta_{4})^0$\n",
    "\n",
    "With **Maximum Likelihood**, I get:\n",
    "\n",
    "$\\beta_{2,ML} = \\frac{1}{2}$\n",
    "\n",
    "$\\beta_{3,ML} = \\frac{1}{1} = 1$\n",
    "\n",
    "$\\beta_{4,ML} = \\frac{1}{1} = 1$\n",
    "\n",
    "Also to complete **slide 6**: \n",
    "\n",
    "$\\gamma_{2,ML} = \\frac{3}{4}$\n",
    "\n",
    "Then we have all the numbers we need to complete the tables on **slide 4**.\n",
    "\n",
    "Assuming all priors are Beta(1,1), with **posterior mean**, E.g.:\n",
    "\n",
    "$P(\\theta) = Beta(1,1) \\propto \\theta^{(1-1)}(1-\\theta)^{(1-1)}$?\n",
    "\n",
    "Since all priors are Beta(1,1), we get the following, with probablities on slide 5:\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "P(\\theta|M) = P(M|\\theta)P(\\theta) \\propto \\theta^4(1-\\theta)^1\\theta^0(1-\\theta)^0 \\Rightarrow \\mathbb{E}(\\theta|M)= \\frac{5}{5+2}=\\frac{5}{7}\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "And therefore:\n",
    "\n",
    "$\\mathbb{E}(\\alpha \\mid S)=\\frac{3}{4}$\n",
    "\n",
    "$\\mathbb{E}(\\gamma_1 \\mid F,M=0)=\\frac{1}/{1+2}=\\frac{1}{3}$\n",
    "\n",
    "$\\mathbb{E}(\\gamma_2 \\mid F,M=1)=\\frac{4}/{4+2}=\\frac{2}{3}$\n",
    "\n",
    "$\\mathbb{E}(\\beta_1 \\mid T,F=0,S=0)=\\frac{1}{1+2}=\\frac{1}{3}$\n",
    "\n",
    "$\\mathbb{E}(\\beta_2 \\mid T,F=1,S=0)=\\frac{2}{2+2}=\\frac{1}{2}$\n",
    "\n",
    "$\\mathbb{E}(\\beta_3 \\mid T,F=0,S=1)=\\frac{2}{2+1}=\\frac{2}{3}$\n",
    "\n",
    "$\\mathbb{E}(\\beta_4 \\mid T,F=1,S=1)=\\frac{2}{2+1}=\\frac{2}{3}$\n",
    "\n",
    "Now we have all parameters needed for filling the tables on **slide 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Matrix Algebra & Calculus\n",
    "\n",
    "http://www.statpower.net/Content/312/Handout/Matrix.pdf\n",
    "\n",
    "\n",
    "## Properties of Transposition\n",
    "\n",
    "1. $(A^{'})^{'} = A$\n",
    "2. $(cA)^{'} = cA^{'}$\n",
    "3. $(A + B)^{'} = A^{'} + B^{'}$\n",
    "4. $(AB)^{'} = B^{'}A^{'}$\n",
    "5. $A^{'}B = B^{'}A$\n",
    "6. $(X^{-1})^{'} = (X^{'})^{-1}$\n",
    "\n",
    "## Other Properties\n",
    "\n",
    "* If $x^{-1} == x^{T}$ then $x$ is **orthgonal**.\n",
    "\n",
    "## Calculus\n",
    "\n",
    "Matrix caculus for machine learning [here](https://arxiv.org/abs/1802.01528)\n",
    "\n",
    "Proof: http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf\n",
    "\n",
    "| y | $\\frac{\\partial{y}}{\\partial{x}}$ |\n",
    "|:---:|:---------------:| \n",
    "| $Ax$ | $A^{T}$ |\n",
    "| $x^{T}A$ | $A$ |\n",
    "| $x^{T}x$ | $2x$ |\n",
    "| $x^{T}Ax$ | $Ax + A^{T}x$, if $A$ is **symmetric**, $2Ax$ |\n",
    "\n",
    "| $\\alpha$ | $\\frac{\\partial{\\alpha}}{\\partial{x}}$ | $\\frac{\\partial{\\alpha}}{\\partial{y}}$ |\n",
    "|:---: |: --- :|: --- :|\n",
    "| $y^{T}Ax$ | $y^{T}A$ | $x^{T}A^{T}$ |\n",
    "\n",
    "| $\\alpha$  | $\\frac{\\partial{\\alpha}}{\\partial{z}}$ can't figout out how to make it wider|\n",
    "|:---: |: --- :|\n",
    "| $y^{T}x$ | $x^{T}\\frac{\\partial{y}}{\\partial{z}} + y^{T}\\frac{\\partial{x}}{\\partial{z}}$ |\n",
    "| $y^{T}Ax$ | $x^{T}A^{T}\\frac{\\partial{y}}{\\partial{z}} + y^{T}A\\frac{\\partial{x}}{\\partial{z}}$ |\n",
    "| $x^{T}x$ | $2x^{T}\\frac{\\partial{x}}{\\partial{z}}$ |\n",
    "| $x^{T}Ax$ | $x^{T}(A + A^{T})\\frac{\\partial{x}}{\\partial{z}}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace of Matrices\n",
    "\n",
    "Kevin Murphy's book, p101\n",
    "\n",
    "Trace of matrix $A$ is the sum of its diagonal elements.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "tr(A) =& \\sum_{i} A_{ii} \\\\\n",
    "tr(ABC) =& tr(CAB) = tr(BCA) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Trace trick** last equation above is called the **cyclic permutation property** of the trace operator. Using this we derive the trace trick, which reorders the scaler inner product $x^TAx$:\n",
    "\n",
    "$$x^TAx = tr(x^TAx) = tr(xx^TA) = tr(Axx^T) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 19\n",
    "\n",
    "## Linear Regression - Maximum Likelihood Matrix Maths\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ L(\\theta) & = [y - x \\theta]^{T} [y - x \\theta ] \\\\\n",
    "\\ &= [y^{T} - (x\\theta)^{T}][y - x\\theta] \\\\\n",
    "\\ & = [y^{T} - \\theta^{T}x^{T}][y - x\\theta] \\\\\n",
    "\\ & = y^{T}y - y^{T}x\\theta - \\theta^{T}x^{T}y + \\theta^{T}x^{T}x\\theta \\\\\n",
    "\\ & = y^{T}y - 2y^{T}x\\theta +\\theta^{T}x^{T}x\\theta \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that $y^{T}x\\theta == \\theta^{T}x^{T}y$. See numerical example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.matrix([[2], [3]])\n",
    "x = np.matrix([[3, 5], [6, 8]])\n",
    "theta = np.matrix([[5], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3, 5],\n",
       "        [6, 8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ True]], dtype=bool)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(y.T @ x @ theta - theta.T @ x.T @ y, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set $\\frac{\\partial{L(\\theta)}}{\\partial{\\theta}} = 0$ for maximum likelihood gives:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\frac{\\partial{L(\\theta)}}{\\partial{\\theta}} = -2y^{T}x + 2x^{T}x\\theta &= 0\\\\\n",
    "\\ -2y^{T}x + 2x^{T}x\\theta &= 0 \\\\\n",
    "\\ \\hat{\\theta} &= (x^{T}x)^{-1}x^{T}y \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distributions\n",
    "\n",
    "## Normal Distribution\n",
    "\n",
    "Python: `scipy.stats.norm`\n",
    "\n",
    "Given a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\ PDF(x) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\bigg(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\bigg) \\\\\n",
    "\\ CDF(x) &= \\frac{1}{2}\\big[1 + erf\\big(\\frac{x - \\mu}{\\sigma\\sqrt{2}}\\big)\\big] \\\\\n",
    "\\ erf(x) &= \\frac{1}{\\sqrt{\\pi}}\\int_{-x}^{x}e^{-t^2}dt = \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{x}e^{-t^2}dt\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Control of Precision**: when we need to evaluate the PDF many times, we set $\\beta = \\frac{1}{\\sigma^2}$:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\ PDF(x) &= \\sqrt{\\frac{\\beta}{2\\pi}} \\exp\\bigg(-\\frac{1}{2}\\beta(x-\\mu)^2\\bigg) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta Distribution\n",
    "\n",
    "https://en.wikipedia.org/wiki/Beta_distribution\n",
    "\n",
    "Python: `scipy.stats.beta`\n",
    "\n",
    "In Bayesian inference, the beta distribution is the **conjugate prior** probability distribution for the **Bernoulli, binomial, negative binomial and geometric distributions**. \n",
    "\n",
    "Given $Beta(\\alpha, \\beta)$:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\ \\Gamma(x) &= (n - 1)! \\, \\forall n \\in \\text{positive integers, i.e. }\\mathbb{N}^+ \\\\\n",
    "\\ \\Gamma(z) &= \\int_{0}^{\\infty}x^{z-1}e^{-x}dx \\\\\n",
    "\\ B(\\alpha, \\beta) &= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\\\\n",
    "\\ PDF(x) &= \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)} \\\\\n",
    "\\ CDF(x) &= I_x(\\alpha, \\beta) = \\frac{B(x; \\alpha,\\beta)}{B(\\alpha, \\beta)} \\\n",
    " = \\frac{\\int_{0}^{\\infty}t^{\\alpha-1}(1-t)^{\\beta-1}dt}{B(\\alpha, \\beta)} \\\\\n",
    "\\ \\mathbb{E}[Beta(\\alpha, \\beta)] &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\n",
    "\\ var(Beta(\\alpha, \\beta) &= \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Distribution\n",
    "\n",
    "See **Nando's lecture notes 19** for a good discussion on its application.\n",
    "\n",
    "Given $ 0 < p < 1, p \\in \\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ PDF(p,k) &= p^k(1-p)^{1-k}, &k \\in \\{0, 1\\} \\\\\n",
    "\\ CDF(p,k) &= \n",
    "\\begin{cases}\n",
    "0 &k < 0 \\\\\n",
    "1-p &0 \\leq k < 1 \\\\\n",
    "1 &k \\geq 1 \n",
    "\\end{cases} \\\\\n",
    "\\ Mean = p \\\\\n",
    "\\ Variance = p(1-p)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Binomial Distribution\n",
    "\n",
    "Given $B(n, p), n \\in \\mathbb{N}_0, p \\in [0, 1]$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "PDF &= \\binom{n}{k} p^k(1-p)^{n-k} \\\\\n",
    "CDF &= \\sum_{i=0}^{\\lfloor k \\rfloor} \\binom{n}{i} p^i(1-p)^{n-i} = I_{1-p}(n-k, 1+k) \\\\\n",
    "Mean &= np \\\\\n",
    "Variance &= np(1-p)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Notes 15\n",
    "\n",
    "## Multivariate Normal Distribution\n",
    "\n",
    "Let $y \\in \\mathbb{R}^{n\\times1}$, then the PDF of a **n-dimensional** multivariable normal distribution is (only when $\\Sigma$ is **positive-definite**):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "pdf(y) &= (2\\pi)^{-n/2}\\det(\\Sigma)^{-1/2}\\exp\\bigg[-\\frac{1}{2}(y-\\mu)^T\\Sigma^{-1}(y-\\mu)\\bigg] \\\\\n",
    "&= \\frac{1}{\\sqrt{(2\\pi)^{n}det(\\Sigma)}} \\exp\\bigg[-\\frac{1}{2}(y-\\mu)^T\\Sigma^{-1}(y-\\mu)\\bigg] \\\\\n",
    "\\mu &= \\mathbb{E}(y) \\\\\n",
    "\\Sigma &= \\mathbb{E}[(y-\\mu)^T(y-\\mu)] = \\text{covariance} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Mahalanobis distance** measures the distance between a data vector $x$ and the mean vector $\\mu$, It is the expression inside the exponential in the multivariate normal PDF formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Control of Precision**: when we need to evaluate the PDF many times, we need to invert $\\Sigma$ first and assign as $\\beta$: See Deep Learning (Goodfellow) p63.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "pdf(y) &= (2\\pi)^{-n/2}\\det(\\Sigma)^{1/2}\\exp\\bigg[-\\frac{1}{2}(y-\\mu)^T\\beta(y-\\mu)\\bigg] \\\\\n",
    "\\mu &= \\mathbb{E}(y) \\\\\n",
    "\\Sigma &= \\mathbb{E}[(y-\\mu)^T(y-\\mu)] = \\text{covariance} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvector & Eigenvalues\n",
    "\n",
    "Deep Learning (Goodfellow)\n",
    "\n",
    "An **eigenvector** of a **square** matrix $A$ is a nonzero vector $v$ such that:\n",
    "$$ Av = \\lambda v $$\n",
    "Where $\\lambda$ is a constant scaler, known as the **eigenvalue** of this corresponding eigenvector.\n",
    "\n",
    "Concatenating all the eigenvectors to form a matrix $V$ with one eigenvector per column, and eigenvalues into a diagonal matrix:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V =& \\big[v^{(1)},\\dots,v^{(n)}\\big] \\\\\n",
    "D =& \\text{diag}([\\lambda_1, \\dots, \\lambda_n])\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We have the **Eigen-decomposition**:\n",
    "\n",
    "$$ A = V D V^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Eigen-decomposition\n",
    "\n",
    "$V$ above is an **orthogonal matrix** of eigenvectors, satisfying $V^{-1} = V^T$ and $V^T V=I$\n",
    "\n",
    "$D$ is a diagonal, square matrix of eigenvalues\n",
    "\n",
    "Note the inverse of a diagonal matrix exists only if every diagonal entry is nonzero. In this case:\n",
    "$$ diag(v)^{-1} = [1/v_1, \\dots, 1/v_n]^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Dependence / Singular Matrix\n",
    "\n",
    "A set of vectors is **linearly independent** if no vector in the set is a linear combination of the other vectors.\n",
    "\n",
    "A **square** matrix with linearly dependent columns is know as **singular**.\n",
    "\n",
    "For square matrices the left inverse and right inverse are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Fractional Power\n",
    "\n",
    "http://math.stackexchange.com/questions/732511/fractional-power-of-matrix \n",
    "\n",
    "If a matrix is diagonalizable, first diagonalize it (**eigen-decomposition**), apply the power then convert back.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A &= VDV^{-1} \\\\\n",
    "A^n &= VD^nV^{-1} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "See **power of matrices** section at https://en.wikipedia.org/wiki/Matrix_multiplication\n",
    "\n",
    "**For a diagonal matrix $A$, $A^n$ is just raising the diagonal elements of $A$ to the power of $n$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "\n",
    "Deep Learning (Goodfellow)\n",
    "\n",
    "Assume matrix $A$ is $m \\times n$, SVD is\n",
    "\n",
    "$$ A = UDV^{T} $$\n",
    "\n",
    "where:\n",
    "\n",
    "1. $U$ is $m \\times m$, orthogonal, columns are **left-singular vectors** of $A$\n",
    "2. $D$ is $m \\times n$, diagonal matrix of **singular values** of $A$\n",
    "3. $V$ is $n \\times n$, orthogonal, columns are **right-singular vectors** of $A$\n",
    "\n",
    "### Other Properties\n",
    "\n",
    "1. $U$ are the eigenvectors of $AA^T$\n",
    "2. $V$ are the eigenvectors of $A^T A$\n",
    "3. the nonzero singular values of $A$ are the square roots of eigenvalues of $A^T A$, same is true for $AA^T$\n",
    "\n",
    "### Moore-Penrose Pseudoinverse\n",
    "\n",
    "Solving equations: $Ax = y$, given $A$ is $n \\times m$.\n",
    "\n",
    "1. $n > m$ - possible that there is no solution\n",
    "2. $n < m$ - could be multiple solutions\n",
    "\n",
    "Pseudoinverse definition:\n",
    "\n",
    "$$ A^+ = VD^+U^T $$\n",
    "\n",
    "where $U$, $D$, $V$ are coming from the SVD of $A$, the pseudoinverse $D^+$ of diagonal matrix $D$ is obtained by taking the reciprocal of its nonzero elements then taking the transpose of the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 20 (Notes PDF L16b)\n",
    "\n",
    "Ridge regression is the mean of Bayesian regression (posterior mean), in special case where prior mean == 0.\n",
    "\n",
    "Ridge regression maths essentially gets rid of the small eigenvalues.\n",
    "\n",
    "## Bayesian Linear Regression\n",
    "\n",
    "The likelihood is Gaussian, $\\mathcal{N}(y\\mid X\\theta, \\sigma^{2}I_d)$.\n",
    "\n",
    "The conjugate prior is also Gaussian, $\\mathcal{N}(\\theta \\mid \\theta_{0}, V_{0})$\n",
    "\n",
    "With Bayes rule, the posterior is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ p(\\theta \\mid X, y, \\sigma^{2}) &\\propto \\mathcal{N}(\\theta \\mid \\theta_{0}, V_{0}) \\mathcal{N}(y \\mid X\\theta, \\sigma^{2}I) = \\mathcal{N}(\\theta | \\theta_{n}, V_{n}) \\\\\n",
    "\\ \\theta_{n} &= V_{n}V_{0}^{-1}\\theta_{0} + \\frac{1}{\\sigma^2}V_{n}X^{T}y \\\\\n",
    "\\ V_{n}^{-1} &= V_{0}^{-1} + \\frac{1}{\\sigma^2}X^{T}X \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## Slide title Bayesian versus ML plugin prediction, minute 35:00\n",
    "\n",
    "For the Bayesian, the posterior is the new prior, $\\mathcal{N}(\\theta \\mid \\theta_{n}, V_{n})$. \n",
    "\n",
    "In the special case where:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\theta_0 &= 0 \\\\\n",
    "\\ V_0 &= \\tau^2 I_d \\\\\n",
    "\\ \\text{define: } \\lambda &= \\frac{\\sigma^2}{\\tau^2} \\\\\n",
    "\\ \\text{Posterior mean: } \\theta_{n} &= (X^{T}X + \\lambda I_d)^{-1}X^{T}y \\\\\n",
    "\\ \\text{Posterior variance: } V_{n} &= \\sigma^{2}(X^{T}X + \\lambda I_d)^{-1} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Bayesian linear regression produces difference variance to Maximum Likelihood method. \n",
    "\n",
    "Implication is that Bayesian predictions have **lower** variance in places where it has seen data, but **higher** variance where it has not seen data before. This is shown by the variance term for Bayesian prediction, which has a data depended term associated with $V_{n}$, which is essentially the **inverse of data matrix**.\n",
    "\n",
    "Bayesian, given $D = (X, y)$ is the data matrix:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ P(y|x_{*}, D, \\sigma^{2}) &= \\mathcal{N}(y \\mid x_{*}^{T}\\theta_{n}, \\sigma^{2} + x_{*}^{T}V_{n}x_{*}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Also the posterial is proper if $n > k$ where $n$ is the number of rows and $k$ the number of features, and $rank(X) == k$. Ref: https://www.youtube.com/watch?v=d1iIUtnDngg, min 4:17\n",
    "\n",
    "Maximum Likelihood:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ P(y|x_{*}, D, \\sigma^{2}) &= \\mathcal{N}(y \\mid x_{*}^{T}\\theta_{ML}, \\sigma^{2}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "This means that Bayesian believes there are many possible values of $\\theta$, they are weighted by their probabilities. \n",
    "\n",
    "When a new **data point** $x_{*}$ comes in, it is evaluated with all possible $\\theta$'s, then weighted by their probabilities again. Posterior $\\propto$ Prior * Likelihood.\n",
    "\n",
    "For Maximum Likelihood, the posterior is a delta function that takes a value at $\\theta_{ML}$, i.e. there is only one solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 21\n",
    "\n",
    "## Sub-differentiation\n",
    "\n",
    "This is the treatment of taking derivative of absolute functions. The idea is to consider all possible slopes (derivatives), and define the solution as a set. Example below. This is also why for **Lasso** some parameters will be **zero**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ J(\\theta) &= \\delta^{2}|\\theta| \\\\\n",
    "\\ \\frac{\\partial{J}}{\\partial{\\theta}} &= \n",
    "\\begin{cases}\n",
    "-1, \\text{if $\\theta < 0$} \\\\\n",
    "\\lbrack -1, 1 \\rbrack, \\text{if $\\theta = 0$} \\\\\n",
    "1, \\text{if $\\theta > 0$}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Coordinate Descent Algorithm for Sparse Prediction\n",
    "\n",
    "In the derivation of derivative of the objective function, for a vector $x$, the notation $x_{i-j}$ refers to all the elements of $x$ except the $j^{th}$ element. Similarly, $x_{j}$ refers to only the $j^{th}$ element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
