{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Book\n",
    "\n",
    "# Chapter 10 Sequence Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Forcing, p372\n",
    "\n",
    "A RNN whose only recurrence is the feedback connection from the output of the hidden layer, not the hidde layer itself, is **less powerful** than those which have direct connection from hidden layer $h^{(t-1)}$ to $h^{(t)}$. p370.\n",
    "\n",
    "The advantage of eliminating hidden-to-hidden recurrence is that, for any loss function based on comparing the prediction at time $t$ to the training target at time $t$, all the time steps are decoupled. Tranining can thus be parallelized, with gradient for each step t computed in isolation.\n",
    "\n",
    "Teacher forcing is a procedure that emerges from the maximum likelihood criterion, in which during training the model receives the ground truth output $y^{(t)}$ as input at time $t+1$, instead of the output from the previous step, thus parallelizing training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "\n",
    "The backprop algorithm applied to the unrolled graph with $\\mathbb{O}(\\tau)$ cost is called **back-propagation through time (BPTT)**. As soon as the hidden units become a function of earlier time steps, the BPTT algorithm is necessary.\n",
    "\n",
    "Example is based on the RNN architecture given in figure 10.3 on page 369. This network is described as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^{(t)} &= b + W h^{(t-1)} + U x^{(t)}, &(10.8)\\\\\n",
    "h^{(t)} &= \\tanh\\big(a^{(t)}\\big), &(10.9)\\\\\n",
    "o^{(t)} &= c + V h^{(t)}, &(10.10)\\\\\n",
    "\\hat{y}^{(t)} &= \\text{softmax}\\big( o^{(t)} \\big), &(10.11)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "* $x$ are input sequences\n",
    "* $o$ outputs of corresponding $x$\n",
    "* $y$ training target for corresponding $x$\n",
    "* $U$ weight matrix for input-to-hidden connections\n",
    "* $W$ weight matrix for hidden-to-hidden connections\n",
    "* $V$ weight matrix for hidden-to-output connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, **derivate of `softmax(x)`**, see this [post](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/), using the quotient rule:\n",
    "\n",
    "$$ \\big(\\frac{f}{g}\\big)' = \\frac{f'g - fg'}{g^2} $$\n",
    "\n",
    "Let $f = e^x$, $g = \\sum_i e^{x_i}$, $y = \\text{softmax}(x)$ and $y_i$ be the result for $x_i$, $x \\in R^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $i = j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial y_i}{\\partial x_i} &= \\frac{e^{x_i} g - e^{x_i} (0 + \\cdots + e^{x_i} + 0 + \\cdots + 0)}{g^2} \\\\\n",
    "&= \\frac{e^{x_i} g - e^{x_i} e^{x_i}}{g^2} \\\\\n",
    "&= \\frac{e^{x_i} (g - e^{x_i})}{g^2} \\\\\n",
    "&= \\frac{e^{x_i}}{g} \\times \\frac{g - e^{x_i}}{g}\\\\\n",
    "&= y_i \\times (1 - \\frac{e^{x_i}}{g}) \\\\\n",
    "&= y_i \\times (1 - y_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $i \\neq j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial y_i}{\\partial x_j} &= \\frac{0 \\times g - e^{x_i} e^{x_j}}{g^2} \\\\\n",
    "&= -\\frac{e^{x_i}}{g} \\frac{e^{x_j}}{g} \\\\\n",
    "&= - y_i y_j\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to BPTT, we start from the node immediately preceding the final loss:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial L^{t}} = 1 $$\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$(\\triangledown_{o^{t}}L)_i = \\frac{\\partial L}{\\partial o^{t}_i} = \\frac{\\partial L}{\\partial L^{t}} \\frac{\\partial L^{t}}{\\partial o^{t}_i} = \\hat{y}^{(t)}_i - 1_{i,y^{(t)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - Long Short-Term Memory\n",
    "\n",
    "Chris Olah's [post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "Components of LSTM cell:\n",
    "\n",
    "## Forget gate \n",
    "\n",
    "Decides what info we are going to throw away from the cell state. \n",
    "\n",
    "* Inputs: $h_{t-1}$, $x_t$\n",
    "* Output: a number between 0 and 1 for each number in the cell state $C_{t-1}$. 1 is keep everything, 0 is forget everything.\n",
    "\n",
    "$$ f_t = \\sigma\\big( W_f \\times [ h_{t-1}, x_t] + b_f\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Gate, tanh layer\n",
    "\n",
    "Next step is to decide what new info we are going to store in the cell state, done in two parts. \n",
    "* Input gate\n",
    "* tanh layer\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "i_t &= \\sigma\\big( W_i \\times [h_{t-1}, x_t] + b_i \\big) \\\\\n",
    "\\tilde{C}_t &= \\tanh \\big( W_C \\times [h_{t-1}, x_t] + b_C \\big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell State\n",
    "\n",
    "Next step is to update the **old** cell state, $C_{t-1}$, into the **new** cell state $C_t$. Inputs are the outputs from **forget gate**, **input gate**, and **tanh layer**. \n",
    "\n",
    "$$ C_t = f_t \\times C_{t-1} + i_t \\times \\tilde{C}_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ouput \n",
    "\n",
    "Work out the output of the cell, using sigmoid to decide which parts of the cell state we are going to output. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "o_t &= \\sigma \\big( W_o \\times [h_{t-1}, x_t] + b_o \\big) \\\\\n",
    "h_t &= o_t \\times \\tanh(C_t)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Variants\n",
    "\n",
    "## Gers & Schmidhuber (2000)\n",
    "\n",
    " Add **peephole connections**, allowing the gates to look at the cell state. Modifications:\n",
    " \n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_t &= \\sigma \\big( W_f \\times [C_{t-1}, h_{t-1}, x_t] + b_f \\big) \\\\\n",
    "i_t &= \\sigma \\big( W_i \\times [C_{t-1}, h_{t-1}, x_t] + b_i \\big) \\\\\n",
    "o_t &= \\sigma \\big( W_o \\times [C_{t}, h_{t-1}, x_t] + b_o \\big) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Coupled Forget and Input Gates\n",
    "\n",
    "Make **forget** and **input** (cell update) gates make decision together.\n",
    "\n",
    "$$ C_t = f_t \\times C_{t-1} + (1-f_t) \\times \\tilde{C}_t $$\n",
    "\n",
    "## GRU (Gated Recurrent Unit)\n",
    "\n",
    "Combines **forget** and **input** gates into a single **update** gate, merges **cell state** and **hidden state**, plus othe changes. \n",
    "\n",
    "* Input: $h_{t-1}$, $x_t$\n",
    "* Output: $h_t$\n",
    "\n",
    "It is simpler than the standard LSTM, increasingly popular. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_t &= \\sigma \\big( W_z \\times [h_{t-1}, x_t] \\big) \\\\\n",
    "r_t &= \\sigma \\big( W_r \\times [h_{t-1}, x_t] \\big) \\\\\n",
    "\\tilde{h}_t &= \\tanh \\big( W \\times [r_t \\times h_{t-1}, x_t] \\big) \\\\\n",
    "h_t &= (1 - z_t) \\times h_{t-1} + z_t \\times \\tilde{h}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Next big step is **attention**. This is as of 2015. See Olah's other [post](https://distill.pub/2016/augmented-rnns/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on RNN, Andrea Karpathy's [post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
