{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Book\n",
    "\n",
    "# Chapter 10 Sequence Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Forcing, p372\n",
    "\n",
    "A RNN whose only recurrence is the feedback connection from the output of the hidden layer, not the hidde layer itself, is **less powerful** than those which have direct connection from hidden layer $h^{(t-1)}$ to $h^{(t)}$. p370.\n",
    "\n",
    "The advantage of eliminating hidden-to-hidden recurrence is that, for any loss function based on comparing the prediction at time $t$ to the training target at time $t$, all the time steps are decoupled. Tranining can thus be parallelized, with gradient for each step t computed in isolation.\n",
    "\n",
    "Teacher forcing is a procedure that emerges from the maximum likelihood criterion, in which during training the model receives the ground truth output $y^{(t)}$ as input at time $t+1$, instead of the output from the previous step, thus parallelizing training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "\n",
    "The backprop algorithm applied to the unrolled graph with $\\mathbb{O}(\\tau)$ cost is called **back-propagation through time (BPTT)**. As soon as the hidden units become a function of earlier time steps, the BPTT algorithm is necessary.\n",
    "\n",
    "Example is based on the RNN architecture given in figure 10.3 on page 369. This network is described as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^{(t)} &= b + W h^{(t-1)} + U x^{(t)}, &(10.8)\\\\\n",
    "h^{(t)} &= \\tanh\\big(a^{(t)}\\big), &(10.9)\\\\\n",
    "o^{(t)} &= c + V h^{(t)}, &(10.10)\\\\\n",
    "\\hat{y}^{(t)} &= \\text{softmax}\\big( o^{(t)} \\big), &(10.11)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "* $x$ are input sequences\n",
    "* $o$ outputs of corresponding $x$\n",
    "* $y$ training target for corresponding $x$\n",
    "* $U$ weight matrix for input-to-hidden connections\n",
    "* $W$ weight matrix for hidden-to-hidden connections\n",
    "* $V$ weight matrix for hidden-to-output connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, **derivate of `softmax(x)`**, see this [post](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/), using the quotient rule:\n",
    "\n",
    "$$ \\big(\\frac{f}{g}\\big)' = \\frac{f'g - fg'}{g^2} $$\n",
    "\n",
    "Let $f = e^x$, $g = \\sum_i e^{x_i}$, $y = \\text{softmax}(x)$ and $y_i$ be the result for $x_i$, $x \\in R^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $i = j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial y_i}{\\partial x_i} &= \\frac{e^{x_i} g - e^{x_i} (0 + \\cdots + e^{x_i} + 0 + \\cdots + 0)}{g^2} \\\\\n",
    "&= \\frac{e^{x_i} g - e^{x_i} e^{x_i}}{g^2} \\\\\n",
    "&= \\frac{e^{x_i} (g - e^{x_i})}{g^2} \\\\\n",
    "&= \\frac{e^{x_i}}{g} \\times \\frac{g - e^{x_i}}{g}\\\\\n",
    "&= y_i \\times (1 - \\frac{e^{x_i}}{g}) \\\\\n",
    "&= y_i \\times (1 - y_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $i \\neq j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial y_i}{\\partial x_j} &= \\frac{0 \\times g - e^{x_i} e^{x_j}}{g^2} \\\\\n",
    "&= -\\frac{e^{x_i}}{g} \\frac{e^{x_j}}{g} \\\\\n",
    "&= - y_i y_j\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to BPTT, we start from the node immediately preceding the final loss:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial L^{t}} = 1 $$\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$(\\triangledown_{o^{t}}L)_i = \\frac{\\partial L}{\\partial o^{t}_i} = \\frac{\\partial L}{\\partial L^{t}} \\frac{\\partial L^{t}}{\\partial o^{t}_i} = \\hat{y}^{(t)}_i - 1_{1,y^{(t)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
