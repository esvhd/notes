{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "**SVM tends to do better then logistic regression when the classes are well separated. In the more overlapping regimes, logistics regression is often prefered.** ISRL, p357\n",
    "\n",
    "A great set of tutorials on SVM maths can be found here: \n",
    "\n",
    "http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Vector Maths\n",
    "\n",
    "Given a vector $v$ of $p$ dimension, $v=(v_1, v_2, \\ldots, v_p)$ its **magnitude or length or norm** is defined as: \n",
    "\n",
    "$$ \\lVert v \\rVert = \\sqrt{v^2} = \\sqrt{\\sum_{i=1}^{p}(v_i^2)} $$\n",
    "\n",
    "The **direction vector** of the $v$ is given by:\n",
    "\n",
    "$$u = \\frac{v}{\\lVert v \\rVert} = (\\frac{v_1}{\\lVert v \\rVert}, \\frac{v_2}{\\lVert v \\rVert}, \\ldots, \\frac{v_p}{\\lVert v \\rVert})$$ \n",
    "\n",
    "This is can also be derived from trigomitry, where for example in a 2-dimensional vector setting: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\cos(\\theta) &= \\frac{v_1}{\\lVert v \\rVert} \\\\\n",
    "\\ \\sin(\\theta) &= \\frac{v_2}{\\lVert v \\rVert} = \\cos(90^\\circ - \\theta) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Addition and Subtraction\n",
    "\n",
    "Subtraction is **not commutative**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ v + u &= (v_1 + u_1, v_2 + u_2, \\ldots, v_p + u_p) \\\\\n",
    "\\ v - u &= (v_1 - u_1, v_2 - u_2, \\ldots, v_p - u_p), \\text{vector points to }v \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Dot Product / Inner Product / Scaler Product are the same thing\n",
    "\n",
    "Given vector $x$ and $y$, and $\\theta$ is the angle between them:\n",
    "\n",
    "$$x \\cdot y = \\lVert x \\rVert \\lVert y \\rVert \\cos(\\theta) = \\sum_{i=1}^{p}(x_i y_i)$$\n",
    "\n",
    "### Orthogonal Projection\n",
    "\n",
    "Given vector $x$ and $y$, and $\\theta$ is the angle between them, let the projection of $x$ onto $y$ be $z$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ \\cos(\\theta) &= \\frac{\\lVert z \\rVert}{\\lVert x \\rVert}\\\\\n",
    "\\ \\lVert z \\rVert &= \\lVert x \\rVert \\cos(\\theta) \\\\\n",
    "\\ \\cos(\\theta) &= \\frac{x \\cdot y}{\\lVert x \\rVert \\lVert y \\rVert} \\\\\n",
    "\\ \\therefore \\lVert z \\rVert &= \\lVert x \\rVert \\frac{x \\cdot y}{\\lVert x \\rVert \\lVert y \\rVert} \\\\\n",
    "\\ \\lVert z \\rVert &= \\frac{x \\cdot y}{\\lVert y \\rVert} \\\\\n",
    "\\ \\text{Define direction of }y = u &= \\frac{y}{\\lVert y \\rVert} \\\\\n",
    "\\ therefore \\lVert z \\rVert &= u \\cdot x \\\\\n",
    "\\ \\because & u \\text{ and } y \\text{ are in the same direction} \\\\\n",
    "\\ u &= \\frac{z}{\\lVert z \\rVert} \\\\\n",
    "\\ z &= \\lVert z \\rVert u = u \\cdot x \\cdot u \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "With orthogonal projection, we can caluclate the distance from $x$ to the line that goes through $y$ as $\\lVert x-z \\rVert$.\n",
    "\n",
    "### Hyperplane\n",
    "\n",
    "A hyperplane is defined as follows, with $w$ being weights:\n",
    "\n",
    "$$ w^T x = 0 $$\n",
    "\n",
    "An important property is that the vector $w$ is **perpendicular to the hyperplane**. \n",
    "\n",
    "### Distance of a point to a hyperplane\n",
    "\n",
    "Let point $A$ be a point outside the hyperplane $H_0$ defined by $w^T x = 0$. We can use trigonometry to find the perpendicular distance from $A$ to $H_0$.\n",
    "\n",
    "In principle, to do so we need to project vector $A$ onto $w$, call it $p$, and then find the magnitude of $p$. \n",
    "\n",
    "We know that $p = \\frac{y}{\\lVert y \\rVert} \\cdot A \\cdot \\frac{y}{\\lVert y \\rVert}$. Therefore, we also konw $\\lVert p \\rVert$.\n",
    "\n",
    "### Margin of the hyperplane, m\n",
    "\n",
    "$$ margin = 2\\lVert p \\rVert $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "URL: http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/\n",
    "\n",
    "We want to find the hyperplane $H: w^T x + b = 0$ with maximum margin $m/2$ that separates a dataset $\\mathcal{D} = \\{ (x_i, y_i) \\mid x_i \\in \\mathbb{R}^p, y_i \\in \\{-1, +1\\}\\}_{i=1}^{n}$, provided that $\\mathcal{D}$ is linearly separable. \n",
    "\n",
    "Let $H_0$ and $H_1$ be two hyperplanes on each side of $H$, with **margin** distance of $m$ each. Defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ H_0: w^T x + b &= -1 \\\\\n",
    "\\ H_1: w^T x + b &= 1 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**The problem is how to find H while maximizing $m$.** Since $w$ is perpendicular to $H, H_0, H_1$, let $u$ be the unit vector:\n",
    "\n",
    "$$ u = \\frac{w}{\\lVert w \\rVert} $$\n",
    "\n",
    "Let $k$ be the vector that represent $m$, we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ k &= m \\cdot u = m \\frac{w}{\\lVert w \\rVert} \\\\\n",
    "\\ \\lVert k \\rVert &= m \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "First, we have to find $m$. Let $x_0$ be a point on $H_0$. We can find a point $z_0$ on $H_1$, defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ z_0 &= x_0 + k \\\\\n",
    "\\ \\therefore w \\cdot x_0 + b &= -1 \\\\\n",
    "\\ \\text{and } w \\cdot z_0 + b &= 1 \\\\\n",
    "\\ w \\cdot (x_0 + k) + b &= 1 \\\\\n",
    "\\ w \\cdot (x_0 + m \\frac{w}{\\lVert w \\rVert}) + b &= 1 \\\\\n",
    "\\ w \\cdot x_0 + m \\frac{w \\cdot w}{\\lVert w \\rVert} + b &= 1 \\\\\n",
    "\\ w \\cdot x_0 + m \\frac{\\lVert w \\rVert^2}{\\lVert w \\rVert} + b &= 1 \\\\\n",
    "\\ w \\cdot x_0 + m \\lVert w \\rVert + b &= 1 \\\\\n",
    "\\ w \\cdot x_0 + b &= 1 - m \\lVert w \\rVert \\\\\n",
    "\\ -1 &= 1 - m \\lVert w \\rVert \\\\\n",
    "\\ m &= \\frac{2}{\\lVert w \\rVert}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, maximizing margin $m$ is the same as minimizing $w$. The SVC problem boils down to the optimization problem of the following:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{minimize }}& \\lVert w \\rVert \\\\\n",
    "& \\text{subject to: }& y_i(w \\cdot x_i + b) \\geq 1 \\\\\n",
    "& &\\forall i = 1, 2 \\ldots, n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Maths Concepts for Optimizations\n",
    "\n",
    "Reference: http://www.svm-tutorial.com/2016/09/unconstrained-minimization/ \n",
    "\n",
    "#### Positive Definite Matrix\n",
    "\n",
    "The following statements are equivalent:\n",
    "\n",
    "* The symmetric matrix $A$ is positive definite.\n",
    "* All eigenvalues of $A$ are positive.\n",
    "* All the leading principal minors of $A$ are positive.\n",
    "* There exists nonsingular square matrix $B$ such that $A=B^TB$\n",
    "\n",
    "Source: http://www.math.ucsd.edu/~njw/Teaching/Math271C/Lecture_03.pdf\n",
    "\n",
    "#### Positive Semi-definite Matrix\n",
    "\n",
    "* The symmetric matrix $A$ is positive semi-definite.\n",
    "* All eigenvalues of $A$ are non-negative.\n",
    "* All the leading principal minors of $A$ are non-negative.\n",
    "* There exists nonsingular square matrix $B$ such that $A=B^TB$\n",
    "\n",
    "Source: http://www.math.ucsd.edu/~njw/Teaching/Math271C/Lecture_03.pdf\n",
    "\n",
    "#### Principal minor\n",
    "\n",
    "A minor of matrix $A$ of order $k$ is principal if it is obtained by deleting $n−k$ rows and the $n−k$ columns with the same numbers.\n",
    "\n",
    "#### Leading principal minor\n",
    "\n",
    "The **leading principal minor** of matrix $A$ of order $k$ is the minor of order $k$ obtained by deleting the last $n−k$ rows and columns, where $n$ is the dimension of a symmetric matrix.\n",
    "\n",
    "\n",
    "#### Convex function \n",
    "\n",
    "https://en.wikipedia.org/wiki/Convex_function \n",
    "\n",
    "More generally, a continuous, twice differentiable function of several variables is convex on a convex set **if and only if its Hessian matrix is positive semidefinite on the interior of the convex set**.\n",
    "\n",
    "If we want to check if a function is convex, one easy way is to check if the Hessian matrix is positive semi-definite.\n",
    "\n",
    "More on this: http://www.math.cmu.edu/~ploh/docs/math/mop2013/convexity-soln.pdf\n",
    "\n",
    "#### Lagrange multipliers\n",
    "\n",
    "In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints. (Wikipedia)\n",
    "\n",
    "**Lagrange multipliers only work with equality constraints**.\n",
    "\n",
    "Problem construct:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{x, y}{\\text{minimize }}& f(x,y) \\\\\n",
    "& \\text{subject to: }& g_1(x,y) = 0 \\\\\n",
    "& & g_2(x,y) = 0 \\\\\n",
    "& & \\ldots \\\\\n",
    "& & g_n(x,y) = 0 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Lagrange found that the minimum of $f(x,y)$ under the constraint $g(x,y)=0$ is obtained **when their gradients point in the same direction** (e.g. on a contour plot for 3D problems).\n",
    "\n",
    "Define **Lagrangian function**, with $\\lambda$ being the **Lagrange multiplier**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(x,y,\\lambda) &= f(x,y) - \\sum_{i=1}^n\\lambda_i g_i(x,y) \\\\\n",
    "\\ \\text{and the gradient } \\partial{\\mathcal{L}(x,y,\\lambda)} &= \\partial{f(x,y)} - \\sum_{i=1}^n\\lambda_i \\partial{g_i(x,y)}\\\\\n",
    "\\ \\text{Solve for } \\partial{\\mathcal{L}(x,y,\\lambda)} &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By solving for this we will find the solutions for $x, y, \\lambda_i$ at the same time by solveing simultaneous equations from above.\n",
    "\n",
    "More on equality and inequality constraints: http://www.engr.mun.ca/~baxter/Publications/LagrangeForSVMs.pdf\n",
    "\n",
    "To adapte for **inequality** constraints, the problem is formed in the same ways with additional rules applied as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ g(x,y) \\geq 0 &\\Rightarrow \\lambda \\geq 0 \\\\\n",
    "\\ g(x,y) \\leq 0 &\\Rightarrow \\lambda \\leq 0 \\\\\n",
    "\\ g(x,y) = 0 &\\Rightarrow \\lambda \\text{ is unconstrainted} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier with Overlaping Classes\n",
    "\n",
    "Maths notes: http://www.tristanfletcher.co.uk/SVM%20Explained.pdf\n",
    "\n",
    "**Based on ESL print 10, chapter 12.**\n",
    "\n",
    "**ESL** defines the margin as $M=\\frac{m}{2} = \\frac{1}{\\lVert w \\rVert}$, where $m$ is the margin definition from the notes aboce.\n",
    "\n",
    "To allow for overlapping classes, ESL introduces **slack variables**:\n",
    "\n",
    "$$ \\xi = \\big(\\xi_1, \\xi_2, \\ldots, \\xi_N \\big) $$\n",
    "\n",
    "The constraints of the previous optimization problem are modified:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{minimize }}& \\lVert w \\rVert \\\\\n",
    "& \\text{subject to: }& y_i(w \\cdot x_i + b) \\geq M(1-\\xi_i) \\\\\n",
    "& & \\xi_i \\geq 0 \\\\\n",
    "& &\\forall i = 1, 2 \\ldots, N \\\\\n",
    "& &\\sum_{i=1}^{N}\\xi_i \\leq \\text{constant (C)} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Computationally this is the same as (p420):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{minimize }}& \\frac{1}{2}\\lVert w \\rVert + C\\sum_{i=1}^{N}\\xi_i \\\\\n",
    "& \\text{subject to: }& y_i(w \\cdot x_i + b) \\geq (1-\\xi_i) \\\\\n",
    "& & \\xi_i \\geq 0 \\\\\n",
    "& &\\forall i = 1, 2 \\ldots, N\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(w) &= \\frac{1}{2}\\lVert w \\rVert + C\\sum_{i=1}^{N}\\xi_i \\\\\n",
    "g(w, y, x, \\xi) &= y(w \\cdot x + b) - (1-\\xi) \\\\\n",
    "h(\\xi) &= \\xi\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore the Lagrange (primal) function is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_P &= f(w) - \\sum_{i=1}^{N}\\alpha_i g(w, y_i, x_i, \\xi_i) - \\sum_{i=1}^{N} \\mu_i h(\\xi_i) \\\\\n",
    "& \\text{where } \\alpha_i \\text{ and } \\mu_i \\text{ are Lagrange multipliers} \\\\\n",
    "L_P &= \\frac{1}{2}\\lVert w \\rVert + C\\sum_{i=1}^{N}\\xi_i\n",
    "- \\sum_{i=1}^{N}\\alpha_i \\big[ y_i(w \\cdot x_i + b) - (1-\\xi_i) \\big] \n",
    "- \\sum_{i=1}^{N}\\mu_i \\xi_i \\\\\n",
    "\\frac{\\partial{L_P}}{\\partial{w}} &= w -\\sum_{i=1}^{N}\\alpha_i y_i x_i = 0 \\Rightarrow\n",
    "w = \\sum_{i=1}^{N}\\alpha_i y_i x_i \\\\\n",
    "\\frac{\\partial{L_P}}{\\partial{b}} &= \\sum_{i=1}^{N}\\alpha_i y_i = 0 \\\\\n",
    "\\frac{\\partial{L_P}}{\\partial{\\xi_i}} &= C - \\sum_{i=1}^{N}\\alpha_i - \\sum_{i=1}^{N}\\mu_i = 0 \n",
    "\\Rightarrow \\alpha_i = C - \\mu_i, \\forall i \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Substituting $w, \\sum_{i=1}^{N}\\alpha_i y_i = 0, \\alpha_i = C - \\mu_i$ into $L_P$, we have the **Lagrangian (Wolfe) dual objective function**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_D &= \\frac{1}{2}\\bigg[\\sum_{i=1}^{N}\\alpha_i y_i x_i \\bigg]^2 + C\\sum_{i=1}^{N}\\xi_i\n",
    "- \\sum_{i=1}^{N}\\alpha_i \\bigg[ y_i (\\sum_{j=1}^{N}\\alpha_j y_j x_i^T x_j + b) - (1 - \\xi_i)\\bigg]\n",
    "- \\sum_{i=1}^{N}\\mu_i \\xi_i\\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \n",
    "+ C\\sum_{i=1}^{N}\\xi_i\n",
    "- \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \n",
    "+ \\sum_{i=1}^{N} \\alpha_i y_i b + \\sum_{i=1}^{N} \\alpha_i (1 - \\xi_i)\n",
    "- \\sum_{i=1}^{N}\\mu_i \\xi_i\\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \n",
    "+ C\\sum_{i=1}^{N}\\xi_i\n",
    "- \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \n",
    "+ 0 + \\sum_{i=1}^{N} \\alpha_i -\\sum_{i=1}^{N} \\alpha_i \\xi_i\n",
    "- \\sum_{i=1}^{N}\\mu_i \\xi_i\\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \n",
    "+ \\sum_{i=1}^{N} \\alpha_i - \\bigg[\\sum_{i=1}^{N} (\\alpha_i - C + \\mu_i) \\xi_i \\bigg]\\\\\n",
    "&= \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i^T x_j - 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We maximize $L_D$ subject to the following Lagrange and **Karush-Kuhn-Tucker** [(3)-(5)] constraints:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 \\leq \\alpha_i \\leq C & \\,\\,\\, (1)\\\\\n",
    "\\sum_{i=1}^{N} \\alpha_i y_i = 0 & \\,\\,\\, (2)\\\\\n",
    "\\alpha_i y_i(w \\cdot x_i + b) - (1-\\xi_i) = 0 & \\,\\,\\, (3)\\\\\n",
    "\\mu_i \\xi_i = 0 & \\,\\,\\, (4)\\\\\n",
    "y_i(w \\cdot x_i + b) - (1-\\xi_i) \\geq 0 & \\,\\,\\, (5)\\\\\n",
    "\\forall i = 1, \\ldots, N\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "We can rewrite $L_D$ with a **kernel function $K$**: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_D &= \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\\\\\n",
    "\\text{where: } K(x_i, x_j) &= \\langle h(x_i), h(x_j) \\rangle \n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "For the equations above, $K(x_i, x_j) = x_i^T x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Popular kernal functions**:\n",
    "\n",
    "* linear kernel, just the original inner products, $K(x, x^{'}) = \\langle x_i, x_i^{'} \\rangle$, turns SVM to classical SVC.\n",
    "* d-degree polynominal, $K(x, x^{'}) = \\big( 1 + \\sum_{j=1}^{p} x_{ij}x_{i^{'}j}\\big)^d $\n",
    "* Radial basis, $K(x, x^{'}) = \\exp \\big( -\\gamma \\sum_{j=1}^{p} (x_{ij} - x_{i^{'} j})^2 \\big)$\n",
    "* Neural network, $K(x, x^{'}) = \\tanh (k_1 \\langle x, x^{'} \\rangle + k_2)$\n",
    "\n",
    "Kernels must be [symmetric](https://en.wikipedia.org/wiki/Symmetric_function) **positive (semi-) definite** function, [Mercer's Theorem](https://en.wikipedia.org/wiki/Mercer%27s_theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Radial basis kernals**: when a test observation $x^*$ is very **far** away from a training pint $x_i$ in terms of Euclidean distance, its associated kernel value will be **close to zero**, looking at the equation above (exponential of a very negative number). Therefore it means that $x_i$ will play almost **no role** in $f(x^*)$. This means that the radial basis kernel has very **local** behaviour, in the sense that only training data nearby of test data will have an effect on the prediction. (See ISLR p353)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding $\\alpha_i$ and $w$\n",
    "\n",
    "From this optimization we can find $alpha_i$, and therefore we can find $w$ with **non-zero** coefficients $\\hat{\\alpha}_i$, whose indices $i$'s indicate **support vectors**, due to (3) constraint above.\n",
    "\n",
    "### Finding $b$\n",
    "\n",
    "For some of the support vectors, they will lie exaclty on the edge of the margin ($\\xi_i = 0$). Based on $\\alpha_i = C - \\mu_i, \\forall i$ and constraint (4), these **margin points** will have $ 0 < \\hat{\\alpha}_i < C$. \n",
    "\n",
    "We can use any of these points with constraint (5) above to solve for $b$. **In practice, we use all of these points to calculate $b$ and take the average for numerical stability.**\n",
    "\n",
    "Note that the remaining support vectors ($\\xi_i > 0$) have $\\hat{\\alpha}_i = C$.\n",
    "\n",
    "Let $S$ represent the margin points, \n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_s \\big[y_s (w \\cdot x_s + b)\\big] &= 1 \\times y_s \\\\\n",
    "y_s^2 w \\cdot x_s + y_s^2 b &= y_s \\\\\n",
    "\\because \\, y_s^2 &= 1 \\\\\n",
    "w \\cdot x_s + b &= y_s \\\\\n",
    "b &= y_s - w \\cdot x_s \\\\\n",
    "\\therefore \\, \\hat{b} &= \\frac{1}{N_S}\\sum_{i=1}^{N_S}(y_s - w \\cdot x_s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Prediction\n",
    "\n",
    "For any new point $x'$, $y' = sign(w \\cdot x' + b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "\n",
    "Similar to the classification case, **but need to introduce a tolerance band**, $\\epsilon$. Penalty is allocated to points where $y_i - \\hat{y}_i > \\epsilon$, using slack variable, either of $\\{\\xi^+, \\xi^-\\}$. \n",
    "\n",
    "ESL introduces two penalty functions: 1) standard $V_\\epsilon$ and 2) Huber $V_H$. **Huber is compared with robust regression**, as it assigns less penalty to large outliers, i.e. linear rather than quadratic.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_\\epsilon(r) &= \\begin{cases}\n",
    "0 & if \\mid r\\mid < \\epsilon \\\\\n",
    "\\mid r \\mid - \\epsilon & otherwise\n",
    "\\end{cases} \\\\\n",
    "V_H(r) &= \\begin{cases}\n",
    "r^2 / 2 & if \\mid r \\mid \\leq c \\\\\n",
    "c\\mid r \\mid - c^2 / 2 & \\mid r \\mid > c\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\epsilon$ is another tuning parameter that can be chosen by CV?\n",
    "\n",
    "ESL mentioned that if we **scaled the response $r$**, preset values for $c$ and $\\epsilon$ can be chosen, while $lambda$ is chosen with CV. (E.g. c = 1.345 achieves 95% efficiency for the Gaussian.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on $C$ in SVM\n",
    "\n",
    "Consider $C$ as the **budget** of how much to allow classes to lie on the wrong side of the hyperplane.\n",
    "\n",
    "Large $C$: lower bias, higher variance\n",
    "\n",
    "Small $C$: higher bias, lower variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuSVM\n",
    "\n",
    "Replace $C$ with $\\mu$, where $\\mu \\in (0, 1]$, essentially tranform from an infinite numerical range for $C$ to ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
