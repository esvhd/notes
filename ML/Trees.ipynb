{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees, Random Forests, Etc\n",
    "\n",
    "Mostly based on **ESL** and **ISLR**\n",
    "\n",
    "`R` package `tree`, useful function:\n",
    "\n",
    "* `cv.tree()` performances cross-validation in order to determine the complexity of the tree.\n",
    "* `predict()`\n",
    "* `prune.tree()`\n",
    "\n",
    "`R` package `randomForest`:\n",
    "\n",
    "* `importance()` - variable importance\n",
    "* `varImpPlot()` - plot variable importance\n",
    "\n",
    "`R` package `gbm` for boosting trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "One major problem with trees is their high variance.\n",
    "\n",
    "## Tree Pruning\n",
    "\n",
    "A better strategy is to grow a very large tree $T_0$, then **prune** it back in order to obtain a subtree that leads to lowest test error.\n",
    "\n",
    "**Cost complexity pruning**, a.k.a. **weakest link pruning**: consider a sequence of trees indexed by nonnegative tuning parameter $\\alpha$. For each value of $\\alpha$ there corresponds a subtree $T \\in T_0$ such that:\n",
    "\n",
    "$$ \\sum_{m=1}^{|T|} \\sum_{x_i \\in R_m} \\big(y_i - \\hat{y}_{R_m} \\big)^2 + \\alpha |T| $$\n",
    "\n",
    "is as small as possible. \n",
    "\n",
    "* $|T|$ indicates the number of terminal nodes of the tree $T$. \n",
    "* $R_m$ is the rectangle (i.e. the subset of predictor space) corresponding to the $m^{th}$ terminal node.\n",
    "* $\\hat{y}_{R_m}$ is the predicted response associated with $R_m$\n",
    "\n",
    "$\\alpha$ controls a trade-off between the subtree's complexity and its fit to the training data. Turns out as we increase $\\alpha$ from zero, branches get pruned from the tree in a nested and predictable fashion. The value of $\\alpha$ can be found using cross-validation. Algorithm 8.1 on page 309 in **ISLR** book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Bootstrap aggreation, or bagging, builds $B$ regression trees with $B$ bootstrapped training set, and average the resulting predictions. For classification, a majority vote is used.\n",
    "\n",
    "The trees built are grown deep, and not pruned. **Hence each tree has high variance but low variance**.\n",
    "\n",
    "## Out-of-Bag Error Estimate\n",
    "\n",
    "It can be shown that with bootstrapping, on average, each bagged tree makes use of around 2/3 of the observations. The remaining 1/3 of the data is not used to fit the tree, hence these are referred to as **out-of-bag** (OOB) observations. \n",
    "\n",
    "We can predict the response for the $i^{th}$ obervation using each of the trees in which the observation was OOB. This yields around B/3 predictions for the $i^{th}$ observation. We can use average of these (for regression) or majority vote (for classification) to get a single OOB prediction for this observation. \n",
    "\n",
    "It can be shown that when B is sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error.\n",
    "\n",
    "# Variable Importance\n",
    "\n",
    "**Regression**: record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all B trees. A **large** value indicates an important predictor. **ISLR** p319.\n",
    "\n",
    "$$ RSS = \\sum_{j=1}^{J} \\sum_{i \\in R_j} \\big(y_i - \\hat{y}_{R_j} \\big)^2 $$\n",
    "\n",
    "**Classification**: add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all B trees. \n",
    "\n",
    "$$ G = \\sum_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk}) $$\n",
    "\n",
    "Where $\\hat{p}_{mk}$ represents the proportion of training observations in the $m^{th}$ region that are the from the $k^{th}$ class.\n",
    "\n",
    "Alternative to the Gini index is **cross-entropy**, given by:\n",
    "\n",
    "$$ D = -\\sum_{k=1}^{K} \\hat{p}_{mk} \\log \\hat{p}_{mk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "For bagged trees, strong predictors will result in correlated trees, therefore the reduction in variance would be limited. \n",
    "\n",
    "Random forest overcomes this issue by having each tree only using a **randomly chosen** subset of $m$ predictors. Typically, $m = \\sqrt{p}$, where $p$ is the number of all predictors.\n",
    "\n",
    "On average $(p - m)/p$ of the splits in a tree will not even consider the strong predictor, so the other predictors would have a change.\n",
    "\n",
    "Some measures of confidence for Random Forest have been developed, such as: \n",
    "\n",
    "* [forestci](https://github.com/scikit-learn-contrib/forest-confidence-interval) package for `scikit-learn`\n",
    "* [randomForestCI](https://github.com/swager/randomForestCI) for `R` and the associated paper.\n",
    "\n",
    "See this [notebook](https://github.com/ianozsvald/data_science_delivered/blob/master/ml_explain_regression_prediction.ipynb) for examples. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Not using bootstrapping, trees are grown **sequentially**. Boosting creates **slow learners**. Each tree can be rather small, with just a few termina lnodes, determined by the parameter $d$ in the algorithm. \n",
    "\n",
    "Given the current model, we fit a tree to the residuals from the model, then we add this tree into the fitted function in order to update the residuals. \n",
    "\n",
    "A shrinkage parameter $\\lambda$ slows down learning further. \n",
    "\n",
    "Third parameter is number of trees, B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
