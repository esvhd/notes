{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera Deeplearning.ai Notes\n",
    "\n",
    "[link](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Logistic Regression](#logistic)\n",
    "* [Logistic Loss Function](#logistic_loss)\n",
    "* [Backprop for Logistic Regression](#logistic_deriv)\n",
    "* [Activation & Derivatives](#activation)\n",
    "* [Gradient Descent](#grad)\n",
    "* [Dimensions](#dimension)\n",
    "* [Dimensions Again](#dimension_again)\n",
    "* [Random Initialization](#random_init)\n",
    "* [Regularization](#regularization)\n",
    "* [Dropout](#dropout)\n",
    "* [Optimization](#opt)\n",
    "* [Weight Initialization](#weight_init)\n",
    "* [Gradient Checking](#grad_checking)\n",
    "* [Minibatch](#minibatch)\n",
    "* [Momentum, RMSprop, Adam](#opt_algo)\n",
    "* [Random Search](#random_search)\n",
    "* [Batch Norm](#batchnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1\n",
    "\n",
    "# Week 1\n",
    "\n",
    "Some points from Hinton's interview\n",
    "\n",
    "* Residual Networks - linked to initalization with identity matrix\n",
    "* Hinton showed that ReLU is approximately a stack of logistic units.\n",
    "* Hinton working on capusles, essentially it is a DL network that partitions neurons into groups that represent features of a single input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "## Binary Classification\n",
    "\n",
    "A picture of size 64 x 64 in RGB is represented by a 3 x 64 x 64 tensor. Unrolling this would save this into a 1D vector.\n",
    "\n",
    "## Notation\n",
    "\n",
    "$ (x,y), x \\in \\mathbb{R}^{n_x}, y \\in \\{0, 1\\} $\n",
    "\n",
    "For $m$ training examples: $\\big \\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots, (x^{(m)}, y^{(m)}) \\big \\} $ \n",
    "\n",
    "**Data matrix dimension** is $n_x \\times m$, i.e. $X \\in \\mathbb{R}^{n_x \\times m}$, this is `numpy` format and same as `X.shape` in `python`.\n",
    "\n",
    "$Y = [y^{(1)}, y^{(2)}, \\cdots, y^{(m)}], Y \\in \\mathbb{R}^{1 \\times m}$, hence $Y.shape == (1, m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<id a='logistic'></a>\n",
    "## Logistic Regression\n",
    "\n",
    "$$ \\hat{y} = \\sigma\\big(w^T x + b\\big), \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Loss function**: squared loss not working here, because the problem becomes non-convex. So the correct one is:\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}, y) = - \\big(y \\log \\hat{y} + (1 - y)\\log(1-\\hat{y})\\big) $$\n",
    "\n",
    "This is derived as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y} &= p(y=1 \\mid x) \\\\\n",
    "\\text{If } y=1 &: p(y\\mid x) = \\hat{y}\\\\\n",
    "\\text{If } y=0 &: p(y\\mid x) = 1 - \\hat{y}\\\\\n",
    "p(y \\mid x) &= \\hat{y}^y (1-\\hat{y})^{(1-y)} \\\\\n",
    "\\log p(y \\mid x) &= y\\log\\hat{y} + (1-y)\\log(1-\\hat{y})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<a id='logistic_loss'></a>\n",
    "**Cost function**:\n",
    "\n",
    "$$ J(w, b) = \\frac{1}{m}\\sum_{i=1}^{m}\\mathcal{L}\\big(\\hat{y}^{(i)}, y^{(i)}\\big) $$\n",
    "\n",
    "This means that the cost function is a **maximizing likelihood estimator**.\n",
    "\n",
    "**Gradient Descent**\n",
    "\n",
    "    Repeat {\n",
    "        w := w - alpha * dJ(w,b)/dw\n",
    "        b := b - alpha * dJ(w,b)/db\n",
    "    } until minimum reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph\n",
    "\n",
    "A graph that represents all temp variables for a formula.\n",
    "\n",
    "**Notation**: $dz$ always refers to the deriviative of the final loss function $\\mathcal{L}$ with respect to $z$.\n",
    "\n",
    "For logistic regression of $X \\in \\mathbb{R}^{2}$:\n",
    "\n",
    "**Forward pass**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z &= w^T X + b = w_1 x_1 + w_2 x_2 + b \\\\\n",
    "a &= \\sigma(z) \\\\\n",
    "\\mathcal{L}(a, y) &= -\\big(y\\log a + (1-y) \\log (1-a)\\big)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<a id='logistic_deriv'></a>\n",
    "**Backprop**:\n",
    "\n",
    "Since $\\frac{d}{dx}\\log x = \\frac{1}{x}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "da &= \\frac{\\partial \\mathcal{L}}{\\partial a} \n",
    "= -\\big[ \\frac{y}{a} + \\frac{1-y}{1-a} (-1) \\big] \\\\\n",
    "&= -\\frac{y}{a} + \\frac{1 - y}{1 - a} \\\\\n",
    "\\frac{\\partial a}{\\partial z} &= a(1-a) \\\\\n",
    "dz &= \\frac{\\partial \\mathcal{L}}{\\partial z} \n",
    "= \\frac{\\partial \\mathcal{L}}{\\partial a} \\frac{\\partial \\mathcal{a}}{\\partial z}\n",
    "= \\bigg[-\\frac{y}{a} + \\frac{1 - y}{1 - a}\\bigg] \\big[a(1-a)\\big] \\\\\n",
    "&= -y(1-a) + (1-y)a = -y + ay + a - ay \\\\\n",
    "&= a - y \\\\\n",
    "dw1 &= dz \\frac{\\partial z}{\\partial w1} = x_1 dz \\\\\n",
    "dw2 &= dz \\frac{\\partial z}{\\partial w2} = x_2 dz \\\\\n",
    "db &= dz \\frac{\\partial z}{\\partial b} = dz\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then update $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Implementation\n",
    "\n",
    "**Forward Pass**:\n",
    "\n",
    "Avoid the for loop to go through all $m$ training examples by using `np.dot(w.T, X)`.\n",
    "\n",
    "**Backward Pass**:\n",
    "\n",
    "$A = [a^{(1)}, a^{(2)}, \\cdots, a^{(n)}], Y=[y^{(1)}, y^{(2)}, \\cdots, y^{(n)}]$\n",
    "\n",
    "Therefore $dz = A - Y$, $dw = \\frac{1}{m}X dz^T$\n",
    "\n",
    "**Python tip**: \n",
    "\n",
    "* Don't use `np.random.rand(5)` but use `np.random.rand(5,1)`. The formal gives shape `(5,)`, the latter gives shape `(5,1)`. Use `keepdim=True` parameter in `numpy` functions to avoid rank 1 arrays.\n",
    "* use `assert()` to ensure shape in code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Neural Nets\n",
    "\n",
    "Counting layers: input layer not counted. \n",
    "\n",
    "## Vectorization\n",
    "\n",
    "**Notation** \n",
    "\n",
    "$W^{[i]}$ represents the the weight matrix for layer $i$, where **each row $j$**, $(w_{j}^{[i]})^{T}$, stores weights for a neuron in this layer. \n",
    "\n",
    "$[\\cdot]$ brackets are used to denote **layers**, $(\\cdot)$ brackets are used to denote **training examples**, $\\{\\cdot\\}$ denotes **minibatches**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='activation'></a>\n",
    "## Activation\n",
    "\n",
    "$\\tanh$ **almost always work better than** sigmoid function. The **exception** is the output layer. \n",
    "\n",
    "**ReLU** tend to be the default these days. **Leaky ReLU** tends to work better but not widely used.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tanh(z) &= \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\\\\n",
    "\\text{ReLU} &= max(0, z) \\\\\n",
    "\\text{Leaky ReLU} &= max(0.01z, z)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Derivatives of Activation Functions\n",
    "\n",
    "Let $g(z)$ be an activation function.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g_{sigmoid}'(z) &= g_{sigmoid}(z)(1-g_{sigmoid}(z)) \\\\\n",
    "g_{tanh}'(z) &= 1 - (\\tanh(z))^2 \\\\\n",
    "g_{ReLU}'(z) &= \\begin{cases}\n",
    "0 &\\text{for } z < 0 \\\\\n",
    "1 &\\text{for } z > 0 \\\\\n",
    "\\text{Undefined} &\\text{for } z = 0\n",
    "\\end{cases} \\\\\n",
    "g_{Leaky ReLU}'(z) &= \\begin{cases}\n",
    "0.01 &\\text{for } z < 0 \\\\\n",
    "1 &\\text{for } z > 0 \\\\\n",
    "\\text{Undefined} &\\text{for } z = 0\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In practice for ReLU and Leaky ReLU, we ignored the $z=0$ case in code and instead set gradient to 1 for $z \\geq 0$. In this case $g'(z)$ becomes a **sub gradient** of the activation function $g(z)$ which is why gradient descent still works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grad'></a>\n",
    "## Gradient Descent \n",
    "\n",
    "Assume that activations are reprsented as $A^{[l]} = g^{[l]}\\big( Z^{[l]} \\big)$.\n",
    "\n",
    "\n",
    "$\\odot$ - Hadamard product / element-wise product\n",
    "\n",
    "**Forward Propagation** for a 2-layer network in matrix form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{[1]} &= W^{[1]}X + b^{[1]} \\\\\n",
    "A^{[1]} &= g^{[1]}\\big(z^{[1]}\\big)\\\\\n",
    "Z^{[2]} &= W^{[2]}A^{[1]} + b^{[2]} \\\\\n",
    "A^{[2]} &= g^{[2]}\\big(Z^{[2]}\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To generalize:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{[l]} &= W^{[l]}A^{[l-1]} + b^{[l]} \\\\\n",
    "A^{[l]} &= g^{[l]}\\big(Z^{[l]}\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Backpropation** (refer to derivations [above](#logistic_deriv) for vector form) in matrix form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dZ^{[2]} &= A^{[2]} - Y \\\\\n",
    "dW^{[2]} &= \\frac{1}{m} dZ^{[2]}\\big(A^{[1]}\\big)^T \\\\\n",
    "db^{[2]} &= \\frac{1}{m} \\mathsf{np.sum}\\big(dZ^{[2]}, \\mathsf{axis=1, keepdims=True} \\big) \\\\\n",
    "dZ^{[1]} &= (W^{[2]})^T dZ^{[2]} \\odot g^{[1]'}\\big(Z^{[1]}\\big) \\\\\n",
    "dW^{[1]} &= \\frac{1}{m} dZ^{[1]}X^T \\\\\n",
    "db^{[1]} &= \\frac{1}{m} np.sum(dZ^{[1]}, axis=1, keepdims=True)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To generalize:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dZ^{[L]} &= A^{[L]} - Y \\\\\n",
    "dW^{[L]} &= \\frac{1}{m} dZ^{[L]}(A^{[L-1]})^T \\\\\n",
    "db^{[L]} &= \\frac{1}{m} \\mathsf{np.sum}\\big(dZ^{[L]}, \\mathsf{axis=1, keepdims=True} \\big) \\\\\n",
    "dZ^{[l]} &= dA^{[l]} \\odot g^{[l]'} \\big( Z^{[l]} \\big) \\\\\n",
    "dW^{[l]} &= \\frac{1}{m} dZ^{[l]}\\big(A^{[l-1]}\\big)^T \\\\\n",
    "db^{[l]} &= \\frac{1}{m} \\mathsf{np.sum}\\big(dZ^{[l]}, \\mathsf{axis=1, keepdims=True} \\big) \\\\\n",
    "dA^{[l-1]} &= (W^{[l]})^T dZ^{[l]} \\\\\n",
    "dZ^{[l]} &= (W^{[l+1]})^T dZ^{[l+1]} \\odot g^{[l]'}\\big(Z^{[l]}\\big)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the bias derivative terms are **summed across all training examples** because gradients accumulate over the training set.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='dimension'></a>\n",
    "### Dimensions\n",
    "\n",
    "`foo` and `dfoo` **always** have the same dimension.\n",
    "\n",
    "The transpose of $A^{[1]}$ in line 2 above is because $w$ before is a **column vector**, whereas $W$ is a matrix stacking $w$'s as rows. \n",
    "\n",
    "**Dimension** of $W^{[i]}$ is $\\big(n^{[i]}, n_{A^{[i-1]}}\\big)$, where $n^{[i]}$ is the no. of neurons in this layer, i.e. its width, and $n_{A^{[i-1]}}$ is the no. of rows of $A^{[i-1]}$, e.g. $n_{A^{[0]}} = n_x$.\n",
    "\n",
    "Dimension of $dZ^{[1]}$ is $(n^{[1]}, m)$, so are the dimensions of the two terms that formed the equation, i.e. they have the same dimension, therefore the we have an element-product.\n",
    "\n",
    "### Pattern\n",
    "\n",
    "1. forward propagation, compute output and store intermediate outputs of each layer in cache. \n",
    "    * Inputs: $A^{[l-1]}, W^{[l]}, b^{[l]}$, \n",
    "    * output: $Z^{[l]}$ and $A^{[l]}$ and cache $Z^{[l]}, W^{[l]}, b^{[l]}$.\n",
    "2. compute loss\n",
    "3. backprop.\n",
    "    * inputs: $dA^{[l]}, dZ^{[l]}$,  cache of $Z^{[l]}, W^{[l]}, b^{[l]}$\n",
    "    * output: $dA^{[l-1]}$, caches $dW^{[l]}, db^{[l]}$.\n",
    "4. update parameters\n",
    "\n",
    "## Working out the dimensions\n",
    "\n",
    "Follow the shape of the network, the number of rows for the weight matrix $W$ for a layer should be the same as the no. of neurons in the layer. \n",
    "\n",
    "For the output layer, the dimension must match the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='random_init'></a>\n",
    "## Random Initalization\n",
    "\n",
    "Bias terms an be initalizaed to zero.\n",
    "\n",
    "Randam initalization to **small numbers**, e.g. $W^{[1]} = np.random.rand((2,2)) \\times 0.01$. Reason is if weights are large then derivatives tend to be saturated (e.g. tanh) resulting in slow training.\n",
    "\n",
    "For deep networks, a **different** multiplying constant maybe needed.\n",
    "\n",
    "Logistic regression doesn't have a hidden layer. So even if the weights are initalized to zero, the derivatives depend on the input value $X$, therefore it can still work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 DL Networks\n",
    "\n",
    "<a id='dimension_again'></a>\n",
    "## Dimensions Again\n",
    "\n",
    "$n^{[i]}$ - no. of activiations in layer $i$.\n",
    "\n",
    "Dimension of $W^{[i]}$ is $\\big(n^{[i]}, n^{[i-1]}\\big)$\n",
    "\n",
    "Dimension of $b^{[i]}$ is $\\big(n^{[i]}, 1\\big)$. In vectorized versions the dimension of this doesn't change due to broadcasting.\n",
    "\n",
    "Dimension of $z^{[i]}$ and $a^{[i]}$ is $\\big(n^{[i]}, 1 \\big)$\n",
    "\n",
    "Dimension of $Z^{[i]}$ and $A^{[i]}$ is $\\big(n^{[i]}, m \\big)$\n",
    "\n",
    "## Circuit Theory, why deep? \n",
    "\n",
    "There are functions that you can compute with a **small** L-layer deep network that shallower networks require **exponentially** more hidden units to compute. Example, XOR or $n$ inputs. A deep network needs $\\log n$ layers, whereas a single layer network would need $2^n$ neurons.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "* Learning rate\n",
    "* no. of epochs\n",
    "* no. of hidden layers\n",
    "* no. of units per layer\n",
    "* activation function, ReLU, tanh, etc.\n",
    "* momentum\n",
    "* minibatch size\n",
    "* regularization\n",
    "\n",
    "These parameters may change over time, due to hardware changes for example. Therefore once after a while, it's useful to re-evaluate and try a number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "\n",
    "## Train/DevTest\n",
    "\n",
    "In big data era, where millions of data examples are available, train/cv/test split ratios can be changed. For 1mm examples, maybe reserve 10k each for cv and test, the rest is used for training, i.e. 98%/1%/1% split.\n",
    "\n",
    "**Mismatched train/test distribution**\n",
    "\n",
    "E.g. high resolution images versus low resolution images.\n",
    "\n",
    "\n",
    "## Bias/Variance\n",
    "\n",
    "Should be viewed in context with Bayes (optimal) error. \n",
    "\n",
    "\n",
    "## Basic Recipe\n",
    "\n",
    "High bias (training set performance) -> Bigger network, train larger, NN architecture search, until bias is reduced to acceptable level.\n",
    "\n",
    "High variance (dev set performance) -> More data, regularisation, NN architecture search.\n",
    "\n",
    "\n",
    "**Bias/Variance tradeoff**: No longer apply in DL era. There are tools to drive down both without hurting one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularization'></a>\n",
    "## Regularization\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "$$ J\\big(W^{[1]}, b^{[1]}, \\cdots, W^{[L]}, b^{[L]}\\big) = \\frac{1}{m}\\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}, y) + \n",
    "\\frac{\\lambda}{2m}\\sum_{l=1}^{l=L} \\| W^{[l]} \\|^2$$\n",
    "\n",
    "$$ \\| W^{[l]} \\|^2_F = \\sum_{i=1}^{n^{[l-1]}}\\sum_{j=1}^{n^{[l]}}\\big( w_{ij}^{[l]}\\big)^2 $$\n",
    "\n",
    "$\\| W^{[l]} \\|^2$ is called the **Frobenius Norm**, $w : (n^{[l]}, n^{[l-1]})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backprop:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "dW^{[l]'} &= dW^{[l]} + \\frac{\\lambda}{m}W^{[l]} \\\\\n",
    "W^{[l]} &= W^{[l]} - \\alpha \\times dW^{[l]'} \\\\\n",
    "&= W^{[l]} - \\alpha \\big( dW^{[l]} + \\frac{\\lambda}{m}W^{[l]} \\big) \\\\\n",
    "&= W^{[l]} - \\frac{\\alpha \\lambda}{m}W^{[l]} - \\alpha \\times dW^{[l]} \\\\\n",
    "&= \\big(1-\\frac{\\alpha \\lambda}{m}\\big) W^{[l]} - \\alpha \\times dW^{[l]}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is also known as **weight decay**. Most used method by Andrew Ng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization constraints $W$'s value to be within a smaller range for activiation functions, therefore may not be reaching the non-linear areas, such as tanh.\n",
    "\n",
    "In debugging, always plot the full loss value including both terms, including the regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropout'></a>\n",
    "### Dropout\n",
    "\n",
    "**Inverted Dropout**\n",
    "\n",
    "At **training** time:\n",
    "\n",
    "Generate dropout matrix, with `keep_prob=0.8` for example.\n",
    "\n",
    "    d3 = np.random.randn(a3.shape[0], a3.shape[1]) < keep_prob\n",
    "    a3 = np.multiply(a3, d3)\n",
    "    a3 /= keep_prob #revert to back original value for the ones kept\n",
    "\n",
    "From this you see that for different examples, you use different dropout vectors. This goes back to the what the DL books said about forcing the network to focus on different part of the data. \n",
    "\n",
    "In each gradient descent iternation, use different dropout matrix.\n",
    "\n",
    "**For backprop, shut down the same neurons using the same dropout matrix, then scale back:**\n",
    "\n",
    "    da3 = np.multiply(da3, d3)\n",
    "    da3 = np.divide(da3, keep_prob)\n",
    "\n",
    "\n",
    "**At test time, no drop out.** \n",
    "\n",
    "Dropout **shrink the weights**, like $L^2$ regularization.\n",
    "\n",
    "Different layers can have different `keep_prob`, smaller layers may have larger `keep_prob` values. \n",
    "\n",
    "Dropout is commonly used in computer vision to combat overfitting, where you never have enough data so always tend to overfit. Don't always generalize to other areas. **Unless you have an overfitting problem, no need to bother with dropout.**\n",
    "\n",
    "With dropout, the loss curve will not be monotonically decreasing versus no. of iterations. **Trick is to turn off dropout first to make sure that the loss curve is monotonically decreasing, and then turn it back on for real training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "E.g. Flip, roatate, crop, or distort your data images. \n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Plot both test and dev set loss curve, stop when dev loss start to increase.\n",
    "\n",
    "### Orthogonalization\n",
    "\n",
    "Separate the tasks in ML. Optimization to reduce $J$, Regularzation are separate tasks, work on them independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='opt'></a>\n",
    "## Optimization \n",
    "\n",
    "### Normalization\n",
    "\n",
    "Use the $\\mu$ and $\\sigma$ from training set to normalize dev/test sets.\n",
    "\n",
    "Normalization makes the optimization problem easier to solve.\n",
    "\n",
    "### Vanishing / Exploding Gradients\n",
    "\n",
    "If weights are a lot larger than 1 or a lot smaller than one, having lots of layers means weights would take power of $L-1$ to propgate back, could either explode or vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='weight_init'></a>\n",
    "### Weight Initalization\n",
    "\n",
    "This helps with vanishing / expoloding gradients but wont' resolve the issue.\n",
    "\n",
    "**Single Neuron**\n",
    "\n",
    "$z = w^T \\times x$, when you have lots of weights, i.e. $w_n$ when $n$ is large, set: \n",
    "\n",
    "$$Variance(w_i) = \\frac{1}{n} $$\n",
    "\n",
    "Therefore: $W^{[l]} = np.random.randn(shape) \\times np.sqrt\\big(\\frac{1}{n^{[l-1]}}\\big)$\n",
    "\n",
    "For **ReLU**, $variance(w_i) = \\frac{2}{n}$ works better.\n",
    "\n",
    "**He initialization** (He et al., 2015), $variance = \\frac{2}{n^{[l-1]}}$.\n",
    "\n",
    "For $\\tanh$, use $Variance(w_i) = \\frac{1}{n}$. Known as Xavier initalization.\n",
    "\n",
    "Some people also use $\\frac{2}{n^{[l-1]} + n^{[l-]}}$.\n",
    "\n",
    "This variance measure can be a hyperparameter to tune, Andrew Ng would rank it as a lower priority though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grad_checking'></a>\n",
    "### Gradient Checking\n",
    "\n",
    "Given $f(x)$ and its derivative $g(x)$:\n",
    "\n",
    "$$ g(\\theta) = \\lim_{\\epsilon \\rightarrow 0} \\frac{f(\\theta+\\epsilon) - f(\\theta-\\epsilon)}{2\\epsilon} $$\n",
    "\n",
    "The order of the error is $\\mathcal{O}(\\epsilon^2)$. This is better than just using $(f(\\theta+\\epsilon) - f(\\theta))$ which has error in the order of $\\mathcal{O}(\\epsilon)$.\n",
    "\n",
    "#### Procedure:\n",
    "\n",
    "Take all parameters, $W^{[1]}, b^{[1]}, \\cdots, W^{[L]}, b^{[L]}$, reshape into a big vector $\\theta$.\n",
    "\n",
    "Take all $dW^{[1]}, db^{[1]}, \\cdots, dW^{[L]}, db^{[L]}$, reshape into a big vector $d\\theta$.\n",
    "\n",
    "For each $i$, compute:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "d\\theta_{approx}[i] &= \\frac{J(\\theta_1, \\theta_2, \\cdots, \\theta_i + \\epsilon, \\cdots) - \n",
    "J(\\theta_1, \\theta_2, \\cdots, \\theta_i - \\epsilon, \\cdots)}{2\\epsilon} \\\\\n",
    "&\\approx d\\theta[i] = \\frac{\\partial J}{\\partial \\theta_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Check: \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\| d\\theta_{approx} - d\\theta \\|_2}{\\| d\\theta_{approx} \\|_2 - \\| d\\theta \\|_2} &\\approx \n",
    "\\begin{cases}\n",
    "10^{-7} &\\text{great!} \\\\\n",
    "10^{-5} &\\text{worth checking, probably ok, check all components of the vector} \\\\\n",
    "10^{-3} &\\text{worry, must be a bug!}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note $\\| \\cdot \\|_2$ = np.linalg.norm()\n",
    "\n",
    "Create a vector to store all $d\\theta[i]$, iterate through $\\forall i \\in \\theta_i$, calculate $d\\theta_{approx}[i]$ and store in this vector. See course assignment for details.\n",
    "\n",
    "Then cacluate the check with `np.linalg.norm()` etc, use threshold levels above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Notes\n",
    "\n",
    "Don't use in training, only to debug.\n",
    "\n",
    "If alo fails grad check, look for the componements in $\\theta$ to identify bug. Is it for $W$ or $b$?\n",
    "\n",
    "Remember regularizatoin. \n",
    "\n",
    "Doesn't work with dropout. Turn off dropout to check first, then turn it back on.\n",
    "\n",
    "Run at random initalization, perhaps later again after some training. Possibly that an implementation only works when weights are close to 0, but not when they move away from zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 2 \n",
    "\n",
    "<a id='minibatch'></a>\n",
    "## Minibatch Gradient Descent\n",
    "\n",
    "**Notation**: $X^{\\{i\\}}$ to denote minibatch $i$\n",
    "\n",
    "    for t in range(5000):\n",
    "        y_hat, cache = forward_prop(X^{t}, W, b)\n",
    "        compute_cost(y^{t}, y_hat)\n",
    "        back_prop(cache)\n",
    "        update_parameters()\n",
    "\n",
    "In batch gradient descent, you'd expect the cost function to decrease monotonically. \n",
    "\n",
    "In minibatch gradient descent, costs may go up or down but should trend down over iterations.\n",
    "\n",
    "If minibatch size == $m$ : Batch gradient descent, usually takes large, non-noisy steps towards the minimum. Too long per iteration.\n",
    "\n",
    "If minibatch size == 1 : Stochastic Gradient Descent (SGD), won't converge but circle around the minimum. Loses speed up for vectorization. \n",
    "\n",
    "**Small training sets**, $m \\le 2000$ -> batch gradient descent\n",
    "\n",
    "Typical minibatch size: 64, 128, 256, 512, 1024... Make sure data fits into GPU/CPU memory.\n",
    "\n",
    "When partitioning the dataset into minibatches, the last batch may be smaller than the batch size, this last batch is also used for training (see assignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='opt_algo'></a>\n",
    "## EWMA \n",
    "\n",
    "$$ V_t = \\beta V_{t-1} + (1-\\beta) \\theta_t \\approx \\text{averaging over } \\frac{1}{1-\\beta} \\text{ days} $$\n",
    "\n",
    "Rule of thumb, $(1 - \\epsilon)^{1/\\epsilon} = \\frac{1}{e}$, **larger** $\\beta$ means averaging over a **longer** window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Correction\n",
    "\n",
    "Deal with the first few EWMA values, which starts at lower level. To correct this: $V'_t = \\frac{V_t}{1-\\beta^t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Momentum\n",
    "\n",
    "On epoch iteration $t$:\n",
    "    \n",
    "    compute dW, db on current minibatch\n",
    "    VdW = beta * VdW + (1-beta) * dW\n",
    "    Vdb = beta * Vdb + (1-beta) * db\n",
    "    \n",
    "    W = W - learning_rate * VdW\n",
    "    b = b - learning_rate * Vdb\n",
    "    \n",
    "The idea is to use EWMA to smooth out the gradients across minibatches, i.e. smooth out the randomness in update directions. Typically use $\\beta = 0.9$, average of around 10 iterations. Possible values range from 0.8 to 0.999.  Normally people don't use bias correction for `VdW`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Momentum \n",
    "\n",
    "Based on cs231 Lecture 6.\n",
    "\n",
    "The idea is that if we know that the next gradient update is momentum + gradient, instead of evaluating gradient at current spot, we can evaluate gradient at current + momentum (look ahead).\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_t &= \\mu v_{t-1} - \\epsilon \\triangledown f(\\theta_{t-1} + \\mu v_{t-1}) \\\\\n",
    "\\theta_t &= \\theta_{t-1} + v_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From normal backprop, we have $\\theta_{t-1}, \\triangledown f(\\theta_{t-1})$. Let $\\phi_{t-1} = \\theta_{t-1} + \\mu v_{t-1}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_t &= \\mu v_{t-1} - \\epsilon \\triangledown f(\\phi_{t-1}) \\\\\n",
    "\\phi_t &= \\theta_t + \\mu v_t \\\\\n",
    "&= \\theta_{t-1} + v_t + \\mu v_t \\\\\n",
    "&= \\phi_{t-1} - \\mu v_{t-1} + (1 + \\mu) v_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "```\n",
    "v_prev = v\n",
    "v = mu * v - learning_rate * dx\n",
    "x += -mu * v_prev + (1 + mu) * v\n",
    "```\n",
    "\n",
    "**Almost always** better than simple momentum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop - Hinton proposed this in a Coursera course!\n",
    "\n",
    "On epoch iteration $t$:\n",
    "\n",
    "    compute dW, db on current minibatch\n",
    "    SdW = beta * SdW + (1-beta) dW**2\n",
    "    Sdb = beta * Sdb + (1-beta) db**2\n",
    "    \n",
    "    \n",
    "    W = W - learning_rate * dW / np.sqrt(SdW + 1e-8)\n",
    "    b = b - learning_rate * db / np.sqrt(Sdb + 1e-8)\n",
    "    \n",
    "The intuition is when the updates in some parameter directions are large versus others, you may want to slow down learning in those directions, e.g. you want to slow down learning in `b` but have fast learning in `W`. Dividing by the root squares achieves this.\n",
    "\n",
    "RMSprop overcomes the weakness in AdaGrad, which results in gradient updates to vanish as momentum accumulates. [Stanford cs231](https://www.youtube.com/watch?v=hd_KFJ5ktUc&t=525s&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&index=6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam (Adaptive Moment Estimation)\n",
    "\n",
    "[paper](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "\n",
    "    VdW, SdW, Vdb, Sdb = 0, 0, 0, 0\n",
    "    On epoch iteration t:\n",
    "        compute dW, db on current minibatch\n",
    "        \n",
    "        VdW = beta1 * VdW + (1-beta1) * dW\n",
    "        Vdb = beta1 * Vdb + (1-beta1) * db\n",
    "        \n",
    "        SdW = beta2 * SdW + (1-beta2) dW**2\n",
    "        Sdb = beta2 * Sdb + (1-beta2) db**2\n",
    "        \n",
    "        # bias correction\n",
    "        VdW_corrected = VdW / (1 - np.power(beta1, t))\n",
    "        Vdb_corrected = Vdb / (1 - np.power(beta1, t))\n",
    "        \n",
    "        SdW_corrected = SdW / (1 - np.power(beta2, t))\n",
    "        Sdb_corrected = Sdb / (1 - np.power(beta2, t))\n",
    "        \n",
    "        W = W - learning_rate * VdW_corrected / np.sqrt(SdW_corrected + 1e-8)\n",
    "        b = b - learning_rate * Vdb_corrected / np.sqrt(Sdb_corrected + 1e-8)\n",
    "        \n",
    "Hyperparameter choices: \n",
    "\n",
    "* `learning_reate` : tuned\n",
    "* `beta1` : 0.9\n",
    "* `beta2` : 0.999 (proposed by original paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Melis, et al. 2017](https://arxiv.org/abs/1707.05589) used Adam with $\\beta_1 = 0$ and $\\beta_2 = 0.999$ and $\\epsilon = 10^{-9}$. This turns of the exponential moving average for the estimates of the means of the gradientds and brings Adam very close to RMSProp without momentum, but due to Adam's bias correction, **larger** learning rates can be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay\n",
    "\n",
    "1 epoch = 1 pass through all data $m$, $\\alpha$ is the learning rate.\n",
    "\n",
    "$$ \\alpha = \\frac{1}{1 + \\text{decay_rate} \\times \\text{epoch_num}}\\alpha_0 $$\n",
    "\n",
    "Try $\\alpha_0 = .2$, $\\text{decay_rate} = 1$.\n",
    "\n",
    "Some variations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha &= 0.95^{\\text{epoch_num}} \\alpha_0 \\\\\n",
    "\\alpha &= \\frac{k}{\\sqrt{\\text{epoch_num}}} \\alpha_0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Or discrete staircase, manual decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Optima\n",
    "\n",
    "In deep learning where you have zero gradient, more likely you have a saddle point, rather than a local optima.\n",
    "\n",
    "**Plateaus** slows down learning significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "Secondary importance: momentum, minibatch size, no. of hidden units. \n",
    "\n",
    "Grid search is out dated now. Ok when # of parameters is small.\n",
    "\n",
    "Better to use **random search**,as suggested by the DL book. \n",
    "\n",
    "Use **Coarse to fine** scheme, like what's suggested by Goodfellow. \n",
    "\n",
    "<a id='random_search'></a>\n",
    "## Appropriate Scale for Random Search\n",
    "\n",
    "**Uniform sampling** may be reasonable for # of layers, # of hidden units in a layer.\n",
    "\n",
    "Use **log** scale for **learning rate**. \n",
    "\n",
    "    # r is uniform\n",
    "    r = -4 * np.random.rand()\n",
    "    alpha = np.power(10, r)\n",
    "    \n",
    "**Momemtum $\\beta$**: \n",
    "    \n",
    "    r = np. -3 * np.random.rand()\n",
    "    beta = 1 - np.power(10, r)\n",
    "    \n",
    "Theoratical reason is that when $\\beta$ is close to 1, it has a huge impact, e.g. $\\beta$ from 0.999 to 0.9995 is moving from averaging 1000 samples to 2000 samples. \n",
    "\n",
    "## Babysitting One Model\n",
    "\n",
    "Watch cost function over a number of days and adjust each day when you don't have a lot of resources. (Pandas approach)\n",
    "\n",
    "Alternatively, train many models in parallel. (Caviar approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='batchnorm'></a>\n",
    "## Batch Normalization\n",
    "\n",
    "[paper](https://arxiv.org/abs/1502.03167) \n",
    "\n",
    "[R2RT Tensorflow Example](https://r2rt.com/implementing-batch-normalization-in-tensorflow.html)\n",
    "\n",
    "In practice, people tend to normalize $Z^{[l]}$ more than $A^{[l]}$. This is the approached used in this course.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Given intermediate values in a network, $Z^{(1)}, Z^{(2)}, \\cdots, Z^{(m)}$, from some hidden layer $l$, data $i$, i.e. $Z^{[l](i)}$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu &= \\frac{1}{m} \\sum_{i=1}^{m} Z^{[l](i)} \\\\\n",
    "\\sigma^2 &= \\frac{1}{m} \\sum_{i=1}^{m} \\big( Z^{[l](i)} - \\mu \\big)^2 \\\\\n",
    "Z^{[l](i)}_{norm} &= \\frac{Z^{[l](i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\\n",
    "\\widetilde{Z}^{[l](i)} &= \\gamma \\times Z^{[l](i)}_{norm} + \\beta \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Use $\\widetilde{Z}^{[l](i)}$ instead of $Z^{[l](i)}$. $\\gamma$ and $\\beta$ are training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a deep network, batch norm is done between computing $Z^{[l]}$ and $A^{[l]}$, over a minibatch.\n",
    "\n",
    "Parameters in the network becomes: $W^{[l]}, b^{[l]}, \\gamma^{[l]}, \\beta^{[l]}$.\n",
    "\n",
    "$b^{[l]}$ can be eliminated due to the BN calculation, therefore can be set to 0 permanently, i.e. $z^{[l]} = w^{[l]}a^{[l-1]}$.\n",
    "\n",
    "**Dimensions** for one training example: $Z^{[l]}, W^{[l]}, b^{[l]}, \\gamma^{[l]}, \\beta^{[l]} \\rightarrow (n^{[l]}, 1)$, $n^{[l]}$ is the number of hidden units in layer $l$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "    for t in range(num_minibatch):\n",
    "    \n",
    "        Compute forward prop on X_minibatch_t\n",
    "            In each hidden layer, use BN to replace Z^{[l]} with \\widetilde{Z}^{[l]}\n",
    "        \n",
    "        Use backprop to compute dW^{[l]}, dbeta^{[l]}, dgamma^{[l]}\n",
    "        \n",
    "        Update parameters\n",
    "        \n",
    "Works with momentum, RMSprop, Adam, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Why BatchNorm works\n",
    "\n",
    "Makes weights in later layers more robust to changes in earlier layers. \n",
    "\n",
    "**Covariate Shift**: data distribution between training and test changes. BN reduces this effect.\n",
    "\n",
    "### Regularization Effect\n",
    "\n",
    "Mean/variance computed on just one minibatch, therefore there is **noise** in the values of $Z$. This has a slight regularization effect.\n",
    "\n",
    "**Andrew Ng: the regularization effect is light, so don't rely on it. Use dropout or other methods.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BN At Test Time\n",
    "\n",
    "$\\mu, \\sigma^2$ at test time is based on an estimate using EWMA across minibatches.\n",
    "\n",
    "Why though? Doesn't that favour the later batches? Posted question in Discussion Forum [here](https://www.coursera.org/learn/deep-neural-network/discussions/weeks/3/threads/fGY3XZWnEeeImxLZqEy9Mg)\n",
    "\n",
    "Original paper uses running average collected during training, using the full population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "Used for multi-class classification problems. \n",
    "\n",
    "### Activation Function\n",
    "\n",
    "Given an $N$ class problem, dimension in brackets:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t &= e^{Z^{[l]}}, (N, 1) \\\\\n",
    "a^{[l]} &= \\frac{t_i}{\\sum_{j=1}^{N} t_j}, (N, 1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}, y) = -\\sum_{j=1}^{N} y_j \\log \\hat{y}_j $$\n",
    "\n",
    "Equavilent to Maximum Likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TensorFlow\n",
    "\n",
    "`tf.placeholder()` used for data input. \n",
    "\n",
    "Define `tf.Variable()`, cost function is specified in terms of variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
