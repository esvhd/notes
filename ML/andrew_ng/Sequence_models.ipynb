{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning.ai - Sequence Models\n",
    "\n",
    "## Notation\n",
    "\n",
    "For a given sequence, use $x^{<t>}$ to index into position $t$ in the sequence.\n",
    "\n",
    "$T_x$: **length** of the input sequence $x$. \n",
    "\n",
    "$x^{(i)<t>}$: training example $i$, position $t$.\n",
    "\n",
    "$T^{(i)}_x$: **length** of the $i$th training example equence $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Representations\n",
    "\n",
    "Build a **Volcabulary** of all possible words.\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "For sequence $x$, $x^{<t>}$ is an one-hot vector, with the vector length of the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "FC network doesn't work well, problems:\n",
    "\n",
    "* inputs, outputs can be of different lengths.\n",
    "* doesn't share features learned across different positions of text\n",
    "* can have lots of params, e.g. if vocab is 50k or 100k.\n",
    "\n",
    "Typically for RNNs, the initial state $a^{<0>}$ is initialized with zeros. Parameters are shared for all time steps in the same example.\n",
    "\n",
    "\n",
    "$$ a^{<t>} = g_t \\bigg( W_{aa} a^{<t-1>} + W_{ax} x^{<t>} + b_a \\bigg) $$\n",
    "\n",
    "**Notation**: $W_{ax}$ means weight $W$ for computing $a$ by mutiplying $x$.\n",
    "\n",
    "Alternatively, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^{<t>} &= g_t \\bigg( W_{a} [a^{<t-1>}, x^{<t>}] + b_a \\bigg) \\\\\n",
    "\\hat{y}^{<t>} &= g \\bigg( W_{ya} a^{<t>} + b_y \\bigg)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "```\n",
    "# concat horizontally\n",
    "W_a = np.concatenate([W_aa, W_ax], axis=0) \n",
    "\n",
    "c, d = W_aa.shape\n",
    "c, e = W_ax.shape\n",
    "\n",
    "assert(W_a.shape == (c, d + e))\n",
    "\n",
    "# for a and x, concat vertically\n",
    "ax = np.concatenate([a, x], axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Backward pass\n",
    "\n",
    "See course assignment 1 for maths equations for the backpass. Here I write down the code for an RNN cell backpass which I find more useful in understanding the shapes of matrices. \n",
    "\n",
    "For detailed backpass code for a full RNN, see assignment. Basically, gradients is **accumulated** from all timesteps in reverse order. Another thing to watch out for is that in a multi-step RNN, upstream gradients can come from 2 places (`da_next` and `da_prev`), therefore the upstream gradient is the **sum of these two sources**.\n",
    "\n",
    "```\n",
    "# Wax.shape == (n_a, n_x), xt.shape == (n_x, m)\n",
    "# Waa.shape == (n_a, n_a), a_prev.shape == (n_a, m), b.shape = (n_a, 1)\n",
    "\n",
    "# foward\n",
    "z = np.dot(Wax, xt) + np.dot(Waa, a_prev) + b  # z.shape == (n_a, m)\n",
    "a_next = np.tanh(z)\n",
    "\n",
    "# backward\n",
    "dz = (1 - a_next**2) * da_next  # da_next is gradient from upstream\n",
    "dWax = np.dot(dz, xt.T)  # dWax.shape == (n_a, n_x)\n",
    "dWaa = np.dot(dz, a_prev.T) # dWaa.shape == (n_a, m) * (m, n_a) == (n_a, n_a)\n",
    "db = np.sum(dz, axis=1, keepdims=True) # db.shape == (n_a, 1)\n",
    "dxt = np.dot(Wax.T, dz) # dxt.shape == (n_x, n_a) * (n_a, m) == (n_x, m)\n",
    "da_prev = np.dot(Waa.T, dz) # da_prev.shape == (n_a, m)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "\n",
    "Example, binary crossentropy loss, the loss is the sum of individual losses.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}^{<t>}(\\hat{y}^{<t>}, y^{<t>}) &= -y^{<t>} \\log \\hat{y}^{<t>} - (1 - y^{<t>})\\log (1-\\hat{y}^{<t>}) \\\\\n",
    "&= \\sum^{T_y}_{t=1} \\mathcal{L}^{<t>}(\\hat{y}^{<t>}, y^{<t>})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of RNN Architectures\n",
    "\n",
    "Inspired by Andrea Karpathy's blog post The Unresonable Effectiveness of RNN.\n",
    "\n",
    "Many-to-Many, e.g. `T_x == T_y`, many-to-one, one-to-many (music generation).\n",
    "\n",
    "Many-to-Many but `T_x != T_y`, e.g. machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model \n",
    "\n",
    "A language model returns/estimates `Prob(sentence)`.\n",
    "\n",
    "Training over large corpus of english text. `<EOS>` token appened to every sentence. `<UNK>` represents unknown words, ie. not in vocab.\n",
    "\n",
    "RNN to predict next word in sentence, loss function: softmax loss, $\\mathcal{L}(\\hat{y}^{<t>}, y^{<t>}) = -\\sum_i y^{<t>}_i \\log\\hat{y}^{<t>}_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from a Trained RNN\n",
    "\n",
    "Use `np.random.coice()` with the probability output from $\\hat{y}^{<t>}$ to sample from possible outputs (e.g. vocab) and feed into the next time step.\n",
    "\n",
    "Character level RNN is more computationally expensive, but it can model rare words not in the vocabulary easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients\n",
    "\n",
    "Simple RNNs do not handle long term dependencies very well. Vanishing gradient more common for RNNs. Exploding gradients can be handled by clipping gradients.\n",
    "\n",
    "## GRU - Gated Recurrent Unit\n",
    "\n",
    "**Notation**: memory cell $C$, $C^{<t>} = a^{<t>}$. \n",
    "\n",
    "### Simplified GRU\n",
    "\n",
    "$u$ subscript below indicates **update** gate.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{C}^{<t>} &= \\tanh \\big( W_c [ C^{<t-1>}, x^{<t>}] + b_c \\big) \\\\\n",
    "\\Gamma_u &= \\sigma \\big( W_u [C^{<t-1>}, x^{<t>}] + b_u \\big) \\\\\n",
    "C^{<t>} &= \\Gamma_u \\odot \\tilde{C}^{<t>} + (1 - \\Gamma_u) \\odot C^{<t-1>} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$C^{<t>}$ can be high dimensional vectors, in which case $\\odot$ above is the **Hadamard** operator for element-wise multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full GRU\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{C}^{<t>} &= \\tanh \\big( W_c [ \\Gamma_r \\odot C^{<t-1>}, x^{<t>}] + b_c \\big) \\\\\n",
    "\\Gamma_u &= \\sigma \\big( W_u [C^{<t-1>}, x^{<t>}] + b_u \\big) \\\\\n",
    "\\Gamma_r &= \\sigma \\big( W_r [C^{<t-1>}, x^{<t>}] + b_r \\big) \\\\\n",
    "C^{<t>} &= \\Gamma_u \\odot \\tilde{C}^{<t>} + (1 - \\Gamma_u) \\odot C^{<t-1>} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\Gamma_r$ is based on lots of research on the different variations of GRUs, it's shown to handle long term dependency better with $\\Gamma_r$. \n",
    "\n",
    "Based on Andrew's experience, GRUs is computationally easier, therefore easier to build a bigger network with GRUs than LSTMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "LSTM is more powerful than GRU but computationally more expensive to use.\n",
    "\n",
    "Here $C^{<t>} != a^{<t>}$. LSTM has a **forget gate** denoted by subscript $f$, and **output gate** denoted by $o$.\n",
    "\n",
    "Comparing to `cs231n`, which uses `ifog` for all the gates:\n",
    "\n",
    "* `i`-gate is $\\Gamma_u$ here, \n",
    "* `g`-gate is $\\tilde{C}^{<t>}$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{C}^{<t>} &= \\tanh \\big( W_c [ a^{<t-1>}, x^{<t>}] + b_c \\big) \\\\\n",
    "\\Gamma_u &= \\sigma \\big( W_u [a^{<t-1>}, x^{<t>}] + b_u \\big) \\\\\n",
    "\\Gamma_f &= \\sigma \\big( W_f [a^{<t-1>}, x^{<t>}] + b_f \\big) \\\\\n",
    "\\Gamma_o &= \\sigma \\big( W_o [a^{<t-1>}, x^{<t>}] + b_o \\big) \\\\\n",
    "C^{<t>} &= \\Gamma_u \\odot \\tilde{C}^{<t>} + \\Gamma_f \\odot C^{<t-1>} \\\\\n",
    "a^{<t>} &= \\Gamma_o \\odot \\tanh(C^{<t>})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Peephole Connection\n",
    "Insert $C^{<t-1>}$ in **all** gate updates. E.g. for forget gate:\n",
    "\n",
    "$$ \\Gamma_o = \\sigma \\big( W_o [C^{<t-1>}, a^{<t-1>}, x^{<t>}] + b_o \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Backward Pass\n",
    "\n",
    "Again, code the LSTM cell here, for full LSTM RNN, see assignment 1.\n",
    "\n",
    "```\n",
    "# ot, ft, it are gamma_o, gamma_f, gamma_u, shapes are all (n_a, m)\n",
    "# cct is c_tilde\n",
    "# da_next is upstream gradient\n",
    "# a_prev.shape == (n_a, m) == c_prev.shape\n",
    "# xt.shape == (n_x, m)\n",
    "\n",
    "tc = np.tanh(c_next)\n",
    "z = 1 - tc**2\n",
    "\n",
    "dot = da_next * a_next * (1 - ot)  # dot.shape == (n_a, m)\n",
    "dcct = dc_next * it + ot * z * it * da_next * cct * (1 - np.tanh(cct)**2)\n",
    "dit = dc_next * cct + ot * z * cct * da_next * it * (1 - it)\n",
    "dft = dc_next * c_prev + ot * z * c_prev * da_next * ft * (1 - ft)\n",
    "\n",
    "dit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN\n",
    "\n",
    "Useful for language translation where order of timesteps is less useful or inadquate to solve the problem at hand.\n",
    "\n",
    "BRNN is a **Acyclic graph**, it trains two RNNs and combines their outputs together. Each timestep output prediction is therefore: \n",
    "\n",
    "$$ \\hat{y}^{<t>} = g \\big( W_y [\\overrightarrow{a}^{<t>}, \\overleftarrow{a}^{<t>}] + b_y \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pics/brnn.png' width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN\n",
    "\n",
    "Formula for computing the cells:\n",
    "\n",
    "$$ a^{[2]<3>} = g \\big( W^{[2]}_a [a^{[2]<2>}, a^{[1]<3>}] + b^{[3]}_a \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pics/deep_rnn.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "One hot vectors are binary, sparse, high dimensional. Word embeddings are low-dimensional floating-point vectors.\n",
    "\n",
    "### Properties\n",
    "\n",
    "**Analogies**\n",
    "\n",
    "[2013 Mikolov et. al. Linguistic regularities in continuous space word representations]\n",
    "\n",
    "Question: Man -> Woman as King -> ?\n",
    "\n",
    "Find vectors so that $e_{man} - e_{woman}$ is close to $e_{king} - e_{queen}$. Alternatively, find a word $w$, such that:\n",
    "\n",
    "$$\\underset{x}{argmax} \\text{ Similarity}(e_w, e_{king} - e_{man} + e_{woman}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly used similarity function: cosine, squared difference, etc.\n",
    "\n",
    "**Cosine Similarity** (angle between vector $u$ and $v$):\n",
    "\n",
    "$$sim(u, v) = \\frac{u^T v}{\\|u\\|_2 \\|v\\|_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix\n",
    "\n",
    "Vocab size 10k, embedding dimension 300, results in embedding matrix $E$ shape is (300, 1000).\n",
    "\n",
    "Given a one-hot $o_j$ vector,  $E \\cdot o_j = e_j$, gives the embedding vector for $j$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "#### Skip Gram Model:\n",
    "\n",
    "**Skip gram**: context around a word you are trying to predict, e.g. given 4 words before and after, predict the word in the middle.\n",
    "\n",
    "From embedding matrix to embedding vector: $\\hat{y} = \\text{Softmax}(E \\cdot o_j)$, target levels $y$ are also one-hot vectors. \n",
    "\n",
    "Assume **vocab size is 10k**. $\\theta$ is parameters in the softmax layer. $\\theta$ and $e_c$ have vectors of the same dimension. \n",
    "\n",
    "$$\\hat{y} = p(t \\mid c) = \\frac{e^{\\theta_t^T e_c}}{\\sum_{j=1}^{10000} e^{\\theta_j^T e_c}}$$\n",
    "\n",
    "Loss function $\\mathcal{L}(\\hat{y}, y) = -\\sum_{i=1}^{\\text{vocab_size}} y_i \\log \\hat{y}_i$\n",
    "\n",
    "Problems: slow. When vocab is large, computing $\\hat{y}$ is slow. Solution: use hierarchical softmax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling\n",
    "\n",
    "Mikolov et, al. 2013\n",
    "\n",
    "Context: $c$, word: $t$, target: $y$.\n",
    "\n",
    "From a data point with $y=1$, generate $K$ negative examples with $y = 0$. \n",
    "\n",
    "$P(y = 1 \\mid c, t) = \\sigma( \\theta_t^T e_c)$\n",
    "\n",
    "Turning 10k-way softmax problem into 10k binary classification problem. \n",
    "\n",
    "How to choose the negative examples?\n",
    "\n",
    "Sample with empirical frequency of words $f(w_i)$ in corpus. Not so great. \n",
    "\n",
    "Suggestion: $P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{j=1}^{10000}f(w_j)^{3/4}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Word Vectors\n",
    "\n",
    "$X_{ij} =$ number of times $i$ appears in the context of $j$. \n",
    "\n",
    "$$\\text{Minimize} \\sum_{i=1}^{10000}\\sum_{j=1}^{10000} f(X_{ij}) \\big(\\theta_i^T e_j + b_i + b_j - \\log X_ij\\big)^2$$\n",
    "\n",
    "where $f(X_{ij}) = 0$ when $X_{ij} = 0$.\n",
    "\n",
    "$\\theta_i$ and $e_j$ are symmetric. For a n-dimensional GloVe embedding, each vector is of shape (n,)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification\n",
    "\n",
    "Method 1: \n",
    "\n",
    "Average all word embedding vectors, apply softmax classification. Pros: can handle variable lengh inputs. Cons: ignores word order.\n",
    "\n",
    "### RNN\n",
    "\n",
    "Feed embedding vectors to a many-to-one RNN. \n",
    "\n",
    "## Debiasing Word Embeddings\n",
    "\n",
    "Bolukbasi et. al., 2016\n",
    "\n",
    "1. Identify bias direction\n",
    "2. Neutralize: for every word that is not definitional, project to get rid of bias.\n",
    "3. Equalizae pairs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/debiasing_embedding.png', width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Architectures\n",
    "\n",
    "Sutskever et al., 2014. Cho et al., 2014. \n",
    "\n",
    "**Seq2Seq: encoding network + decoding network**. \n",
    "\n",
    "**Image Captioning**: CNN to extract image features, which is then fed to an RNN to learn the captions.\n",
    "\n",
    "Machine translation as building a **conditional language model**. $P(y^{<1>}, \\cdots, y^{<T_y>} \\mid x)$, where $y^{<1>}, \\cdots, y^{<T_y>}$ are the English targets, and $x$ are French inputs. \n",
    "\n",
    "Objective:\n",
    "\n",
    "$$\\underset{y^{<1>}, \\cdots, y^{<T_y>}}{argmax} P(y^{<1>}, \\cdots, y^{<T_y>} \\mid x) $$\n",
    "\n",
    "Greedy search doesn't work here, i.e. cannot try to maximize $P(y^{<t>})$ greedily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "**Beam Width**, $B = 3$, keeps tract of the first $B$ likely targets / words for each word. At each step, only $B$ combinations are stored. \n",
    "\n",
    "Compute:\n",
    "\n",
    "$$ p(y^{<1>}, y^{<2>} \\mid x) = p(y^{<1>} \\mid x) p(y^{<2>} \\mid x, y^{<1>}) $$\n",
    "\n",
    "$p(y^{<1>} \\mid x)$ is stored based on the beam width setting above. $y^{<1>}$ is hard-wired as inputs to the network for computing $y^{<2>}$. \n",
    "\n",
    "$B = 1$ equates greedy search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length Normalization \n",
    "\n",
    "Objective function: \n",
    "\n",
    "$$ \\underset{y}{argmax} \\sum_{t=1}^{T_y} \\log P(y^{<t>} \\mid x, y^{<1>}, \\cdots, y^{<T_y>}) $$\n",
    "\n",
    "This objective results in preference for **shorter** sentences, (less less than 1 probabilities). Trick is to use the below:\n",
    "\n",
    "$$ \\underset{y}{argmax} \\frac{1}{T_y^{\\alpha}} \\sum_{t=1}^{T_y} \\log P(y^{<t>} \\mid x, y^{<1>}, \\cdots, y^{<T_y>}) $$\n",
    "\n",
    "Where $\\alpha \\in [0, 1]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Width\n",
    "\n",
    "**Large beam width**, $B$: better results, more memory usage, slower.\n",
    "\n",
    "**Small** $B$: worse results, faster. \n",
    "\n",
    "Production systems usually see $B = 10$. In research you'd see large B such as 3000.\n",
    "\n",
    "Unlike exact search algos such as breath first search and depth first search, Beam Search runs faster but is **not** guaranteed to find exact maximum for this objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis for Beam Search\n",
    "\n",
    "Pick an incorrectly predicted example, $y^*$ is **ground truth**, $\\hat{y}$ is **predicted** result. \n",
    "\n",
    "Feed input through RNN to compute $P(y^* \\mid x)$ and $P(\\hat{y} \\mid x)$. \n",
    "\n",
    "If $P(y^* \\mid x) > P(\\hat{y} \\mid x)$: Beam search is at fault, it did not find the desired result.\n",
    "\n",
    "If $P(y^* \\mid x) \\leq P(\\hat{y} \\mid x)$: RNN model is at fault.\n",
    "\n",
    "Perform this analysis for incorrectly predicted examples, then compute the fraction of errors due to beam search vs RNN model. **Only if a large number is due to beam search, would you consider increasin beam width.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/beam_error_analysis.png', width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu Score\n",
    "\n",
    "A single row number evaluation metric. \n",
    "\n",
    "Measures how good a machine generated translation is. **BLEU = Bi-Lingual Evaluation Understudy**.\n",
    "\n",
    "Papineni et. al., 2002, Bleu: A method for automatic evaluation of machine translation.\n",
    "\n",
    "**Modified Precision**: measures the frequency of a ground truth word's occurance in the predicted result, ie. count / nunique($\\hat{y}$). \n",
    "\n",
    "**Bigrams** does the above with two words, count / total number of bigrams.\n",
    "\n",
    "For **n-grams**:\n",
    "\n",
    "$$ P_n = \\frac{\\sum_{n-grams \\in \\hat{y}} Count_{clip} (n-gram)}{\\sum_{n-gram \\in \\hat{y}} Count(n-gram)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $n \\in \\{1, 2, 3, 4\\}$, the combine Bleu score:\n",
    "\n",
    "$$ BP \\times \\exp \\bigg(\\frac{1}{4} \\sum_{n=1}^{4} P_n \\bigg)$$\n",
    "\n",
    "$BP$ is a brevity penlty factor, as shorter sentences are more likely to have higher precision. \n",
    "\n",
    "```\n",
    "# MT = machine translation, y_hat\n",
    "if MT_output_length > reference_output_length:\n",
    "    BP = 1\n",
    "else:\n",
    "    BP = np.exp(1 - MT_output_length / reference_output_length)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Model\n",
    "\n",
    "Bahdanau et. al. 2014. Neural machine trnslation by jointly learning to align and translate.\n",
    "\n",
    "Problem with Long sequences: **Bleu score declines with the length of the sentence for machine translation systems**.\n",
    "\n",
    "The normal translation is done by a Bidirectionarl RNN. \n",
    "\n",
    "Attention model computes attention weights for words in the input with a uni-directional RNN. \n",
    "\n",
    "**Context** $C$ defined as weighted sum of features from the Bidirectional RNN. \n",
    "\n",
    "$t$ is the timestep in the **attention** RNN, $t'$ is the tiemstep in the usual **translation** RNN.\n",
    "\n",
    "$\\alpha^{<t, t'>}$ is amount of attention $y^{<t>}$ should pay to $a^{<t'>}$. \n",
    "\n",
    "$f()$ is a small single layer network. \n",
    "\n",
    "This algo runs in **quadratic time**... \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\forall \\alpha^{<i, t'>} &\\geq 0 \\\\\n",
    "\\sum_{t'} \\alpha^{<i, t'>} &= 1, \\forall i \\in T_s\\\\\n",
    "C^{<i>} &= \\sum_{t'} \\alpha^{<i, t'>} a^{<t'>}  \\\\\n",
    "\\alpha^{<t, t'>} &= \\frac{\\exp\\big( e^{<t, t'>} \\big)}{\\sum_{t'=1}^{T_x} \\exp\\big( e^{<t, t'>}\\big)} \\\\\n",
    "e^{<t, t'>} &= f(s^{<t-1>}, a^{<t'>}) \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/attention.png' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition\n",
    "\n",
    "[Spectrogram](https://en.wikipedia.org/wiki/Spectrogram): y-axis is time, x-axis is frequency. \n",
    "\n",
    "\n",
    "### CTC Cost for speech recognition \n",
    "\n",
    "Alex Graves et al. \n",
    "\n",
    "CTC = Connectionist temporal classification\n",
    "\n",
    "Basic rule: collapse repeated characters not separated by \"black\", e.g. ttt_h_eee_______blank_____qqq\n",
    "\n",
    "## Trigger Word Detection\n",
    "\n",
    "Label as 1 when the trigger word is said. **Problem**: training data is imbalanced. **Hack**: repeat 1 label multiple times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/trigger_word.png' width='800'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
