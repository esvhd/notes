{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Strategy\n",
    "\n",
    "Based on Deeplearning.ai's Coursera course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "\n",
    "## Orthogonalization\n",
    "\n",
    "The **chain of assumptions** in ML is:\n",
    "\n",
    "    1. fit training set well on cost function\n",
    "    2. fit dev set well\n",
    "    3. fit test set well\n",
    "    4. perform well in real world\n",
    "\n",
    "We need **different/orthogonal** tuning techniques at the different stages above.\n",
    "\n",
    "Andrew Ng: typically **don't** using early stopping, because it affects both 1 and 2 above.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Define a **single** number evaluation metric.\n",
    "\n",
    "F1 score is the **harmonic mean** of precision and recal.\n",
    "\n",
    "When you have N metrics, choose one as the **optimization objective**, others become **constraints**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Dev/Test Distributions\n",
    "\n",
    "They must be coming from the **same distribution**! Otherwise you'd be moving the target post, i.e. training to hit one, but test with another.\n",
    "\n",
    "## When to change the metrics\n",
    "\n",
    "When the metric is not ranking the algos in the desirable order, i.e. when some other factors are not considered.\n",
    "\n",
    "## Human Level Performance\n",
    "\n",
    "**Avoidable bias** - difference between human performance and model performance. When this is large, focus on reducing bias. \n",
    "\n",
    "Human level performance sometimes is hard to define. 1 person, professional, or a team of professtionals?\n",
    "\n",
    "**Bayes optimal error rate** is the lowest theratically error rate possible. Human level performance can often be close to but not as good as the Bayes error rate. \n",
    "\n",
    "When **training error rate** is far from human level performance, but difference between the training and dev error rates are not as large, the focus should be to **reduce bias**, such as:\n",
    "\n",
    "* using a larger model, \n",
    "* use a better optimization algorithm, or train longer\n",
    "* use a different NN architecture/hyperparameter, etc. \n",
    "\n",
    "When the **difference between training and dev error rates** are large in the context of the difference between training and Bayes error rates, then focus should be to **reduce variance**, such as: \n",
    "\n",
    "* using more regularization, or \n",
    "* getting more training data, or\n",
    "* use a different NN architecture/hyperparameter, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "## Error Analysis\n",
    "\n",
    "Count mis-labelled dev set examples to get an idea of upper bound imporovement potential. \n",
    "\n",
    "Use a spreadsheet to score each potential issues/ideas.\n",
    "\n",
    "### Incorrect Labels\n",
    "\n",
    "DL algorithms are quite robust to **random errors** in the training set. They are not robust to **systematic errors**.\n",
    "\n",
    "For **dev & test sets**, in error analysis, you can try to identify them, and work out the precentage impact. If significant, fix the data.\n",
    "\n",
    "Breakdown the overall dev set error into:\n",
    "\n",
    "* error due to incorrect labels, %\n",
    "* errors due to other issues, %\n",
    "\n",
    "When correcting labels:\n",
    "\n",
    "* apply the same process for both dev and test sets to ensure they continume to come from the same distribution\n",
    "* consider examples your algo got right as well as ones it got wrong\n",
    "* training and dev/test data may now come from slightly different distribution (often this is ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mismatched Training and Dev/Test set\n",
    "\n",
    "Problem: 2 sets of data coming from different distributions, e.g. high and low resolution images.\n",
    "\n",
    "### Option 1\n",
    "\n",
    "Combine data and shuffle. **Disadvantage**: dev and test set may only have a small portion of the data that you actually care about.\n",
    "\n",
    "### Option 2\n",
    "\n",
    "Split the data coming from the distribution you **want to target** into two parts, one part is mixed with the other data set to form the training set. The remaining half is **further split** into dev and test sets.\n",
    "\n",
    "This method gives better performance because even though most of the training data is from a different distribution, the dev/test sets are from your target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bias / Variance \n",
    "\n",
    "With option 2 above, bias/variance trade off assessment changes too. Dev set error would be higher than the training set error because they belong to different distributinos, we can **no longer** say we have a high variance problem.\n",
    "\n",
    "**Solution**: shuffle the original training set, then split a small portion as the **training-dev** set, the rest remain as training set. Train the model with the smaller training set, and use training-dev set for error analysis. If training-dev set error is still high, then we can conclude that there is a variance problem. \n",
    "\n",
    "If training-dev set error is close to training error, but dev/test set error rates are high, then you have a **data mismatch** problem.\n",
    "\n",
    "\n",
    "|       | Error Rate | Comment |\n",
    "|:------|-----------:|:--------|\n",
    "| Human Level | 4% | (proxy for Bayes Error) |\n",
    "| Training set | 7% | Avoidable Bias = 7% - 4% = 3% |\n",
    "| Training-dev set | 10% | Variance = 10% - 7% = 3% | \n",
    "| Dev set | 12% | Data mismatch error = 12% - 10% = 2% | \n",
    "| Test set | 12% | Degree of overfitting to dev set = 12% - 12% = 0% |\n",
    "\n",
    "More general formulation\n",
    "\n",
    "|                | General Data          | Specific Target Data | \n",
    "|:---------------|:---------------------|:--------------------|\n",
    "| Human Level    | \"Human level\", 4%     | 6% (Ask human to label)  |\n",
    "| Error on data trained on | \"Training error\", 7% | 12% (use target data in training)   |\n",
    "| Error on data not trained on | \"Training-dev error\", 10% | \"Dev/Test error\", 12% | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mismatch\n",
    "\n",
    "* Carry out manual error analysis to understand the differences between training and dev/test sets\n",
    "* Make training data more similar; or collect more data similar to dev/test sets\n",
    "\n",
    "**Artifical Data Analysis** \n",
    "\n",
    "Example: Combine clean speech data + car noise = **synthesized** in-car audio\n",
    "\n",
    "**Caution**: if car noise is too small here, e.g. 1hr vs 10k hours clean speech, it's possible that the network would **overfit** the noise/synthesized data. \n",
    "\n",
    "Synthesized data may only represent a small subset of the full data set/distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Example: Train a network with images for an image recognition task, then remove the weights for the last layer (typically FC), $L$, replace with another layer with randomly initialized weights, using radiology images to training the network again for a radiology diagnosis task. Weights for layers before the last layer are **frozen** in the new training.\n",
    "\n",
    "Sometimes we pre-compute the input fed through the frozen layers, then save the output to disk for transfer learning training.\n",
    "\n",
    "Depending on how much radiology image data you have, you can:\n",
    "\n",
    "* small data: freeze weights for previous layers and only retrain the weights for the last layer\n",
    "* large data: retrain all weights (in this case, the first training is known as **pre-training**, second training is known as **fine-tuning**.\n",
    "\n",
    "When does it make sense for transfer learning from A to B?\n",
    "\n",
    "* Task A and B have the same input x\n",
    "* You have a lot more data for task A than B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task Learning\n",
    "\n",
    "Autonomous driving needs to identify a lot of different objects. Output label $\\hat{y} \\in \\mathbb{R}^N$, then loss function is (summing over all dimensions of $\\hat{y}$:\n",
    "\n",
    "$$ \\mathcal{L} = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{N} \\mathcal{L}\\big(\\hat{y}_j^{(i)}, y^{(i)}\\big) $$\n",
    "\n",
    "In the lables $y$, there could be `nan` values because some objects may not be labelled. The summation above on $j$ should ignore these `nan` fields.\n",
    "\n",
    "When does multi-task learning make sense?\n",
    "\n",
    "* Train on a set of tasks that could benefit from having shared lower-level features\n",
    "* Usually: amount of ata you have for each task is quite similar\n",
    "* Can train a big enough network to do well on all the tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Learning\n",
    "\n",
    "Traditional pipeline type of flow works well with small datasets, such as 3000hrs of data.\n",
    "\n",
    "But end-to-end learning works better with 10k+ hours of data.\n",
    "\n",
    "Sometimes you don't have enough data to solve an end-to-end problem, then you can break it down into steps, when for each step you have a lot more data.\n",
    "\n",
    "End-to-End works quite well for machine translation. Not so well for estimating child's age, solution is to break it down, measure each bone's length. \n",
    "\n",
    "### When to use End-to-End?\n",
    "\n",
    "Pros:\n",
    "* Let the data speak\n",
    "* Less hand-designing of components needed\n",
    "\n",
    "Cons:\n",
    "* May need large amout of data\n",
    "* Excludes potentially useful hand-signed componenets\n",
    "\n",
    "Key question: **Do you have sufficient data to learn a function of the complexity needed to map x to y?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
