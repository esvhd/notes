---
title: "Applied Predictive Modeling"
author: "zwl"
date: "25 July 2018"
output: html_document
---

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(AppliedPredictiveModeling)
library(caret)
library(corrplot)
```

# Quick Reference

## Model Weaknesses

### Models sensitive to **centering & scaling**

* PCA, because PCA tries to explain variance.
* PLS, same as PCA.
* SVM, because kernel is sum of cross products of support vectors and new 
samples.
* KNN, based on distance measure so scale can have huge impact.
* LDA, also remove near zero variance predictors.

Those **less** sensitive: trees.


### Models sensitive to **Outliers**

* OLS, because of the tendency to reduce residual for outliers.
* PCA, because of variance caused by outliers.

Less sensitive to **outliers**:

* Robust linear regression with Huber loss
* Trees based methods (due to splits)
* SVM (due to support vectors)

### **Collinearity**

* Linear regression, causing unstable coefficients / large variance
* Logistic regression, unstable coefficients / large variance
* LDA, better than logistic regression but still breaks when multicollinearity
becomes extreme.
* Trees, choice of which correlated feature to split becomes somewhat random,
also impact importance values.

### Number of features **larger** than number of samples, $N < p$

* Linear regression, cannot invert $X^T X$
* LDA, cannot invert covariance matrix of the data. Could happen in CV when 
some data is reserved for validation.


## Model Comparson Plots and Functions:

* For regression, **predicted vs targets** and **predicted vs residuals**.

* `caret::defaultSummary()` for model summary.

* `caret::featurePlot()` shows univariate plot of feature vs tarets.

* `Hmisc::describe()` describes data by colume, p228

* `parallelPlot(model)` **parallel-coordinate plots**: p230-231, 
use line plots to show resampledresults (RMSE or $R^2$) across different models. Each line shows the **same** sample set across all models.


## `R` Coding

### Formula

* `.` represent everything, e.g. `outcome ~ .`
* `(.)^2` expandas into model with all linear terms and all **two-factor**
interactions
* `I()` **as is** function, e.g. `I(feature^2)` will use squared feature

Use `paste()` and `as.formula()` to create formula from multiple string text. 

```{r paste, eval=FALSE}
f <- paste('out ~ x + ', 'y + I(c^2)')
f <- as.formula(f)
```

### Useful functions




# Data Preprocessing

## Transforming Features

Rule of thumb: ratio of highest and lowest values exceeds 20 indicates 
significant skewness. Or compute skewness statistics with `e1072::skewness()`.

To resolve **skewness** in data, can apply either log, square root or inverse.

Alternatively, use **Box-Cox** transformations `caret::BoxCoxTrans()`, which is 
more straightforward, less prone to numerical issues and just as effective as 
Box and Tidwell.

## Transformation for Outliers / Examples

Tree-based models (due to splits) and SVM classification models (due to 
support vectors) are less prone to outliers. 

Center and scale data, then apply **spatial sign** transformation 
`caret::spatialSign()`.

## Transformation for Entire Dataset

PCA is such an example. `prcomp()`

$$ PC_1 = (a_{j1} \times Predictor_1) + (a_{j2} \times Predictor_2) + \cdots
+ (a_{jp} \times Predictor_p) $$

Coefficients $a_{j1}, a_{j2}, \cdots, a_{jp}$ are known as **loadings**, 
stored in returned result from `prcomp()` as `rotation`.

PCA is naturally drawn to summarizing predictors that have more variation, 
therefore **centering and scaling** is very important for PCA.

When using scatter plot to show pairs of princial components, it is important
for the charts to have the **same scale**. Otherwise the effects may be 
distorted by different scale graphically.


## Missing Data

**Informative missingness**: missing data pattern is instructional on its own,
e.g. its pattern is related to the outcome. E.g. customer ratings, those who
rate usually have strong opinions, either good or bad.

For some data where below a certain threshould it becomes harder to measure by
the device at hand, we can impute them as a random number between 0 and the 
device lowest threshold.

Some methods such as tree-based models can specifically account for missing
values. Ch. 8 for more.

Most relevant scheme is to build an **imputation model** for each predictor in 
the dataset.

* K-means and take average
  * Pros: imputed data is confined to be within the range of training set 
  values.
  * Cons: entire training data has to be stored; number of neighbour is a 
  tuning parameter. (Troyanskaya et al. 2001 showed that nearest neighbour
  approach to be fairly robust to the tuning parameters, as well as the 
  amount of missing data)
  
Some data that has very little variation can be removed. `caret::nearZeroVar()`
method can help to identify those columns / predictors.


## Collinearity / Multicollinearity

PCA can help. If the first PC accounts for a large % of variance, it implies
that there is at least one group of predictors that represent the same
information.

PCA **loadings** can be used to understand which predictors are associated with
each component to tease out this relationship.

**VIF**, variance inflation factor, can be used to identify multicollinearity,
but beyond linear regression, it may be inadequate: 

* designed for linear models, requires n > p, more samples than predictors
* when it identifies issues, it does not determine which should be reomoved to
solve the problem.

The book includes a huristic algorithm that iteratively remove predictors by
looking at correlated predictor pairs and the remove the one with the 
**higher** average correlation with the rest of the predictors.


## One-Hot Encoding

When converting categorical variables to one-hot encoding format, use 
`num_category - 1` if model has an intercept, otherwise if using `num_category`
there may be numerical issues.

Use `dummyVars()` to create one-hot encoding. Recommended to use the full
set of dummy variables when working with tree-based models.

```
simpleMod <- dummyVars(~Mileage + Type, data=data, levelsOnly=TRUE)
# Mileage is numerical, Type is categorical
# converts Type to one-hot
predict(simpleMod, data)
```

## Binning

Never mannually bin continuous variables.


## Code

`caret::preProcess()` can apply a pipeline of transformations to given data.

`caret::predict()` applys a model to given data.

```{r prepro, eval=FALSE}
data(segmentationOriginal)
segData <- subset(segmentationOriginal, Case == 'Train')
cellID <- segData$Cell
class <- segData$Class
case <- segData$Case
segData <- segData[, -(1:3)]
statusColNum <- grep('Status', names(segData))
segData <- segData[, -statusColNum]

skewness(segData$AngleCh1)
skewValues <- apply(segData, 2, skewness)
head(skewValues)

# compute box-cox transform lambda
Ch1AreaTrans <- BoxCoxTrans(segData$AreaCh1)
# apply box-cox transform
predict(Ch1AreaTrans, head(segData$AreaCh1))

pc <- prcomp(segData, center=TRUE, scale. = TRUE)
pc.var <- pc$sdev^2 / sum(pc$sdev^2)*100

trans <- preProcess(segData, method=c('BoxCox', 'center', 'scale', 'pca'))
trans

# beautiful correlation matrix
corrplot(cor(segData), order='hclust')

```


# Overfitting / Model Tuning

## Data Splitting

`caret::createDataPartition(classes, p=.8, list=FALSE)`

Resampling usually produced better results than a single test set.

**Stratified Random Sampling** draws random samples within each subgroup of 
the data. 

**Maximum dissimilarity sampling**, computationally high cost as it'd require
a lot of pair-wise computation. p68. `maxDissim()`

## Resampling

### `k`-Fold Cross Validation

```{r kfold, eval=FALSE}
library(AppliedPredictiveModeling)
data(twoClassData)

# a few different ways to do the same
caret::createDataPartition(classes, p=.8, times=3)
caret::createFolds(data, k=10, returnTrain=TRUE)
caret::createMultiFolds(classes, k = 10, times=3)
```

Randomly sample and divide data into train / validation sets, repeat `k` times
and take the average performance.

As `k` gets **larger**, the difference in size between the training set and the 
resampling subset gets **smaller**, the **bias** of the technique **decreases**.

`k`-fold CV generally has **high variance** compared to other methods, might
not be attractive. With large training sets, the potential issue with variance
and bias become negligible.

### Generalized CV

$$ GCV = \frac{1}{n} \sum^n_{i=1} \bigg( \frac{y_i - \hat{y}_i}{1 - df / n} \bigg)^2 $$

Where `df` is degree of freedom / # of parameters estimated by the model.

## Repeated Training / Test Splits

Aka Leave Group Out CV, or Monte-Carlo CV. 

Repeatedly resample with replacement and split data into **modeling** and 
**prediction** sets, 

Bias decreases as the amount of data in the subset approaches the amount in the 
modeling set. Rule of thumb 75-80%.

**Higher** no. of repetitions **decreases** the uncertainty of the performance 
estimate. Book recommends 50-200 iterations (i.e. large) to get stable estimates 
of performance.


## Boootstrap

Randomly resample data **with replacement**. Samples not selected are referred
to as **out-of-bag** samples.

Bootstrap error rates tend to have **less uncertainty** than k-fold CV. However, 
on average **63.2%** of the data in bootstrap is represented at least once, 
therefore this technique has **bias similar** to k-fold CV when $k \approx 2$.
This bias will decrease as data size becomes larger.

### 632 Method

To address the bias issue, the 632 method combines simple bootstrap estimate 
and the estimate from re-predicting the training set (aka. the **apparent
error rate**). 

632 error = 0.632 * simple bootstrap estimate + 0.368 * apparent error rate

632 method reduces bias but can be unstable with small sample size. It can 
still result in unduly optimisitic results when the model **severely** overfits,
i.e. **apparent error rate is close to zero**. 

**632+ method** was created to address this severely overfit case.

## Author's recommendations

No resampling method is uniformly better than others. 

If sample size is small, use repeated 10-fold CV:

* Good bias and variance properties
* computational cost not high

For model selection, use bootstrap methods given low variance.

For large data set, the differences between resampling methods become less
pronounced, use simple 10-fold CV, which gives acceptable variance and low bias.

See later chapters for **optimization bias** for small datasets.

## Model Selection

Here the tips are actually quite similar to what Andrew Ng gave for DL models.

Essentially fit a more flexible / complicated model first to discover 
**performance ceiling**. Then use simpler models and choose one that is 
reasonaly close to the complex model.

With increased precision, there is higher likelihood that models can be
differentiated in terms of **sensitivity** than for **specificity**.


## Parameter Tuning

Examples below.

```{r resample, eval=FALSE}
data(GermanCredit)

library(doMC)
# register to use 8 cores
registerDoMC(8)

GermanCredit <- GermanCredit[, -nearZeroVar(GermanCredit)]
GermanCredit$CheckingAccountStatus.lt.0 <- NULL
GermanCredit$SavingsAccountBonds.lt.100 <- NULL
GermanCredit$EmploymentDuration.lt.1 <- NULL
GermanCredit$EmploymentDuration.Unemployed <- NULL
GermanCredit$Personal.Male.Married.Widowed <- NULL
GermanCredit$Property.Unknown <- NULL
GermanCredit$Housing.ForFree <- NULL

## Split the data into training (80%) and test sets (20%)
set.seed(100)
inTrain <- createDataPartition(GermanCredit$Class, p = .8)[[1]]
GermanCreditTrain <- GermanCredit[ inTrain, ]
GermanCreditTest  <- GermanCredit[-inTrain, ]

# a few methiods availalbe
# e1071::tune()
# ipred::errorest()
# Design::validate()

svm.fit <- caret::train(Class ~ ., 
                        data=GermanCreditTrain,
                        method='svmRadial',
                        preProc=c('center', 'scale'),
                        # tune SVM cost parameter from [2^-1, 2^-2, ..., 2^7]
                        tuneLength=10,
                        # by default bootstrap is used to evaluate performance
                        # use trainControl() to use k-fold CV.
                        trControl=trainControl(method='repeatedcv', 
                                               repeats = 5, 
                                               classProbs = TRUE))

plot(svm.fit, scales=list(x=list(log=2)))

glmProfile <- train(Class ~ .,
                    data = GermanCreditTrain,
                    method = "glm",
                    trControl = trainControl(method = "repeatedcv", 
                                             repeats = 5))


# To compare models 
resamp <- caret::resamples(list(SVM = svm.fit, Logistic = glmProfile))
summary(resamp)

summary(diff(resamp))

predictedProbs <- predict(svm.fit, newdata = GermanCreditTest, type = "prob")
head(predictedProbs)

```

# Regression Performance Measurement

## $R^2$ 

$R^2$ is a measure of correlation, **not accuracy**. 

It depends on the **variation** in the outcome. If RMSE is 1, for outcomes with
variance 4 and 3, $R^2$ is 3/4 and 2/3 respectively.

**Rank correlation, Spearman's rank correlation**, to compute this, rank the
observed and predicted outcomes, the correlation between these rank is 
calculated. `corr(..., method='spearman')`

## Bias-Variance Trade-off

For $MSE = \frac{1}{n} \sum^n_{i=1} (y_i - \hat{y}_i)^2, with **assumptions**:

1. data points are statistiaclly independent
2. residuals have a theoretical mean of zero and constant variance $\sigma^2$,
which is called the **irreducible noise**.

We have:

$$ \mathbb{E}(MSE) = \sigma^2 + (\text{Model Bias})^2 + \text{Model Variance}$$

`extendrange(c(1, 5))` extends a numerical range by a percentage.

`abline()` base plot function to add a line in a chart.

A useful **diagnosis plot** is to plot:

1. observed values versus predicted
2. predicted values versus residual


# Linear Regressin & Its Cousins

Given $y = \beta X$ the solution is $\beta = (X^T X)^-1 X^T y$

A unique solution of $(X^T X)^-1$ exists when:

1. no predictor can be determined from a combination of one or more of the 
other predictors
2. the number of samples is greater than the number of predictors

When **collinearity** exists, R fits the largest identifiable model by 
removing variables in the reverse order of appearance in the model formula.

Another **drawback** of multiple linear regression is that the solution is a 
hyperplane, which cannot handle curvature or nonlinear structure.

**Robust linear regression** using **Huber loss** (uses squared loss when error
is small and L1 loss when error is above a threshold) can defend against 
outliers.

## Partial Least Squares

Package: `pls::plsr()`

High collinearity results in high variance in OLS parameters. Solution could
be: 

1. Remove highly correlated predictors described in Section 3.3
2. usd PCA

(1) does not resolve multi-collinearity issue, hence does not guarantee a 
stable least squares solution.

PCA does **not** necessarily produce new predictors that explain the response.
PCA tris to explain the variability, however, if the variability in the 
predictor space is not related to the variability of the response, **PCR**
can have difficulty identifying a predictive relationship when one might exist.

Author recommends **PLS** when there are correlated predictors and a linear
regression type solution is desired.

**PLS** finds linear combinations of the predictors (components), aiming to 
**maximize component covariance with the response**, i.e. it finds components
that:

* maximally summarize the variation of the predictors while simultaneously
* requiring these components to have maximum correlation with the response

Like PCA, data need to be **centered and scaled** before applying `PLS`.

`PLS` has one tuning parameters: number of components to retain. Resampling
can be used to determine the optimal number of components.

NIPALS algorithm works well for small/moderate datasets (e.g. $< 2500$ samples
and $< 30$ predictors). Dayal & MacGregor (1997) is the most computationally
efficient for various sizes (e.g. 500-10000 samples, 10-30 predictors,
1-15 responses, 3-10 components).

`PLS` components summarize the data through linear strctures of the original
predictor space that are related to the response. When more intricate 
relationships exist, authoers suggest employing other techniques, rather than
trying to improve `PLS` through augementation.

### Variable Importance in the Projection (VIP)

`k`-component case, for the importance of $j$th predictor:

* numerator: weighted sum of normalized weights corresponding to the $j$th 
predictor
* denominator: total amount of response variation explained by all `k` 
components.

Rule of thumb: VIP > 1.0 is considered to have predictive information for the
response

## Penalized Models

Combating **colinearity** by using **biased** models may result in regression
models where the **overall MSE** is competitive.

**LARS** model can be used to fit lasso models more efficiently, especailly
in high dimensional problems.

Elastic net allows for effective regularization via L2 loss with feature 
selection quality of lasso L1 penalty.

## Code

```{r linear_reg, eval=FALSE}
data(solubility)

set.seed(2)

trainingData <- solTrainXtrans

# visualize data vs target
featurePlot(solTrainXtrans[, -notFingerprints],
            solTrainY,
            between = list(x = 1, y = 1),
            type = c("g", "p", "smooth"),
            labels = rep("", 2))

# fit linear model
trainingData$Solubility <- solTrainY
lmFit <- lm(Solubility ~ ., data=trainingData)

summary(lmFit)
lmPred <- predict(lmFit, solTestXtrans)
lmValues <- data.frame(obs=solTestY, pred=lmPred)

# caret::defaultSummary() to estimate performance
defaultSummary(lmValues)

# robust linear model rlm()
lmFit <- rlm(Solubility ~ ., data=trainingData)

# use 10-fold CV
ctrl <- trainControl(method = 'cv', number=10)
set.seed(100)
lmFit1 <- train(x = solTrainXtrans, y = solTrainY, method='lm', 
                trControl = ctrl)

# find highly correlated predictors and remove
tooHigh <- findCorrelation(cor(solTrainXtrans), .9)
trainXfiltered <- solTrainXtrans[, -tooHigh]
testXfiltered  <-  solTestXtrans[, -tooHigh]

lmTune <- train(x = trainXfiltered, y = solTrainY,
                method = "lm",
                trControl = ctrl)

rlmPCA <- train(x = trainXfiltered, y = solTrainY,
                method = "rlm",
                # preProcess = c('center', 'scale', 'pca'),
                preProcess = 'pca',
                trControl = ctrl)
rlmPCA
```

```{r pls, eval=FALSE}
library(pls)
plsFit <- plsr(Solubility ~ ., data=trainingData)

# can choose number of component for prediction
predict(plsFit, solTestXtrans[1:5,], ncomp=1:2)

loadings(plsFit)
scores(plsFit)

scoreplot(plsFit)
plot(plsFit)

plsTune <- train(x = solTrainXtrans, y = solTrainY,
                 method='pls', 
                 tuneLength = 20,
                 trControl = trainControl(method = 'repeatedcv', 
                                          number = 10),
                 preProcess = c('center', 'scale'))
```

```{r penalized, eval=FALSE}
library(elasticnet)

# Ridge
lm.ridge()

# elastic-net, lambda parameter is ridge weight
# or in glmnet package
elasticnet::enet(x = as.matrix(solTrainXtrans), y = solTrainY, lambda = .001)

```


# Non-Linear Methods

## MARS

Skipped for now.

## SVM

Book focuses on **$\epsilon$-insensitive regression**.

SVMs for regression use a function similar to the Huber function, but with 
difference. 

Given $\epsilon$ give by user, data points with residuals **within** the 
threshold do not contribute to the regression fit, while data points with
an absolute difference **greater** than the threshold contribute a linear-scale
amount.

Consequence: 

1. more robust t outliers, 
2. samples that fit well have **no effect** on the regression equation.

Poorly predicted points define the line. 

SVM is over-parameterized, but the cost value effectively regularizes the model,
helps to alleviate this problem.

Individual training data points are required for new predictions, these are 
known as **support vectors**.

Authors found that the cost parameter provides more flexibility for tuning the
model and therefore suggest fixing a value for $\epsilon$ and tuning over the
other kernel parameters.

Since the predictors enter into the model as sum of cross products, differences
in predictor scales can affect the model. Therefore, data should be 
**centered and scaled** before feeding into svm.

Bayesian analog to SVM: *relevance vector machine* (Tipping 2001). There are 
usually less relevance vectors than support vectors in model.

## KNN

Data should be **centered and scaled** before feeding into KNN. 

KNN has one parameter to tune: the number of neighbours, `k`.

KNN needs to store all training data, therefore is not memory efficient.

KNN can have **poor performance** when local predictor structure is not 
relevant to the response.

## Code

Neural network package `nnet`.

`MARS` found in `earth` package, useful functions:

* `earth()` to fit a model
* `plotmo()` to plot 
* `earth::evimp()` or `caret::varImp()` for variable importance

`KNN` use `knnreg::knn()`

```{r svm, eval=FALSE}

# e1071::svm()
# kernlab package has more extensive implementation of SVM for regression
# use kernlab::ksvm
# kernlab laso has relevance vector machine, rvm()

library(kernlab)
data(solubility)

solTrainXtrans$solTrainY <- solTrainY

# supported kernels: rbfdot, polydot, vanilladot, etc.
svmFit <- ksvm(solTrainY ~ ., data = solTrainXtrans,
               # x = solTrainXtrans, y = solTrainY,
               kernel = 'rbfdot', kpar = 'automatic',
               C = 1, epsilon = .1)

# use train() to tune model. method parameter can take:
# svmRadial, svmLinear, svmPoly to fit different kernels
svmRTuned <- train(solTrainY ~ ., data = solTrainXtrans,
                   method = 'svmRadial',
                   preProc = c('center', 'scale'),
                   tuneLength = 14, 
                   trControl = trainControl(method = 'cv'))

# to choose the selected model:
fm <- svmRTuned$finalModel

# to access the support vectors indices
fm@SVindex

```


# Regression Trees / Rule-based Models

By construction, trees / rule-based models can effective handle many types 
of predictors (sparse, skewded, continuous, categorical, etc.), **without**
the need to pre-process them.

No need to specify the form of the predictor's relationship with the response.

They can handle missing data and implicitly conduct feature selection. 

## Single Tree Weaknesses:

1. model instability, e.g. a small change in data can drastically change the 
structure of the tree.
2. Less than optimal predictive performance, due to rectanglur decision 
boundries, which may not suit the data / true relationship.
3. For a single tree, the number of possible predicted outcome from a tree is
finite and is determined by the number of terminal nodes.
4. Trees suffer from **selection bias**: predictors with a higher number of
distinct values are favoured over more granular predictors. 
5. As the number of missing values increase, the selection of predictors become
more **biased**.

Solution: ensemble methods.

## Trees

Single tree building:
`rpart::rpart()`
`party::ctree()`

### Cost Complexity Pruning

Goal of pruning is the find a right sized tree that has the smallest error
rate. Penalize the error rate using the size of the tree:

$$ SSE_{c_p} = SSE + c_p \times \text{# Terminal Nodes} $$

where $c_p$ is the tuned **complexity parameter**. Large values
may result the tree to have just one or no splits - which means that 
**no predictor** adequately explains enough of the variance in the outcome at
the chosen value of complexity parameter.

Also uses **one-standard deviation** rule for model selection. Alterantively,
use tree associated with the numerically smallest error.

### Missing Values

Use **surrogate splits**. A surrogate split is one whose results are similar to 
the original split actually used in the tree.

### Feature Importance

One way is to measure importance is to keep track of the overal reduction ni 
the optimization criteria for each predictor.

**Disadvantage** of this approach: when some predictors are highly correlated,
the choice of which to use in a split is somewhat random. Which results in
more predictors being selected than those actually needed, affecting variable
importance values. 

See Weakness sector for more disadvantages. 

**Unbiased** regression trees:

* GUIDE
* conditional inference trees

## Regression Model Trees

`RWeka::M5P()` for trees

`RWeka::M5Rules()` for rule based model.

**Drawback** for simple regression trees: each terminal node uses the 
**average** of the training set outcomes in the node for prediction. 

**Implication**: the model may not do a good join in prediction outcomes 
that are extremely high or low.

Alternatively, use a **different estimator** in the terminal node.

**Model tree** vs simple trees:

* split criterion is different
* terminal node predicts the outcome using a linear model
* when a sample is predicted, it is often a combination of the predictions from
different models along the same path through the tree.

Main implementation: **M5** in `Weka` package.

**Split** criterion:

The **expected reduction** in the node's error rate is used.

$$\text{reduction} = SD(S) - \sum_{i=1}^{P}\frac{n_i}{n} \times SD(S_i) $$

Where:

* $S$ is the entire dataset
* $S_1, \cdots, \S_P$ - $P$ subsets of the data after splitting
* $n_i$ number of samples in partion $i$

This metrics determines if the total variation in the splits, weighted by 
sample size, is lower than in the presplit data.

See p185 for more details on tree construction.

## Rule-Based Model

p190

## Bagged Trees

Short for Bootstrap Aggregation. 

Bagging advantages: 

1. Effectively reduces variance of a prediction through its 
aggregation process.
2. They provide their own internal estimate of predictive performance (using
**out of bag** data) that correlates well with either CV or test set estimates. 

Bagging stable, lower variance models like linear regression and MARS offer
**less** improvment in predictive performance.

Tress are great for bagging because they have high variance.

One parameter to tune: number of bootstrap samples to aggregate $m$.

often we see exponential decrease in predictive improvement as the number of
iterations increases. The most improvments in prediction perforamnce is obtained
with a smaller number of trees (m < 10).

If performance is not at an acceptable level after 50 bagging iterations,
authors suggest trying other more powerful ensemble methods as random forest
and boosting.

**Caveats**:

1. high computational cost and memory requirement
2. bagged model is less intrepretable.

## Random Forest

Need to reduce tree correlation. Algo on page 200, 8.2 listing.

Algo randomly selects predictors at each split, decreases tree correlation.

Tuning parameters:

* number of randomly selected predictors, $k$, to choose from at each split, 
referred to as $m_{try}$
  * Breiman 2001 suggests setting $m_{try} = \frac{P}{3}$ where $P$ is the 
  number of predictors.
* number of trees for the forest.
  
Breiman proved that RF is **protected from overfitting**. Authors found that
tuning parameters do not have a drastic effect on performance.

RF is robust to **noisy response**. The independence of learnings can underfit 
data when response is not noisy.

Strobl et al. (2007) proved that the correlations between predictors can have a
significant impact on the importance value. E.g. non-informative predictor
that has high correlation with informative predictor end up having higher or
equal importance as weakly informative predictors.

Also showed that $m_{try}$ tuning parameter has a **series** effect on the 
importance values.

Impact of between-predictor correlations is to **dilute the importance** of key 
predictors. This is **not** addressed by Strobl's 2007 alternative importance
measure.

RF and single tree such as CART may have very different ordering of feature
importance, this highlights that a single tree's greediness prioritize 
predictors differently than a RF.

## Boosting

Several researchers showed that boosting can be interpreted as a forward
stepwise additive model that minimizes exponential loss.

Basic principles, see p205 for algo:

* Given a **loss function** and a **weak** learner, the algo seeks to find an
additive model that minimizes the loss function.
* The model is fit to the residual to minimize the loss
* The current model is added to the previous model, and the procedure continues
for a pre-defined number of iterations.

Requires a **weak** learner, almost any technique with tuning parameters can
be made into a weak learner. Trees are great for reasons below:

1. can be weak learner simply by restricting its depth
2. separate trees can be easily added together
3. trees can be generated quickly.

For **tree** based gradient boosting, two parameters:
1. number of iterations
2. depth of each tree, aka. **interation depth**

Boosting applies a greedy strategy of choosing the optimal weak learner at each
stage. The drawback is that it might not find the optimal global model and
can overfit the training data.

Remedy: regularization, only a fraction of the current predicted value is added
to the previous iteration's predicted value. The learning rate, $\lambda$ is 
between 0 and 1. 

**Stochastic Gradient Boosting** (Friedman) added another step in each 
iteration: randomly sample a fraction of the training data to be used in 
the current training iteration. This fraction can be tuned in CV.

Variable importance for boosting is a function of the reduction in squared
error: the improvment in squared error due to each predictor is summed within 
each tree in the ensemble, then this improvment sum is averaged over the 
ensemble to yield an overall importance value. 

The importance profile for boosting has a much **steeper** slope than the on 
for random forest. This is due to the fact that the trees in boosting are 
dependent on each other and therefore will have correlated structures as the
method follows by the gradient.

## Code

Packages: `rpart`, `party`, `partykit`

### Single trees:

* `rpart::rpart()` - fits w/ CART
* `party::ctree()` - fits w/ conditional inference trees

```{r trees, eval=FALSE}
library(rpart)
library(party)
# Turning for rpart() is through paramter `rpart.control`
# for party::ctree() this is through `ctree_control`

# Or, via train() to tune both complexity parameter and max node depth.
# method = 'rpart' to tune over complexity parameter
# method = 'rpart2' to tune over max node depth.
# method = 'ctree' to tune mincriterion
# method = 'ctree2' to tune over max node depth.
rpartTuned <- train(solTrainY ~ ., data = solTrainXtrans,
                    method='rpart2', tuneLength=10,
                    trControl = trainControl(method = 'cv'))

library(partykit)
# plot tree
rpartTree2 <- as.party(rpartTree)
plot(rpartTree2)
```

### Model Trees

Packages: `RWeka`

`M5` model trees: `RWeka::M5P()` with formula interface. For `train()`,
use parameters:

* `method='M5'`
* `control = Weka_control(M = 10)`

`M5` results can be plotted by calling `plot(model)`.

### Bagging

* `ipred::bagging()` - formula interface
* `ipred::ipredbagg()` - non-formula interface

Other ways:

* `RWeka::Bagging()`
* `caret::bag()` - general framework that works for many models
* `party::cforest()` - Conditional inference trees, with parameter:
`controls = cforest_control(mtry = ncol(trainData) - 1)` to set 
$m_{try} = \text{# of predictors}$.

### Random Forest

Package: `randomForest::randomForest()`

Parameters:

* `mtry` - number of predictors that are randomly sampled as candidates for 
each split. Default is # of predictors / 3.
* `ntree` - # of bootstrap samples. Default 500, but should use at least 1000.
* `importance` - default is FALSE. First set to `TRUE` then call 
`randomForest::importance(..., type=1, scale=F)` to get the right importance 
values.

For `caret::train()`: `method = 'rf'` or `'cforest'`

For `cforest` objects, variable importance can be obtained by calling
`party::varimp()`

`caret::varImp()` has a **unified wrapper** that works for tree-models from:
`rpart`, `classbagg` produced by `ipred`'s bagging, `randomForest`, 
`cforest`, `gbm`, and `cubist`

### Boosted Trees

* `gbm::gbm.fit(x, y, distribution = 'gaussian')` - non-formula interface
* `gbm::gbm(y ~ x, data = data, distribution = 'gaussian')` - formula interface

`distribution` parameter defines the type of loss function that will be 
optimized during boosting. Use `gaussian` for ** continuous response**. 

Other parameters for `gbm`:

* `n.trees` - number of trees
* `interaction.depth` - depth of trees
* `shrinkage`
* `bag.fraction` - proportion of observations to be sampled

For `caret::train()`, see below example:

```{r boost, eval=FALSE}
library(gbm)

gbmGrid <- expand.grid(interaction.depth = seq(1, 7, by = 2),
                       n.trees = seq(100, 1000, by = 50),
                       shrinkage = c(.01, .1),
                       n.minobsinnode=c(5, 10))

gbmTune <- train(solTrainY ~ ., data = solTrainXtrans,
                 method='gbm',
                 tuneGrid = gbmGrid,
                 verbose = F)
```


# Discriminant Analysis / Other Linear Classifiers

## Logistic Regression

When model residuals folloer a normal distribution, minimizing the sum of 
squared residuals also produces **maximum likelihood estimates**.

Given probability of a class $p$ in a binary classification task, because 
$0 <= p <= 1$, to formulate a linear model that we need to model 
**odds**, $\frac{p}{1-p}$. Hence logistic regression is defined by:

$$ \log\bigg( \frac{p}{1-p} \bigg) = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $$

Where $p$ is the number of predictors.

Logistic regression produces **linear** boundaries, unless the predictors used
in the model are non-linear versions of the data (e.g. squared features).


## Linear Discriminant Analysis (LDA)

LDA finds an optimal discriminant vector.

Fisher (1936) and Welch (1939) both discovered it from different angles.

Welch took the approach of minimizing the total probability of 
mis-classification. Imposes assumption of Gaussian data with idential 
covariance.

Non-identical covariance leads to **Quadratic discriminant analysis**.

Fisher tried to find a linear combination of the predictors such that 
**the between group variance was maximized, relative to the within group variance.**

For multi-class tasks, Fisher's model would require $C*p + p(p + 1) / 2$
parameters, where:

* $C$ is the number of classes and 
* $p$ is the number of 
predictors.

**Conditions** for when LDA works:

* more samples than predictors
* covariance matrix of data is invertible
* data can be decently separated by linear hyperplane

When number of samples approach number of predictors, be vary **careful**!

