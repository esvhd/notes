---
title: "Applied Predictive Modeling"
author: "zwl"
date: "25 July 2018"
output: html_document
---

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r setup, include=FALSE, eval=FALSE}
knitr::opts_chunk$set(echo = TRUE)

install.packages(c('caret', 'corrplot', 'pls', 'e1071', 'kernlab', 'ipred',
                   'MASS', 'doMC', 'elasticnet', 'party', 'partykit', 
                   'lars', 'randomForest', 'gbm', 'xgboost',
                   'glmnet', 'pamr', 'pROC', 'rms', 'sparseLDA', 'subselect'))

library(AppliedPredictiveModeling)
library(caret)
# library(corrplot)
```

# Quick Reference

![Model Summary](img/kuhn_model_summary.jpg)


## Model Weaknesses

### Models sensitive to **centering & scaling**

* PCA, because PCA tries to explain variance.
* PLS, same as PCA.
* SVM, because kernel is sum of cross products of support vectors and new 
samples.
* KNN, based on distance measure so scale can have huge impact.
* LDA, also remove near zero variance predictors.

Those **less** sensitive: trees.


### Models sensitive to **Outliers**

* OLS, because of the tendency to reduce residual for outliers.
* PCA, because of variance caused by outliers.

Less sensitive to **outliers**:

* Robust linear regression with Huber loss
* Trees based methods (due to splits)
* SVM (due to support vectors)

### **Collinearity**

* Linear regression, causing unstable coefficients / large variance
* Logistic regression, unstable coefficients / large variance
* LDA, better than logistic regression but still breaks when multicollinearity
becomes extreme.
* Trees, choice of which correlated feature to split becomes somewhat random,
also impact importance values.

TODO: methods to combat collinearity

* `findCorrelation()`, then remove columns with high correlation
* p311, `subselect::trim.matrix()` to eliminate linear combinations, takes
a covariance matrix as input.
  * trimmed columns are stored in `names.discarded`
* `caret::findLinearCombos()`


* PLS / PCA
* Removing pair-wise highly correlated predictors

### Number of features **larger** than number of samples, $N < p$

* Linear regression, cannot invert $X^T X$
* Logistic regression, but can be addressed by adding L2 penalty.
* LDA, cannot invert covariance matrix of the data. Could happen in CV when 
some data is reserved for validation.


## Model Comparson Plots and Functions:

* For regression, **predicted vs targets** and **predicted vs residuals**.

* `caret::defaultSummary()` for model summary.

* `caret::featurePlot()` shows univariate plot of feature vs tarets.

* `Hmisc::describe()` describes data by colume, p228

* `parallelPlot(model)` **parallel-coordinate plots**: p230-231, 
use line plots to show resampledresults (RMSE or $R^2$) across different models. 
Each line shows the **same** sample set across all models.

* `xyplot.resamples()`

* `corrplot::corrplot(..., order='hclust')` for correlation matrix.


## `R` Coding

### Formula

* `.` represent everything, e.g. `outcome ~ .`
* `(.)^2` expandas into model with all linear terms and all **two-factor**
interactions
* `I()` **as is** function, e.g. `I(feature^2)` will use squared feature
* `expand.grid()` to generate combinations of tuning parameters.

Use `paste()` and `as.formula()` to create formula from multiple string text. 

```{r paste, eval=FALSE}
f <- paste('out ~ x + ', 'y + I(c^2)')
f <- as.formula(f)
```


# Data Preprocessing

## Transforming Features

Rule of thumb: ratio of highest and lowest values exceeds 20 indicates 
significant skewness. Or compute skewness statistics with `e1072::skewness()`.

To resolve **skewness** in data, can apply either log, square root or inverse.

Alternatively, use **Box-Cox** transformations `caret::BoxCoxTrans()`, which is 
more straightforward, less prone to numerical issues and just as effective as 
Box and Tidwell.

## Transformation for Outliers / Examples

Tree-based models (due to splits) and SVM classification models (due to 
support vectors) are less prone to outliers. 

Center and scale data, then apply **spatial sign** transformation 
`caret::spatialSign()`.

## Transformation for Entire Dataset

PCA is such an example. `prcomp()`

$$ PC_1 = (a_{j1} \times Predictor_1) + (a_{j2} \times Predictor_2) + \cdots
+ (a_{jp} \times Predictor_p) $$

Coefficients $a_{j1}, a_{j2}, \cdots, a_{jp}$ are known as **loadings**, 
stored in returned result from `prcomp()` as `rotation`.

PCA is naturally drawn to summarizing predictors that have more variation, 
therefore **centering and scaling** is very important for PCA.

When using scatter plot to show pairs of princial components, it is important
for the charts to have the **same scale**. Otherwise the effects may be 
distorted by different scale graphically.


## Missing Data

**Informative missingness**: missing data pattern is instructional on its own,
e.g. its pattern is related to the outcome. E.g. customer ratings, those who
rate usually have strong opinions, either good or bad.

For some data where below a certain threshould it becomes harder to measure by
the device at hand, we can impute them as a random number between 0 and the 
device lowest threshold.

Some methods such as tree-based models can specifically account for missing
values. Ch. 8 for more.

Most relevant scheme is to build an **imputation model** for each predictor in 
the dataset.

* K-means and take average
  * Pros: imputed data is confined to be within the range of training set 
  values.
  * Cons: entire training data has to be stored; number of neighbour is a 
  tuning parameter. (Troyanskaya et al. 2001 showed that nearest neighbour
  approach to be fairly robust to the tuning parameters, as well as the 
  amount of missing data)
  
Some data that has very little variation can be removed. `caret::nearZeroVar()`
method can help to identify those columns / predictors.


## Collinearity / Multicollinearity

PCA can help. If the first PC accounts for a large % of variance, it implies
that there is at least one group of predictors that represent the same
information.

PCA **loadings** can be used to understand which predictors are associated with
each component to tease out this relationship.

**VIF**, variance inflation factor, can be used to identify multicollinearity,
but beyond linear regression, it may be inadequate: 

* designed for linear models, requires n > p, more samples than predictors
* when it identifies issues, it does not determine which should be reomoved to
solve the problem.

p47, The book includes a huristic algorithm that iteratively remove predictors by
looking at correlated predictor pairs and the remove the one with the 
**higher** average correlation with the rest of the predictors.


## One-Hot Encoding

When converting categorical variables to one-hot encoding format, use 
`num_category - 1` if model has an intercept, otherwise if using `num_category`
there may be numerical issues.

Use `dummyVars()` to create one-hot encoding. Recommended to use the full
set of dummy variables when working with tree-based models.

```
simpleMod <- dummyVars(~Mileage + Type, data=data, levelsOnly=TRUE)
# Mileage is numerical, Type is categorical
# converts Type to one-hot
predict(simpleMod, data)
```

## Binning

Never mannually bin continuous variables.


## Code

`caret::preProcess()` can apply a pipeline of transformations to given data.

`caret::predict()` applys a model to given data.

```{r prepro, eval=FALSE}
data(segmentationOriginal)
segData <- subset(segmentationOriginal, Case == 'Train')
cellID <- segData$Cell
class <- segData$Class
case <- segData$Case
segData <- segData[, -(1:3)]
statusColNum <- grep('Status', names(segData))
segData <- segData[, -statusColNum]

skewness(segData$AngleCh1)
skewValues <- apply(segData, 2, skewness)
head(skewValues)

# compute box-cox transform lambda
Ch1AreaTrans <- BoxCoxTrans(segData$AreaCh1)
# apply box-cox transform
predict(Ch1AreaTrans, head(segData$AreaCh1))

pc <- prcomp(segData, center=TRUE, scale. = TRUE)
pc.var <- pc$sdev^2 / sum(pc$sdev^2)*100

trans <- preProcess(segData, method=c('BoxCox', 'center', 'scale', 'pca'))
trans

# beautiful correlation matrix
corrplot(cor(segData), order='hclust')

```


# Overfitting / Model Tuning

## Data Splitting

`caret::createDataPartition(classes, p=.8, list=FALSE)`

Resampling usually produced better results than a single test set.

**Stratified Random Sampling** draws random samples within each subgroup of 
the data. 

**Maximum dissimilarity sampling**, computationally high cost as it'd require
a lot of pair-wise computation. p68. `maxDissim()`

## Resampling

### `k`-Fold Cross Validation

```{r kfold, eval=FALSE}
library(AppliedPredictiveModeling)
data(twoClassData)

# a few different ways to do the same
caret::createDataPartition(classes, p=.8, times=3)
caret::createFolds(data, k=10, returnTrain=TRUE)
caret::createMultiFolds(classes, k = 10, times=3)
```

Randomly sample and divide data into train / validation sets, repeat `k` times
and take the average performance.

As `k` gets **larger**, the difference in size between the training set and the 
resampling subset gets **smaller**, the **bias** of the technique **decreases**.

`k`-fold CV generally has **high variance** compared to other methods, might
not be attractive. With large training sets, the potential issue with variance
and bias become negligible.

### Generalized CV

$$ GCV = \frac{1}{n} \sum^n_{i=1} \bigg( \frac{y_i - \hat{y}_i}{1 - df / n} \bigg)^2 $$

Where `df` is degree of freedom / # of parameters estimated by the model.

## Repeated Training / Test Splits

Aka Leave Group Out CV, or Monte-Carlo CV. 

Repeatedly resample with replacement and split data into **modeling** and 
**prediction** sets, 

Bias decreases as the amount of data in the subset approaches the amount in the 
modeling set. Rule of thumb 75-80%.

**Higher** no. of repetitions **decreases** the uncertainty of the performance 
estimate. Book recommends 50-200 iterations (i.e. large) to get stable estimates 
of performance.


## Boootstrap

Randomly resample data **with replacement**. Samples not selected are referred
to as **out-of-bag** samples.

Bootstrap error rates tend to have **less uncertainty** than k-fold CV. However, 
on average **63.2%** of the data in bootstrap is represented at least once, 
therefore this technique has **bias similar** to k-fold CV when $k \approx 2$.
This bias will decrease as data size becomes larger.

### 632 Method

To address the bias issue, the 632 method combines simple bootstrap estimate 
and the estimate from re-predicting the training set (aka. the **apparent
error rate**). 

632 error = 0.632 * simple bootstrap estimate + 0.368 * apparent error rate

632 method reduces bias but can be unstable with small sample size. It can 
still result in unduly optimisitic results when the model **severely** overfits,
i.e. **apparent error rate is close to zero**. 

**632+ method** was created to address this severely overfit case.

## Author's recommendations

No resampling method is uniformly better than others. 

If sample size is small, use repeated 10-fold CV:

* Good bias and variance properties
* computational cost not high

For model selection, use bootstrap methods given low variance.

For large data set, the differences between resampling methods become less
pronounced, use simple 10-fold CV, which gives acceptable variance and low bias.

See later chapters for **optimization bias** for small datasets.

## Model Selection

Here the tips are actually quite similar to what Andrew Ng gave for DL models.

Essentially fit a more flexible / complicated model first to discover 
**performance ceiling**. Then use simpler models and choose one that is 
reasonaly close to the complex model.

With increased precision, there is higher likelihood that models can be
differentiated in terms of **sensitivity** than for **specificity**.


## Parameter Tuning

Examples below.

```{r resample, eval=FALSE}
data(GermanCredit)

library(doMC)
# register to use 8 cores
registerDoMC(8)

GermanCredit <- GermanCredit[, -nearZeroVar(GermanCredit)]
GermanCredit$CheckingAccountStatus.lt.0 <- NULL
GermanCredit$SavingsAccountBonds.lt.100 <- NULL
GermanCredit$EmploymentDuration.lt.1 <- NULL
GermanCredit$EmploymentDuration.Unemployed <- NULL
GermanCredit$Personal.Male.Married.Widowed <- NULL
GermanCredit$Property.Unknown <- NULL
GermanCredit$Housing.ForFree <- NULL

## Split the data into training (80%) and test sets (20%)
set.seed(100)
inTrain <- createDataPartition(GermanCredit$Class, p = .8)[[1]]
GermanCreditTrain <- GermanCredit[ inTrain, ]
GermanCreditTest  <- GermanCredit[-inTrain, ]

# a few methiods availalbe
# e1071::tune()
# ipred::errorest()
# Design::validate()

svm.fit <- caret::train(Class ~ ., 
                        data=GermanCreditTrain,
                        method='svmRadial',
                        preProc=c('center', 'scale'),
                        # tune SVM cost parameter from [2^-1, 2^-2, ..., 2^7]
                        tuneLength=10,
                        # by default bootstrap is used to evaluate performance
                        # use trainControl() to use k-fold CV.
                        trControl=trainControl(method='repeatedcv', 
                                               repeats = 5, 
                                               classProbs = TRUE))

plot(svm.fit, scales=list(x=list(log=2)))

glmProfile <- train(Class ~ .,
                    data = GermanCreditTrain,
                    method = "glm",
                    trControl = trainControl(method = "repeatedcv", 
                                             repeats = 5))


# To compare models 
resamp <- caret::resamples(list(SVM = svm.fit, Logistic = glmProfile))
summary(resamp)

summary(diff(resamp))

predictedProbs <- predict(svm.fit, newdata = GermanCreditTest, type = "prob")
head(predictedProbs)

```

# Regression Performance Measurement

## $R^2$ 

$R^2$ is a measure of correlation, **not accuracy**. 

It depends on the **variation** in the outcome. If RMSE is 1, for outcomes with
variance 4 and 3, $R^2$ is 3/4 and 2/3 respectively.

**Rank correlation, Spearman's rank correlation**, to compute this, rank the
observed and predicted outcomes, the correlation between these rank is 
calculated. `corr(..., method='spearman')`

## Bias-Variance Trade-off

For $MSE = \frac{1}{n} \sum^n_{i=1} (y_i - \hat{y}_i)^2, with **assumptions**:

1. data points are statistiaclly independent
2. residuals have a theoretical mean of zero and constant variance $\sigma^2$,
which is called the **irreducible noise**.

We have:

$$ \mathbb{E}(MSE) = \sigma^2 + (\text{Model Bias})^2 + \text{Model Variance}$$

`extendrange(c(1, 5))` extends a numerical range by a percentage.

`abline()` base plot function to add a line in a chart.

A useful **diagnosis plot** is to plot:

1. observed values versus predicted
2. predicted values versus residual


# Linear Regressin & Its Cousins

Given $y = \beta X$ the solution is $\beta = (X^T X)^-1 X^T y$

A unique solution of $(X^T X)^-1$ exists when:

1. no predictor can be determined from a combination of one or more of the 
other predictors
2. the number of samples is greater than the number of predictors

When **collinearity** exists, R fits the largest identifiable model by 
removing variables in the reverse order of appearance in the model formula.

Another **drawback** of multiple linear regression is that the solution is a 
hyperplane, which cannot handle curvature or nonlinear structure.

**Robust linear regression** using **Huber loss** (uses squared loss when error
is small and L1 loss when error is above a threshold) can defend against 
outliers.

## Partial Least Squares

Package: `pls::plsr()`

High collinearity results in high variance in OLS parameters. Solution could
be: 

1. Remove highly correlated predictors described in Section 3.3
2. usd PCA

(1) does not resolve multi-collinearity issue, hence does not guarantee a 
stable least squares solution.

PCA does **not** necessarily produce new predictors that explain the response.
PCA tris to explain the variability, however, if the variability in the 
predictor space is not related to the variability of the response, **PCR**
can have difficulty identifying a predictive relationship when one might exist.

Author recommends **PLS** when there are correlated predictors and a linear
regression type solution is desired.

**PLS** finds linear combinations of the predictors (components), aiming to 
**maximize component covariance with the response**, i.e. it finds components
that:

* maximally summarize the variation of the predictors while simultaneously
* requiring these components to have maximum correlation with the response

Like PCA, data need to be **centered and scaled** before applying `PLS`.

`PLS` has one tuning parameters: number of components to retain. Resampling
can be used to determine the optimal number of components.

NIPALS algorithm works well for small/moderate datasets (e.g. $< 2500$ samples
and $< 30$ predictors). Dayal & MacGregor (1997) is the most computationally
efficient for various sizes (e.g. 500-10000 samples, 10-30 predictors,
1-15 responses, 3-10 components).

`PLS` components summarize the data through linear strctures of the original
predictor space that are related to the response. When more intricate 
relationships exist, authoers suggest employing other techniques, rather than
trying to improve `PLS` through augementation.

### Variable Importance in the Projection (VIP)

`k`-component case, for the importance of $j$th predictor:

* numerator: weighted sum of normalized weights corresponding to the $j$th 
predictor
* denominator: total amount of response variation explained by all `k` 
components.

Rule of thumb: VIP > 1.0 is considered to have predictive information for the
response

## Penalized Models

Combating **colinearity** by using **biased** models may result in regression
models where the **overall MSE** is competitive.

**LARS** model can be used to fit lasso models more efficiently, especailly
in high dimensional problems.

Elastic net allows for effective regularization via L2 loss with feature 
selection quality of lasso L1 penalty.

## Code

```{r linear_reg, eval=FALSE}
data(solubility)

set.seed(2)

trainingData <- solTrainXtrans

# visualize data vs target
featurePlot(solTrainXtrans[, -notFingerprints],
            solTrainY,
            between = list(x = 1, y = 1),
            type = c("g", "p", "smooth"),
            labels = rep("", 2))

# fit linear model
trainingData$Solubility <- solTrainY
lmFit <- lm(Solubility ~ ., data=trainingData)

summary(lmFit)
lmPred <- predict(lmFit, solTestXtrans)
lmValues <- data.frame(obs=solTestY, pred=lmPred)

# caret::defaultSummary() to estimate performance
defaultSummary(lmValues)

# robust linear model rlm()
lmFit <- rlm(Solubility ~ ., data=trainingData)

# use 10-fold CV
ctrl <- trainControl(method = 'cv', number=10)
set.seed(100)
lmFit1 <- train(x = solTrainXtrans, y = solTrainY, method='lm', 
                trControl = ctrl)

# find highly correlated predictors and remove
tooHigh <- findCorrelation(cor(solTrainXtrans), .9)
trainXfiltered <- solTrainXtrans[, -tooHigh]
testXfiltered  <-  solTestXtrans[, -tooHigh]

lmTune <- train(x = trainXfiltered, y = solTrainY,
                method = "lm",
                trControl = ctrl)

rlmPCA <- train(x = trainXfiltered, y = solTrainY,
                method = "rlm",
                # preProcess = c('center', 'scale', 'pca'),
                preProcess = 'pca',
                trControl = ctrl)
rlmPCA
```

```{r pls, eval=FALSE}
library(pls)
plsFit <- plsr(Solubility ~ ., data=trainingData)

# can choose number of component for prediction
predict(plsFit, solTestXtrans[1:5,], ncomp=1:2)

loadings(plsFit)
scores(plsFit)

scoreplot(plsFit)
plot(plsFit)

plsTune <- train(x = solTrainXtrans, y = solTrainY,
                 method='pls', 
                 tuneLength = 20,
                 trControl = trainControl(method = 'repeatedcv', 
                                          number = 10),
                 preProcess = c('center', 'scale'))
```

```{r penalized, eval=FALSE}
library(elasticnet)

# Ridge
lm.ridge()

# elastic-net, lambda parameter is ridge weight
# or in glmnet package
elasticnet::enet(x = as.matrix(solTrainXtrans), y = solTrainY, lambda = .001)

```


# Non-Linear Methods

## MARS

Skipped for now.

## SVM

Book focuses on **$\epsilon$-insensitive regression**.

SVMs for regression use a function similar to the Huber function, but with 
difference. 

Given $\epsilon$ give by user, data points with residuals **within** the 
threshold do not contribute to the regression fit, while data points with
an absolute difference **greater** than the threshold contribute a linear-scale
amount.

Consequence: 

1. more robust t outliers, 
2. samples that fit well have **no effect** on the regression equation.

Poorly predicted points define the line. 

SVM is over-parameterized, but the cost value effectively regularizes the model,
helps to alleviate this problem.

Individual training data points are required for new predictions, these are 
known as **support vectors**.

Authors found that the cost parameter provides more flexibility for tuning the
model and therefore suggest fixing a value for $\epsilon$ and tuning over the
other kernel parameters.

Since the predictors enter into the model as sum of cross products, differences
in predictor scales can affect the model. Therefore, data should be 
**centered and scaled** before feeding into svm.

Bayesian analog to SVM: *relevance vector machine* (Tipping 2001). There are 
usually less relevance vectors than support vectors in model.

## KNN

Data should be **centered and scaled** before feeding into KNN. 

KNN has one parameter to tune: the number of neighbours, `k`.

KNN needs to store all training data, therefore is not memory efficient.

KNN can have **poor performance** when local predictor structure is not 
relevant to the response.

## Code

Neural network package `nnet`.

`MARS` found in `earth` package, useful functions:

* `earth()` to fit a model
* `plotmo()` to plot 
* `earth::evimp()` or `caret::varImp()` for variable importance

`KNN` use `knnreg::knn()`

```{r svm, eval=FALSE}

# e1071::svm()
# kernlab package has more extensive implementation of SVM for regression
# use kernlab::ksvm
# kernlab laso has relevance vector machine, rvm()

library(kernlab)
data(solubility)

solTrainXtrans$solTrainY <- solTrainY

# supported kernels: rbfdot, polydot, vanilladot, etc.
svmFit <- ksvm(solTrainY ~ ., data = solTrainXtrans,
               # x = solTrainXtrans, y = solTrainY,
               kernel = 'rbfdot', kpar = 'automatic',
               C = 1, epsilon = .1)

# use train() to tune model. method parameter can take:
# svmRadial, svmLinear, svmPoly to fit different kernels
svmRTuned <- train(solTrainY ~ ., data = solTrainXtrans,
                   method = 'svmRadial',
                   preProc = c('center', 'scale'),
                   tuneLength = 14, 
                   trControl = trainControl(method = 'cv'))

# to choose the selected model:
fm <- svmRTuned$finalModel

# to access the support vectors indices
fm@SVindex

```


# Regression Trees / Rule-based Models

By construction, trees / rule-based models can effective handle many types 
of predictors (sparse, skewded, continuous, categorical, etc.), **without**
the need to pre-process them.

No need to specify the form of the predictor's relationship with the response.

They can handle missing data and implicitly conduct feature selection. 

## Single Tree Weaknesses:

1. model instability, e.g. a small change in data can drastically change the 
structure of the tree.
2. Less than optimal predictive performance, due to rectanglur decision 
boundries, which may not suit the data / true relationship.
3. For a single tree, the number of possible predicted outcome from a tree is
finite and is determined by the number of terminal nodes.
4. Trees suffer from **selection bias**: predictors with a higher number of
distinct values are favoured over more granular predictors. 
5. As the number of missing values increase, the selection of predictors become
more **biased**.

Solution: ensemble methods.

## Trees

Single tree building:
`rpart::rpart()`
`party::ctree()`

### Cost Complexity Pruning

Goal of pruning is the find a right sized tree that has the smallest error
rate. Penalize the error rate using the size of the tree:

$$ SSE_{c_p} = SSE + c_p \times \text{# Terminal Nodes} $$

where $c_p$ is the tuned **complexity parameter**. Large values
may result the tree to have just one or no splits - which means that 
**no predictor** adequately explains enough of the variance in the outcome at
the chosen value of complexity parameter.

Also uses **one-standard deviation** rule for model selection. Alterantively,
use tree associated with the numerically smallest error.

### Missing Values

Use **surrogate splits**. A surrogate split is one whose results are similar to 
the original split actually used in the tree.

### Feature Importance

One way is to measure importance is to keep track of the overal reduction ni 
the optimization criteria for each predictor.

**Disadvantage** of this approach: when some predictors are highly correlated,
the choice of which to use in a split is somewhat random. Which results in
more predictors being selected than those actually needed, affecting variable
importance values. 

See Weakness sector for more disadvantages. 

**Unbiased** regression trees:

* GUIDE
* conditional inference trees

## Regression Model Trees

`RWeka::M5P()` for trees

`RWeka::M5Rules()` for rule based model.

**Drawback** for simple regression trees: each terminal node uses the 
**average** of the training set outcomes in the node for prediction. 

**Implication**: the model may not do a good join in prediction outcomes 
that are extremely high or low.

Alternatively, use a **different estimator** in the terminal node.

**Model tree** vs simple trees:

* split criterion is different
* terminal node predicts the outcome using a linear model
* when a sample is predicted, it is often a combination of the predictions from
different models along the same path through the tree.

Main implementation: **M5** in `Weka` package.

**Split** criterion:

The **expected reduction** in the node's error rate is used.

$$\text{reduction} = SD(S) - \sum_{i=1}^{P}\frac{n_i}{n} \times SD(S_i) $$

Where:

* $S$ is the entire dataset
* $S_1, \cdots, \S_P$ - $P$ subsets of the data after splitting
* $n_i$ number of samples in partion $i$

This metrics determines if the total variation in the splits, weighted by 
sample size, is lower than in the presplit data.

See p185 for more details on tree construction.

## Rule-Based Model

p190

## Bagged Trees

Short for Bootstrap Aggregation. 

Bagging advantages: 

1. Effectively reduces variance of a prediction through its 
aggregation process.
2. They provide their own internal estimate of predictive performance (using
**out of bag** data) that correlates well with either CV or test set estimates. 

Bagging stable, lower variance models like linear regression and MARS offer
**less** improvment in predictive performance.

Tress are great for bagging because they have high variance.

One parameter to tune: number of bootstrap samples to aggregate $m$.

often we see exponential decrease in predictive improvement as the number of
iterations increases. The most improvments in prediction perforamnce is obtained
with a smaller number of trees (m < 10).

If performance is not at an acceptable level after 50 bagging iterations,
authors suggest trying other more powerful ensemble methods as random forest
and boosting.

**Caveats**:

1. high computational cost and memory requirement
2. bagged model is less intrepretable.

## Random Forest

Need to reduce tree correlation. Algo on page 200, 8.2 listing.

Algo randomly selects predictors at each split, decreases tree correlation.

Tuning parameters:

* number of randomly selected predictors, $k$, to choose from at each split, 
referred to as $m_{try}$
  * Breiman 2001 suggests setting $m_{try} = \frac{P}{3}$ where $P$ is the 
  number of predictors.
* number of trees for the forest.
  
Breiman proved that RF is **protected from overfitting**. Authors found that
tuning parameters do not have a drastic effect on performance.

RF is robust to **noisy response**. The independence of learnings can underfit 
data when response is not noisy.

Strobl et al. (2007) proved that the correlations between predictors can have a
significant impact on the importance value. E.g. non-informative predictor
that has high correlation with informative predictor end up having higher or
equal importance as weakly informative predictors.

Also showed that $m_{try}$ tuning parameter has a **series** effect on the 
importance values.

Impact of between-predictor correlations is to **dilute the importance** of key 
predictors. This is **not** addressed by Strobl's 2007 alternative importance
measure.

RF and single tree such as CART may have very different ordering of feature
importance, this highlights that a single tree's greediness prioritize 
predictors differently than a RF.

## Boosting

Several researchers showed that boosting can be interpreted as a forward
stepwise additive model that minimizes exponential loss.

Basic principles, see p205 for algo:

* Given a **loss function** and a **weak** learner, the algo seeks to find an
additive model that minimizes the loss function.
* The model is fit to the residual to minimize the loss
* The current model is added to the previous model, and the procedure continues
for a pre-defined number of iterations.

Requires a **weak** learner, almost any technique with tuning parameters can
be made into a weak learner. Trees are great for reasons below:

1. can be weak learner simply by restricting its depth
2. separate trees can be easily added together
3. trees can be generated quickly.

For **tree** based gradient boosting, two parameters:
1. number of iterations
2. depth of each tree, aka. **interation depth**

Boosting applies a greedy strategy of choosing the optimal weak learner at each
stage. The drawback is that it might not find the optimal global model and
can overfit the training data.

Remedy: regularization, only a fraction of the current predicted value is added
to the previous iteration's predicted value. The learning rate, $\lambda$ is 
between 0 and 1. 

**Stochastic Gradient Boosting** (Friedman) added another step in each 
iteration: randomly sample a fraction of the training data to be used in 
the current training iteration. This fraction can be tuned in CV.

Variable importance for boosting is a function of the reduction in squared
error: the improvment in squared error due to each predictor is summed within 
each tree in the ensemble, then this improvment sum is averaged over the 
ensemble to yield an overall importance value. 

The importance profile for boosting has a much **steeper** slope than the on 
for random forest. This is due to the fact that the trees in boosting are 
dependent on each other and therefore will have correlated structures as the
method follows by the gradient.

## Code

Packages: `rpart`, `party`, `partykit`

### Single trees:

* `rpart::rpart()` - fits w/ CART
* `party::ctree()` - fits w/ conditional inference trees

```{r trees, eval=FALSE}
library(rpart)
library(party)
# Turning for rpart() is through paramter `rpart.control`
# for party::ctree() this is through `ctree_control`

# Or, via train() to tune both complexity parameter and max node depth.
# method = 'rpart' to tune over complexity parameter
# method = 'rpart2' to tune over max node depth.
# method = 'ctree' to tune mincriterion
# method = 'ctree2' to tune over max node depth.
rpartTuned <- train(solTrainY ~ ., data = solTrainXtrans,
                    method='rpart2', tuneLength=10,
                    trControl = trainControl(method = 'cv'))

library(partykit)
# plot tree
rpartTree2 <- as.party(rpartTree)
plot(rpartTree2)
```

### Model Trees

Packages: `RWeka`

`M5` model trees: `RWeka::M5P()` with formula interface. For `train()`,
use parameters:

* `method='M5'`
* `control = Weka_control(M = 10)`

`M5` results can be plotted by calling `plot(model)`.

### Bagging

* `ipred::bagging()` - formula interface
* `ipred::ipredbagg()` - non-formula interface

Other ways:

* `RWeka::Bagging()`
* `caret::bag()` - general framework that works for many models
* `party::cforest()` - Conditional inference trees, with parameter:
`controls = cforest_control(mtry = ncol(trainData) - 1)` to set 
$m_{try} = \text{# of predictors}$.

### Random Forest

Package: `randomForest::randomForest()`

Parameters:

* `mtry` - number of predictors that are randomly sampled as candidates for 
each split. Default is # of predictors / 3.
* `ntree` - # of bootstrap samples. Default 500, but should use at least 1000.
* `importance` - default is FALSE. First set to `TRUE` then call 
`randomForest::importance(..., type=1, scale=F)` to get the right importance 
values.

For `caret::train()`: `method = 'rf'` or `'cforest'`

For `cforest` objects, variable importance can be obtained by calling
`party::varimp()`

`caret::varImp()` has a **unified wrapper** that works for tree-models from:
`rpart`, `classbagg` produced by `ipred`'s bagging, `randomForest`, 
`cforest`, `gbm`, and `cubist`

### Boosted Trees

* `gbm::gbm.fit(x, y, distribution = 'gaussian')` - non-formula interface
* `gbm::gbm(y ~ x, data = data, distribution = 'gaussian')` - formula interface

`distribution` parameter defines the type of loss function that will be 
optimized during boosting. Use `gaussian` for ** continuous response**. 

Other parameters for `gbm`:

* `n.trees` - number of trees
* `interaction.depth` - depth of trees
* `shrinkage`
* `bag.fraction` - proportion of observations to be sampled

For `caret::train()`, see below example:

```{r boost, eval=FALSE}
library(gbm)

gbmGrid <- expand.grid(interaction.depth = seq(1, 7, by = 2),
                       n.trees = seq(100, 1000, by = 50),
                       shrinkage = c(.01, .1),
                       n.minobsinnode=c(5, 10))

gbmTune <- train(solTrainY ~ ., data = solTrainXtrans,
                 method='gbm',
                 tuneGrid = gbmGrid,
                 verbose = F)
```

# Measuring Performance in Classification Models


## Well-Calibrated Probabilities

A few ways to do this:

* Use `caret::calibration()` and `xyplot`, example below
* `PresenceAbsence::calibration.plot`
* `gbm::calibrate.plot`

Predicted probabilities must be calibrated to effectively reflected the true
likelihood of the event of interest.

**Calibration Plot** shows observed probability of an event versus the predicted
class probabilities. Steps:

1. Train a classification model, predict class probabilities
2. Bin the data based on their class probabilities, e.g 0-10%, 10-20%, etc.
3. For each bin, determine the **observed event rate**. Eg. 50 samples have
class probabilities < 10%, but only 1 is the true event, the observed event
rate is $1 / 50 = 2%$.
4. Plot the midpoint of each bin on the x-axis and observed event rate on 
y-axis.

Well calibrated models should see observed event rates fall on a **45-dgree** 
line.

Predicted probabilities can be calibrated to more closely reflect the 
likelihood of the data with post-processing. **Two methods**:

1. Use another model, example in the book used a logistic regression model 
`glm()`. 

$$ \hat{p}^* = \frac{1}{1 + \exp(-\beta_0 + \beta_1 \hat{p})} $$

Where: 

* $\hat{p}$ uncalibrated class probabilities,
* $\beta$ are parameters estimated by predicting the true classes as a function
of the $\hat{p}$.

2. Use Bayes' Rule, example uses `NaiveBayes()`

Note that after calibration, the samples **must be reclassified** to ensure 
consistency between the new probabilities and the predicted classes.

```{r calibrate, eval=F}
library(AppliedPredictiveModeling)

### Simulate some two class data with two predictors
set.seed(975)
training <- quadBoundaryFunc(500)
testing <- quadBoundaryFunc(1000)
testing$class2 <- ifelse(testing$class == "Class1", 1, 0)
testing$ID <- 1:nrow(testing)

### Fit models
library(MASS)
qdaFit <- qda(class ~ X1 + X2, data = training)
library(randomForest)
rfFit <- randomForest(class ~ X1 + X2, data = training, ntree = 2000)

### Predict the test set
testing$qda <- predict(qdaFit, testing)$posterior[,1]
testing$rf <- predict(rfFit, testing, type = "prob")[,1]

qdaTrainPred <- predict(qdaFit, training)
qdaTestPred <- predict(qdaFit, testing)

# QDA predicted classes are stored in $class
head(qdaTrainPred$class)
# QDA predicted class probabilities are stored in $posterior
head(qdaTrainPred$posterior)

# for random forsests, need 2x calls to get classes and probabilities
# class probabilities
rfTestPred <- predict(rfFit, testing, type='prob')
head(rfTestPred)
# class labels
rfTestPredLabels <- predict(rfFit, testing)
head(rfTestPredLabels)

### Generate the calibration analysis
library(caret)
calData1 <- calibration(class ~ qda + rf, data = testing, cuts = 10)

### Plot the curve
xyplot(calData1, auto.key = list(columns = 2))

### To calibrate the data, treat the probabilities as inputs into the
### model

trainProbs <- training
# predict training data Class 1 probabilities
trainProbs$qda <- predict(qdaFit)$posterior[,1]
head(trainProbs$qda)

### These models take the probabilities as inputs and, based on the
### true class, re-calibrate them.
library(klaR)
# usekernel = TRUE allows a flexible function to model the probability
# distribution of the class probabilities.
nbCal <- NaiveBayes(class ~ qda, data = trainProbs, usekernel = TRUE)

### We use relevel() here because glm() models the probability of the
### second factor level.
# relevel() reorders so "Class1" becomes the second level for glm()
lrCal <- glm(relevel(class, "Class2") ~ qda, data = trainProbs, 
             family = binomial)

### Now re-predict the test set using the modified class probability
### estimates
# drop=FALSE here ensures that shape is not changed
testing$qda2 <- predict(nbCal, testing[, "qda", drop = FALSE])$posterior[,1]
testing$qda3 <- predict(lrCal, testing[, "qda", drop = FALSE], 
                        type = "response")


### Manipulate the data a bit for pretty plotting
simulatedProbs <- testing[, c("class", "rf", "qda3")]
names(simulatedProbs) <- c("TrueClass", "RandomForestProb", "QDACalibrated")
simulatedProbs$RandomForestClass <-  predict(rfFit, testing)

calData2 <- calibration(class ~ qda + qda2 + qda3, data = testing)
calData2$data$calibModelVar <- as.character(calData2$data$calibModelVar)
calData2$data$calibModelVar <- ifelse(calData2$data$calibModelVar == "qda", 
                                      "QDA",
                                      calData2$data$calibModelVar)
calData2$data$calibModelVar <- ifelse(calData2$data$calibModelVar == "qda2", 
                                      "Bayesian Calibration",
                                      calData2$data$calibModelVar)

calData2$data$calibModelVar <- ifelse(calData2$data$calibModelVar == "qda3", 
                                      "Sigmoidal Calibration",
                                      calData2$data$calibModelVar)

calData2$data$calibModelVar <- factor(calData2$data$calibModelVar,
                                      levels = c("QDA", 
                                                 "Bayesian Calibration", 
                                                 "Sigmoidal Calibration"))

xyplot(calData2, auto.key = list(columns = 1))
```


### Presenting Class Probabilities

For two classes, **histograms** of the predicted classes for each of the 
true outcomes illustrate the strengths and weaknesses of a model. p252

* x-axis shows predicted probabilities of a class.
* y-axis shows the observed event count or rate, essentially a calibration
plot.
* Use one histogram for each class.

For **multi-classes**, use heat map of class probabilities, p253:

* x-axis shows true class labels
* y-axis shows different samples, so each row shows the class probability for
a sample.

```{r class_hist, eval=F}
library(caret)
data(GermanCredit)

## First, remove near-zero variance predictors then get rid of a few predictors 
## that duplicate values. For example, there are two possible values for the 
## housing variable: "Rent", "Own" and "ForFree". So that we don't have linear
## dependencies, we get rid of one of the levels (e.g. "ForFree")

GermanCredit <- GermanCredit[, -nearZeroVar(GermanCredit)]
GermanCredit$CheckingAccountStatus.lt.0 <- NULL
GermanCredit$SavingsAccountBonds.lt.100 <- NULL
GermanCredit$EmploymentDuration.lt.1 <- NULL
GermanCredit$EmploymentDuration.Unemployed <- NULL
GermanCredit$Personal.Male.Married.Widowed <- NULL
GermanCredit$Property.Unknown <- NULL
GermanCredit$Housing.ForFree <- NULL

## Split the data into training (80%) and test sets (20%)
set.seed(100)
inTrain <- createDataPartition(GermanCredit$Class, p = .8)[[1]]
GermanCreditTrain <- GermanCredit[ inTrain, ]
GermanCreditTest  <- GermanCredit[-inTrain, ]

set.seed(1056)
logisticReg <- train(Class ~ .,
                     data = GermanCreditTrain,
                     method = "glm",
                     trControl = trainControl(method = "repeatedcv", 
                                              repeats = 5))
logisticReg

### Predict the test set
creditResults <- data.frame(obs = GermanCreditTest$Class)
creditResults$prob <- predict(logisticReg, GermanCreditTest, type = "prob")[, "Bad"]
creditResults$pred <- predict(logisticReg, GermanCreditTest)
creditResults$Label <- ifelse(creditResults$obs == "Bad", 
                              "True Outcome: Bad Credit", 
                              "True Outcome: Good Credit")

### Plot the probability of bad credit
histogram(~prob|Label,
          data = creditResults,
          layout = c(2, 1),
          nint = 20,
          xlab = "Probability of Bad Credit",
          type = "count")

### Calculate and plot the calibration curve
creditCalib <- calibration(obs ~ prob, data = creditResults)
xyplot(creditCalib)

### Create the confusion matrix from the test set.
confusionMatrix(data = creditResults$pred, 
                reference = creditResults$obs)
```

### Equivocal Zones

One way to improve classification performance is to create an **equivocal**
or **indeterminate zone** where the class is not formally predicted when the
confidence is not high. 

For $C$-class problem, use $1/C + z$ as threshold.

Model performance should be calculated excluding the samples in the 
indeterminate zone.


## Evaluating Predicted Classes

Code: `caret::confustionMatrix()`

**Confusion matrix**, predicted events as rows, observed events as columns:

* Event: TP, FP
* Non-event: FN, TN

Diagnol cells denote cases of correct predictions. 

Ground truth # of events = TP + FN

**Sum of columns** show total number of ground truth for each class.

**Sum of rows** show total number of predictions for each class.

**Accuracy** = TP / (TP + FN), disadvantages:

* makes no distinction about the type of error being made
* one must consider the natural frequencies of each class.
  * What should be the right benchmark accuracy? E.g. always predict False
  for some rare event can have perfect accuracy.
  * The **no-information** rate is the accuracy rate that can be achieved 
  without a model. Alternative definition is the percentage of the largest
  class in the training set (e.g. always predict to be that class).
  
**Kappa statistic / Cohen's Kappa** $= \frac{O - E}{1 - E}$

* $O$ is the observed accuracy
* $E$ is the expected accuracy based on the marginal totals of the confusion
matrix.
* value is $\in [-1, 1]$, 0 means no agreement between observed and predicted
classes, 1 indicates perfect agreement. Negative values indicate opposite 
direction of the truth but it's rarely happens with classification models.

**Kappa stats** can be extended for multi-class problems. **Weighted Kappa**
can be used to penalize certain predictions.

## Two-Class Problems

Functions to compute these stats can be found in `caret` package:

* `sensitivity()`
* `specificty()`
* `posPredValue()`
* `negPredValue()`

Sensitivity and Specificity are both **conditional** measures, they show
probabilities given a class. In other words, they are **likelihoods**, not
to be confused with **posterior**. 

To got to **unconditional** answers, we need **prevalence**. 

To interpret them: 

* sensitivity shows the % of correct prediction for the **event** class.
* specificity shows the % of correct prediction for the **nonevent** class.

Good reference [here](https://en.wikipedia.org/wiki/Precision_and_recall).

**Sensitivity / True Positive Rate / Recall** $= \frac{TP}{TP + FN}$, 
denominator is the ground truth number of positive events.

**Specificity / True Negative Rate** $= \frac{TN}{TN + FP}$, denominator
is the ground truth number of negative events.

**False Positive Rate (FPR)** $= 1 - Specificity$, measure the % of **wrong**
predictions for the non-event class.

**ROC** curve plots FPR (x) vs Sensitivity (y). 
It show that we wants **high** % of **correct** prediction for 
the **event** class, and **low** % of **wrong** prediction for the **non-event** 
class, i.e. high sensitivity and low FPR.

This compares to:

**Precision** $= \frac{TP}{TP + FP}$, a.k.a **Positive Predictive Value (PPV)**

**Negative Predictive Value (NPV)** $= \frac{TN}{TN + FN}$

PPV and NPV are **unconditional** measures, they embed prevalence info. p258

Authors indicated that PPV and NPV are **not** often used to characterize 
the model. ROC curve actually does not use them. The reasons sited:

1. Typically prevalence is hard to quantify
2. Prevalence is also dynamic, i.e. it changes, or can be different in 
different places or situations.

**Youden's $J$ Index** $= Sensitivity + Specificity - 1$ measures the 
proportion of correctly predicted samples for both the event and nonevent
groups.

## Non-Accuracy-Based Criteria

Models should be fit for purpose. Need to consider the impact of the different
types of mis-classifications. 

It's important to take into account of **prevalence** of classes. 

Holte (2000) provides an outline for measuring costs with performance metrics
for two-class problems. The define **probability-cost function (PCF)**, 
which measures the proportion of the total costs assicated with false-positive
samples.

TODO: emailed Max Kuhn about potential error for the denominator.

$$ PCF = \frac{P \times C(+|-)}{P \times C(-|+) + (1 - P) \times C(+|-)} $$
Where:

* $P$ is the prior probability of the event
* $C(-|+)$ is the cost of incorrectly predicting an event (+) as a nonevent (-),
i.e. False Negative (FN)
* $C(+|-)$ is the cost of predicting a nonevent as event, 
i.e. False Positive (FP)

They suggest using **normalized expected cost (NEC)** which takes into account
the **prevalence** of the event, to measure model performance. $NCE \in [-, 1]$

$$ NEC = PCF \times (1 - TF) + (1 - PCF) \times TP $$

Note that NEC only handles **two types of errors** and may not be appropriate
if there are other costs or benefits.

## Receiver Operating Charastics (ROC) Curve

ROC plots False Positive Rate on x-ais and Sensitivity on y-axis.

**Advantage**: **insensitive** to disparities in the class proportions.

**Disadvantage**: obscure as a measure of evaluating models. Common that no
individual ROC curve is uniformly better than other models' ROC curves.

  * There is loss of info in summarizing these curves, especically if one
  particular area of the curve is of interest.
  * **Partial Area Under the ROC Curve (McClish 1989) is an alternative that
  focus on specific parts of the curve.
  
ROC multi-class extensions: 

* Hand and Till 2001
* Lachiche and Flach 2003

```{r roc, eval=F}
# uses results from class probability histogram section

### ROC curves:

### Like glm(), roc() treats the last level of the factor as the event
### of interest so we use relevel() to change the observed class data

library(pROC)
creditROC <- roc(relevel(creditResults$obs, "Good"), creditResults$prob)

coords(creditROC, "all")[,1:3]

auc(creditROC)
ci.auc(creditROC)

### Note the x-axis is reversed
plot(creditROC)

### Old-school:
plot(creditROC, legacy.axes = TRUE)
```

## Lift Charts

`caret::lift()`

For two-class problems (event vs non-event), rank samples by their **scores** 
(using the event class probabilities) and determine the cumulative event rate 
as more samples are evaludated.

When model is **non-informative**, the highest-ranked X% of the data would
contain on average X events. 

The **lift** is the number of samples detected by a model above a completely
random selection of samples.

Lift chart plots the cumulative gain / lift against the cumulative % of samples
that have been screened.

Steps:

1. Predict validation or test set samples
2. determine the **baseline event rate**, i.e. the % of true events in the 
entire data set.
3. Order the data by the predicted probability of the event of interest.
4. For each unique class probability value, calculate the percent of true events
in all samples below the probability value
5. Divide the % of true events for each probability threshold by the baseline
event rate.

```{r lift_chart, eval=F}
### Lift charts

creditLift <- caret::lift(obs ~ prob, data = creditResults)
xyplot(creditLift)
```


# Discriminant Analysis / Other Linear Classifiers

```{r lda_data, eval=F}
library(caret)
library(doMC)
registerDoMC(8)
library(plyr)
library(reshape2)

load("~/tmp/apm/grantData.RData")

pre2008Data <- training[pre2008,]
year2008Data <- rbind(training[-pre2008,], testing)

set.seed(345)
test2008 <- createDataPartition(year2008Data$Class, p = .25)[[1]]

allData <- rbind(pre2008Data, year2008Data[-test2008,])
holdout2008 <- year2008Data[test2008,]
```

## Logistic Regression

When model residuals folloer a normal distribution, minimizing the sum of 
squared residuals also produces **maximum likelihood estimates**.

Given probability of a class $p$ in a binary classification task, because 
$0 <= p <= 1$, to formulate a linear model that we need to model 
**odds**, $\frac{p}{1-p}$. Hence logistic regression is defined by:

$$ \log\bigg( \frac{p}{1-p} \bigg) = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $$

Where $p$ is the number of predictors.

Logistic regression produces **linear** boundaries, unless the predictors used
in the model are non-linear versions of the data (e.g. squared features).

`rms` package has many relevant functions. See Harrell 2001 for details.

```{r logit, eval=F}
modelFit <- glm(Class ~ Day, 
                data=training[pre2008, ],
                # binominal for logistic regression
                family = binomial)

# glm() treats the second factor level as the event of interest, 
# so to get class 1 probabilities, use 1 - results
successProb <- 1 - predict(modelFit, type='response')

# alternative is to use rms::lrm()
library(rms)
# rcs() - restricted cublic spline for fitting flexible nonlinear functions
# of a predictor
rcsFit <- lrm(Class ~ rcs(Day), data=training[pre2008, ])
rcsFit

dayProfile <- rms::Predict(rcsFit, 
                           # predict first 365 samples
                           Day=0:365, 
                           # flip the prediction to get the model for 
                           # successful grants
                           fun=function(x) -x)

# use train() for glm
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     index = list(TestSet = 1:nrow(pre2008Data)),
                     savePredictions = T)

lrFull <- train(training[, fullSet], y=training$Class,
                method='glm', 
                metric='ROC',
                trControl = ctrl)

# fiting with reduced set has higher AUC
lrReduced <- train(training[, reducedSet], y=training$Class,
                method='glm', 
                metric='ROC',
                trControl = ctrl)
lrReduced

# hold out set predictions are saved in $pred
lrReduced$pred

# show confusion matrix for hold out set, norm = is form normalization
confusionMatrix(lrReduced, norm = 'none')

# AUC
lrRoc <- roc(response = lrReduced$pred$obs,
             predictor = lrReduced$pred$successful,
             levels = rev(levels(lrReduced$pred$obs)))
auc(lrRoc)
plot(lrRoc)
# use 1 - specificity for x axis:
plot(lrRoc, legacy.axes=T)

# variable importance
lrImp <- varImp(lrReduced, scale = F)
# plot(lrImp)
```


## Linear Discriminant Analysis (LDA)

LDA finds an optimal discriminant vector.

When number of samples $N$ approach number of predictors $p$, 
be vary **careful**! As $p$ grows, the predicted probabilities become closer
to 0 and 1.

If $N == p$ then we can find at least one vector the perfectly separates the 
samples. 

Authors recommend using LDA only when $N$ is **at least 5-10 times more** than 
$p$. Apply Causian when the ratio is less than 5.

Fisher (1936) and Welch (1939) both discovered it from different angles.

Welch took the approach of minimizing the total probability of 
mis-classification. Imposes assumption of Gaussian data with idential 
covariance.

Non-identical covariance leads to **Quadratic discriminant analysis**.

Fisher tried to find a linear combination of the predictors such that 
**the between group variance was maximized, relative to the within group variance.**

For multi-class tasks, Fisher's model would require $C*p + p(p + 1) / 2$
parameters, where:

* $C$ is the number of classes and 
* $p$ is the number of predictors.

**Conditions** for when LDA works:

* more samples than predictors
* covariance matrix of data is invertible
* data can be decently separated by linear hyperplane

**Transformation of data** and **cross-product** (a.k.a. interaction) can be 
used to produce **non-linear** discriminant boundaries. But they should only 
be used when there is a strong reason, otherwise the covariance matrix may 
not be invertible. 

Authors recommends using **PLSDA** if there could be non-linear relationships 
but don't know which predictors are involved.

```{r lda, eval=F}
# fit LDA model
set.seed(476)
ldaFit <- train(x = training[,reducedSet], 
                y = training$Class,
                method = "lda",
                preProc = c("center","scale"),
                metric = "ROC",
                trControl = ctrl)
ldaFit

confusionMatrix(ldaFit, norm='none')

ldaRoc <- roc(response=ldaFit$pred$obs,
              predictor = ldaFit$pred$successful,
              level = rev(levels(ldaFit$pred$obs)))
auc(ldaRoc)
plot(lrRoc, type='s', col=rgb(.2, .2, .2, .2))
plot(ldaRoc, add=T, type='s')
```


## Partial Least Square Discriminant Analysis

To fine $N < p$ and collinearity, PLSDA is used. 

Responses / target labels need to be converted to **one-hot** for multi-class
prediction. Therefore, cannot use PLS regression.

PLS for multivariate classification has strong mathematical connections to 
**canonical correlation analysis** and LDA.

PLS directions in this context were eith eigenvectors of a slightly perturbed
between-groups covariance matrix. PLS is therefore seeking to find optimal 
group separation while being guided by between-groups information. 

Liu & Rayens (2007) showed that if dimension reduction is **not** necessary, 
then LSD will always provide a lower misclassification error rate than PLS. 

One tuning parameter: **the number of latent variables to be retained**.

PLS can be affected when including predictors that contain little or no
predictive information. See section 19.1.

When PLS performs worse than LDA using a large set of predictors, then next
step would be to examine PLS performance using the reduced set of predictors.

Final output from PLS is converted to class probabilities with softmax.
Alternative approach is to use Bayes' rule to convert the original model output
into class probabilities. This tends to yield more meaningful class 
probabilities, **advantage** is that prior probabilities can be specified.

Feature importance values can be computed.

```{r plsda, eval=F}
set.seed(476)
plsFit <- train(x = training[,fullSet], 
                y = training$Class,
                method = "pls",
                tuneGrid = expand.grid(ncomp = 1:10),
                preProc = c("center","scale"),
                metric = "ROC",
                probMethod = "Bayes",
                trControl = ctrl)
plsFit

plsImpGrant <- varImp(plsFit, scale=F)
plot(plsImpGrant, top=20)

## Only keep the final tuning parameter data
plsFit$pred <- merge(plsFit$pred,  plsFit$bestTune)

plsRoc <- roc(response = plsFit$pred$obs,
              predictor = plsFit$pred$successful,
              levels = rev(levels(plsFit$pred$obs)))
auc(plsRoc)

bestPlsNcomp <- plsFit$results[best(plsFit$results, "ROC", maximize = TRUE), "ncomp"]
bestPlsROC <- plsFit$results[best(plsFit$results, "ROC", maximize = TRUE), "ROC"]

plsCM <- confusionMatrix(plsFit, norm = 'none')

# now fit reduced set
plsFit2 <- train(x = training[,reducedSet], 
                 y = training$Class,
                 method = "pls",
                 tuneGrid = expand.grid(ncomp = 1:10),
                 preProc = c("center","scale"),
                 metric = "ROC",
                 probMethod = "Bayes",
                 trControl = ctrl)
plsFit2

bestPlsNcomp2 <- plsFit2$results[best(plsFit2$results, "ROC", maximize = TRUE), "ncomp"]
bestPlsROC2 <- plsFit2$results[best(plsFit2$results, "ROC", maximize = TRUE), "ROC"]

plsFit2$pred <- merge(plsFit2$pred,  plsFit2$bestTune)

plsRoc2 <- roc(response = plsFit2$pred$obs,
               predictor = plsFit2$pred$successful,
               levels = rev(levels(plsFit2$pred$obs)))
auc(plsRoc2)

plsCM2 <- confusionMatrix(plsFit2, norm = "none")
plsCM2

pls.ROC <- cbind(plsFit$results,Descriptors="Full Set")
pls2.ROC <- cbind(plsFit2$results,Descriptors="Reduced Set")

plsCompareROC <- data.frame(rbind(pls.ROC,pls2.ROC))

xyplot(ROC ~ ncomp,
       data = plsCompareROC,
       xlab = "# Components",
       ylab = "ROC (2008 Hold-Out Data)",
       auto.key = list(columns = 2),
       groups = Descriptors,
       type = c("o", "g"))

## Plot ROC curves and variable importance scores
plot(ldaRoc, type = "s", col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)
plot(lrRoc, type = "s", col = rgb(.2, .2, .2, .2), add = TRUE, legacy.axes = TRUE)
plot(plsRoc2, type = "s", add = TRUE, legacy.axes = TRUE)

plot(plsImpGrant, top=20, scales = list(y = list(cex = .95)))
```

## Penalized Models

For logistic regression, `L2` penalty can be added to maximizing log likelihood:

$$ \underset{\beta}{argmax} \log L(p) - \lambda \sum_{j=1}^P \beta_j^2 $$

Eilers et al. 2012 and Park and Hastie 2008 discussed this model in the context
of large number of predictors and small training samples. The penalty term can 
stabilize the logistic regression model coefficients in these cases.

`glmnet` package uses elastic net style penalty. p303 for equation.

```{r glm_reg, eval=F}
glmnGrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1),
                        lambda = seq(.01, .2, length = 40))
set.seed(476)
glmnFit <- train(x = training[,fullSet], 
                 y = training$Class,
                 method = "glmnet",
                 tuneGrid = glmnGrid,
                 preProc = c("center", "scale"),
                 metric = "ROC",
                 trControl = ctrl)
glmnFit

glmnet2008 <- merge(glmnFit$pred,  glmnFit$bestTune)
glmnetCM <- confusionMatrix(glmnFit, norm = "none")
glmnetCM

glmnetRoc <- roc(response = glmnet2008$obs,
                 predictor = glmnet2008$successful,
                 levels = rev(levels(glmnet2008$obs)))
auc(glmnetRoc)

glmnFit0 <- glmnFit
glmnFit0$results$lambda <- format(round(glmnFit0$results$lambda, 3))

glmnPlot <- plot(glmnFit0,
                 plotType = "level",
                 cuts = 15,
                 scales = list(x = list(rot = 90, cex = .65)))

update(glmnPlot,
       ylab = "Mixing Percentage\nRidge <---------> Lasso",
       sub = "",
       main = "Area Under the ROC Curve",
       xlab = "Amount of Regularization")

plot(plsRoc2, type = "s", col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)
plot(ldaRoc, type = "s", add = TRUE, col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)
plot(lrRoc, type = "s", col = rgb(.2, .2, .2, .2), add = TRUE, legacy.axes = TRUE)
plot(glmnetRoc, type = "s", add = TRUE, legacy.axes = TRUE)

```

## Code

Package list: `glmnet`, `pamr`, `pls`, `pROC`, `rms`, `sparseLDA`, `subselect`

`grantData.RData` can be created using a script that comes with this book's
package.

`caret::twoClassSummary()` computes ROC for two-class tasks.

`caret::train()` notes:

  * By default, accuracy and Kappa statistic are used to evaluate classification
  models.
  * Can use ROC as a evaluation metric by using `metric = 'ROC'`. 
  * By default `train` only generates class predictions. use `classProbs=T` 
  if probabilities are needed.
  
```{r sparseLDA, eval=F}
## Sparse logistic regression
## Package: sparseLDA, PenalizedLDA
## main function: sparseLDA::sda, tuning parameter lambda

set.seed(476)
spLDAFit <- train(x = training[,fullSet], 
                  y = training$Class,
                  "sparseLDA",
                  tuneGrid = expand.grid(lambda = c(.1),
                                         NumVars = c(1:20, 50, 75, 100, 250, 500, 750, 1000)),
                  preProc = c("center", "scale"),
                  metric = "ROC",
                  trControl = ctrl)
spLDAFit

spLDA2008 <- merge(spLDAFit$pred,  spLDAFit$bestTune)
spLDACM <- confusionMatrix(spLDAFit, norm = "none")
spLDACM

spLDARoc <- roc(response = spLDA2008$obs,
                predictor = spLDA2008$successful,
                levels = rev(levels(spLDA2008$obs)))
auc(spLDARoc)

update(plot(spLDAFit, scales = list(x = list(log = 10))),
       ylab = "ROC AUC (2008 Hold-Out Data)")

plot(plsRoc2, type = "s", col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)
plot(glmnetRoc, type = "s", add = TRUE, col = rgb(.2, .2, .2, .2), 
     legacy.axes = TRUE)
plot(ldaRoc, type = "s", add = TRUE, col = rgb(.2, .2, .2, .2), 
     legacy.axes = TRUE)
plot(lrRoc, type = "s", col = rgb(.2, .2, .2, .2), add = TRUE, 
     legacy.axes = TRUE)
plot(spLDARoc, type = "s", add = TRUE, legacy.axes = TRUE)
```

```{r svm_class, eval=F}
# LGOCV = Leave Group Out CV
ctrl <- trainControl(method='LGOCV',
                     summaryFunction = twoClassSummary, 
                     classProbs = TRUE,
                     index = list(TrainSet = pre2008),
                     savePredictions = T)

svmFit00 <- train(allData[,fullSet], allData$Class,
                  method = "svmRadial",
                  tuneGrid = svmrGrid,
                  preProc = c("center", "scale"),
                  metric = "ROC",
                  trControl = ctrl00)
```

# Nonlinear Classification Models

With a few exceptions such as FDA, the techniques described in this chapter
can be adversely affected when a large number of predictors are 
non-informative.

## Nonlinear Discriminant Analysis

### Quadratic and Regularized Discriminant Analysis

* `MASS::qda()` for QDA
* `rrcov::QdaCov()` - outlier resisant version for QDA
* `klaR::rda()` for RDA

QDA relaxes the LDA covariance assumption so that a class-specific covariance
structure can be accommodated.

Implication: 

* decision boundaries now becomes quadratically curvilinear in 
the predictor space.
* data requirement is more stringent: class-specific covariance matrix
need to be invertible, this means that the number of predictors must be less
than the **number of samples within each class**.
* predictors within each class must not have pathological levels of 
**collinearity**.
* if majority of the predictors in the data are categorical, QDA will only
be able to model these as linear functions, limiting its effectiveness.

In practice, data may be separated by structures somewhere between 
**linear and quadratic** class boundaries. Hence the use of 
**regularized** discriminant analysis, which is similar to elastic net and
use both LDA and QDA with a **tuning parameter** $\lambda$. E.g. 

$$ \hat{\Sigma}_l(\lambda) = \lambda \Sigma_l + (1 - \lambda) \Sigma $$

Where:

* $\Sigma_l$ is the covariance matrix of the $l$th class
* $\Sigma$ is the pooled covariance matrix across all classes

### Mixture Discriminant Analysis

`mda::mda()`

MDA allows each class to be represented by multiple multivariate normal 
distributions. 

These distributions may have different means, but like LDA, the covariance 
structure are **assumed to be the same**.

### Flexible Discriminant Analysis

Hastie (1994) described a process where for $C$ classes, a set of $C$ linear
regression models can be fit to binary class indicators and showed that
the regression coefficients from these models can be post-processed to derive
the discriminant coefficients. This forms the concept of FDA.

Bagging FDA - authors stated that based on their experience, bagging FDA or
MARS has a marignal impact on model performance, and increased number of
terms diminishes the interpretation of the discriminant equation.

`mda::fda()` for FDA, accepts formula interface, parameter `method` specifies
the exact method for estimating the regression parameters.

`earth` package fits MARS with a **wider range of options**, and can be used 
together with `fda(..., method = earth)`. Other parameters for `earth()` can
be passed to `fda()` which will pass them to `earth()`.

`caret::train()` can fit FDA models with `method = 'fda'`. `caret:varImp()` 
also works.

### SVM

Vapnik 1995 extended SVM to incorporate a cost parameter on the sum of the 
training set points that are on the boundary or on the wrong side of the 
boundary.

Large cost values increase model complexity rather than restrain it. 
When cost is **low**, model may **underfit** the data. When cost is 
**too large**, the model may **overfit** the data.

Platt (2000) described a way to post-process SVM output to estimate class
probabilities.

Alternatives are:

* Least square SVM
* Relevance vector machines
* Import vector machines

**Graph kernel** can directly relate the content of graph representation 
to the model without deriving descripor variables.

**String kernel** can use entire text of a document directly and has more 
potential to find important relationships than the bag-of-words approach.

SVM can be **adversely affected** by non-informative predictors.

Most comprehensive implementation in `kernlab` package.

* `ksvm()` takes `prob.model = TRUE` to estimate class probabilities with
sigmoid function (Platt 2000). 
* `class.weights` parameter assigns asymmetric cost to each class. Syntax is 
to use **named vectors** of weights or costs. e.g. 
`class.weights = c(successful = 1, unsuccessful = 5)`
* class probabilities are not affected by `class.weights`
* use `predict(model, newdata = ..., type = 'prob')` to predict class 
probabilities.


### KNN

`caret::train(..., method='knn', ...)`

New sample's predicted class is the class with the highest probability estimate
of its neighbours.

Prone to **overfitting** when $K$ is small.

Book example revealed some numerical instability of KNN: as the number of 
neighbours increases, the probability of ties also increases.

### Naive Bayes

Assumes that all of the predictors are **independent** of the others. Makes
computation easier..

Let $Y$ be the class variable and $X$ be the collections of predictor variables.

Conditional probability with this assumption becomes:

$$ Pr[X \mid Y = C_l] = \Omega_{j=1}^{P} Pr[X_j \mid Y = C_l] $$

Probabilty density for each predictor is estimated from the data. When a new
sample comes in, Bayes' rule is applied to compute $P[Y=C_l \mid X]$.

Class prediction if the class with highest probability.

If no **prior** is given, the convention is to use the observed proportions 
from the training set to estimate the prior.

Problem occurs for small sample set when one of the category has zero occurance.
Because conditional probabilities are multiplied together, a zero from one
predictor would coerce the posterior probability to zero.

One method to avoid this issue is to use a **Laplace correction** or 
**Laplace smoothing**, where the same correction factor, usually between 1 and
2, is added to the numerator.

For the denominator, the frequencies are increased by the correction factor
times the number of values of the predictor.

Ironically, class probabilities created by applying Bayes' rule in the normal
fashion tend not to be well-calibrated. As the number of predictors increases,
the posterior probabilities will become more extreme.

See Kevin Murphy Machine Learning book p84 for a more detailed discussion on 
Naive Bayes algorithm. The model had $C \times p$ parameters (not many compare
to others), therefore is **relatively immune to overfitting**.

Continuous predictors are modeled with Gaussian distribution.

Binary predictors are modeled with Bernoulli distribution.

* `e1071::naiveBayes()`
* `klaR::NaiveBayes()`

Both methods offer Laplace corrections (parameter `fL = ` for `klaR`), 
but the version in `klaR` has the option of using conditional density estimate 
that are more flexible.

However, both methods treat binary variables as continuous. Therefore, 
binary features need to be converted to **factors** with `factor()`

```{r naivebayes, eval=F}
### Section 13.6 Naive Bayes

## Create factor versions of some of the predictors so that they are treated
## as categories and not dummy variables

factors <- c("SponsorCode", "ContractValueBand", "Month", "Weekday")
nbPredictors <- factorPredictors[factorPredictors %in% reducedSet]
nbPredictors <- c(nbPredictors, factors)
nbPredictors <- nbPredictors[nbPredictors != "SponsorUnk"]

nbTraining <- training[, c("Class", nbPredictors)]
nbTesting <- testing[, c("Class", nbPredictors)]

for(i in nbPredictors)
{
  if(length(unique(training[,i])) <= 15)
  {
    nbTraining[, i] <- factor(nbTraining[,i], 
                              levels = paste(sort(unique(training[,i]))))
    nbTesting[, i] <- factor(nbTesting[,i], 
                             levels = paste(sort(unique(training[,i]))))
  }
}

set.seed(476)
nBayesFit <- train(x = nbTraining[,nbPredictors],
                   y = nbTraining$Class,
                   method = "nb",
                   metric = "ROC",
                   tuneGrid = data.frame(usekernel = c(TRUE, FALSE), fL = 2),
                   trControl = ctrl)
nBayesFit

nBayesFit$pred <- merge(nBayesFit$pred,  nBayesFit$bestTune)
nBayesCM <- confusionMatrix(nBayesFit, norm = "none")
nBayesCM
nBayesRoc <- roc(response = nBayesFit$pred$obs,
                 predictor = nBayesFit$pred$successful,
                 levels = rev(levels(nBayesFit$pred$obs)))
nBayesRoc
```

# Classification Trees

