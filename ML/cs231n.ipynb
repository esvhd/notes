{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford cs231n Notes\n",
    "\n",
    "Winter 2016 series by Li Fei-Fei, Andrej Karpathy and Justin Johnson\n",
    "\n",
    "[videos](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)\n",
    "\n",
    "[material](http://cs231n.stanford.edu/2016/syllabus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4 Backprop\n",
    "\n",
    "In a backprop computational graph:\n",
    "\n",
    "**Add** gate: gradient distributor\n",
    "\n",
    "**Max** gate: gradient router, gradient goes to the larger input, smaller input has zero gradient.\n",
    "\n",
    "**Multiply** gate: gradient \"switcher\"?\n",
    "\n",
    "If a node's output is used by multiple consumer nodes then in backprop, gradients coming from the consumer nodes should be **added**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5\n",
    "\n",
    "Good example in video that implemented a check after initalizing a network, feed data through the network without regularization and check the loss, on CIFAR-10 the expected loss is $-log(1/10) \\approx 2.3$.\n",
    "\n",
    "**Tip**: make sure that you can **overfit** very small portion of the training data. If you can't overfit a small portion of the data, then things are broken.\n",
    "\n",
    "### Hyperparameter Search\n",
    "\n",
    "Run coarse search for 5 epochs, then run finer searches. \n",
    "\n",
    "Example showed a good result near the boundaries of the learning rate search space. Thsis may be **problematic**, because it may indicate that the optimal parameter may be **outside** the search space.\n",
    "\n",
    "**Track the ratio of weight updates / weight magnitudes**. This should be `-1e-3`. If too high, maybe decrease learning rate, vice verse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 6\n",
    "\n",
    "### Second Order Optimization\n",
    "\n",
    "Uses **Hessian** matrix, this will have faster convergence, and **no hyperparameter**!\n",
    "\n",
    "However, when training large networks, the Hessian matrix ends up being quite huge, inversing it becomes impractically expensive to evaluate. \n",
    "\n",
    "**L-BFGS** is usually used for second order optimization, however, it works for **deterministic** functions. This means it **does not work in minibatch settings**. \n",
    "\n",
    "Adam is usually a good choice, if you can afford to do full batch updates, then try L-BFGS. \n",
    "\n",
    "### Ensemble Trick/Tips\n",
    "\n",
    "Can get a small boost from averaging multiple model checkpoints of a single model.\n",
    "\n",
    "Keep track of a **running average of parameter vector**, to use at test time. \n",
    "```\n",
    "loss = nn.forward()\n",
    "dx = nn.backward()\n",
    "x += - learning_rate * dx\n",
    "x_test = 0.995 * x_test + 0.005 * x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 7 CNN\n",
    "\n",
    "Input images are `width * height * depth`, e.g. 64 * 64 * 3, for 64x64 images with RGB colours.\n",
    "\n",
    "**Filters**, aka kernels, are used to slide through the images, filter size can be 3x3x3, depth is **always** the same as input images. Filters are dot product operators.\n",
    "\n",
    "**Stride** is the step size to slide the filter. \n",
    "\n",
    "**Dimension**: NxN images, FxF filter size, **output size** is given by: \n",
    "\n",
    "$$(N-F) / stride + 1$$\n",
    "\n",
    "**Multiple** filters can be used in a layer to generate multiple **activation maps**, e.g. 6 filters would give 6 maps in a layer, which then are fed to ReLU for example.\n",
    "\n",
    "3D Convolution is just 2D convolution applied at the same time to the 3rd dimension. \n",
    "\n",
    "\n",
    "### Padding\n",
    "\n",
    "Padding of **zeros** are added to the broders of the input images to **preserve size spatially** for the activation maps after convolution operations. \n",
    "\n",
    "If filter size is FxF, then zero-padding size = $(F-1)/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**, image input 32x32x3, 10 filters with size 5x5x3 and stride 1, padding 2, then after convolution size is 32x32x10, e.g. (32 + 2 * 2 - 5)/1 + 1 = 32.\n",
    "\n",
    "Number of parameters = 5 * 5 * 3 * 10 + 10 = 760, 10 * (filter size + bias). \n",
    "\n",
    "### Dimension Summary\n",
    "\n",
    "For a Conv Layer, \n",
    "\n",
    "Input volume size $W_1 \\times H_1 \\times D_1$\n",
    "\n",
    "Requires 4 hyperparameters:\n",
    "\n",
    "* Number of filters, $K$\n",
    "* filter spaitial extend, $F$,\n",
    "* stride, $S$\n",
    "* the amoutn of zero padding, $P$\n",
    "    \n",
    "Output volume size $W_2 \\times H_2 \\times D_2$, where:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_2 &= (W_1 - F + 2P) / S + 1 \\\\\n",
    "H_2 &= (H_1 - F + 2P) / S + 1 \\\\\n",
    "D_2 &= K\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Wither **parameter sharing**, this introduces $F \\times F \\times D_1$ weights per filter, total parameters size $(F \\times F \\times D_1) \\times K + K$.\n",
    "\n",
    "$K$ is usually chosen as powers of 2.\n",
    "\n",
    "\n",
    "\n",
    "In each activation map, neurons share **weights** (from filters) and **local connectivity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "**Max pooling**: use a filter, e.g. 2x2, then take the max of each 2x2 area. **Average pooling**, performs as well as max pooling. \n",
    "\n",
    "Input volume size: $W_1 \\times H_1 \\times D_1$\n",
    "\n",
    "Requires 2 hyperparameters:\n",
    "\n",
    "* their spatial extend, $F$\n",
    "* stride, $S$\n",
    "\n",
    "Output volume size: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_2 &= (W_1 - F) / S + 1 \\\\\n",
    "H_2 &= (H_1 - F) / S + 1 \\\\\n",
    "D_2 &= D_1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Not common to use zero-padding for pooling layers.\n",
    "\n",
    "Pooling **shrinks** the input volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet Example\n",
    "\n",
    "Input: 227x227x3 images, (paper says 224, confused everyone...)\n",
    "\n",
    "First conv layer (CONV1): 96 11x11x3 filters applied at stride 4, output volume size: 55x55x96, (227 - 11)/4 + 1 = 55. \n",
    "\n",
    "Total number of parmas = 11 * 11 * 3 * 96 + 96 = 35k\n",
    "\n",
    "After pooling layer, 3x3x3 filter applied at stride 2, output size 27x27x96, e.g. (55 - 3) / 2 + 1, 0 params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Example\n",
    "\n",
    "Total memory ~93MB / image for forward pass, ~2x for backward pass.\n",
    "\n",
    "Total Params 138mm.\n",
    "\n",
    "### ResNet\n",
    "\n",
    "A lot deeper, 152 layers, ~5mm parameters. Relying on skip connections, each layer is trained to be a **delta** that is added to the original input. This allows gradients to feed back to the first layer easily. \n",
    "\n",
    "Very repaid spatial reduction, but relying on many layers.\n",
    "\n",
    "8 GPUs trained for 2 weeks....\n",
    "\n",
    "As of 2017, Inception-V4 (ResNet + Inception) has the best top-1 accuracy of ~80%. VGG has the highest memory and compute usage. See this [video](https://www.youtube.com/watch?v=DAOcjicFr1Y&index=9&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)\n",
    "\n",
    "ResNeXT was published later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet\n",
    "\n",
    "Dense blocks where each layer is connected to every other layer in feedforward fashion. \n",
    "\n",
    "Alleviates vanishing gradient, strengthens feature propagation, encourages feature reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNN / LSTM Lecture 10\n",
    "\n",
    "[char-rnn code](https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "\n",
    "[2017 video](https://www.youtube.com/watch?v=6niqTuYFZLQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initial hidden state is usually initialized to zero.\n",
    "\n",
    "Because of the recurrence, the same weight matrix $W$ is used many times, in a computational graph, this is essentially reusing the same node many times. In the backprop, this means you end up summing the gradients for $dW$.\n",
    "\n",
    "Input: $X = (x_1, x_2, \\cdots, x_n)$\n",
    "Hidden units: $H = (h_1, h_2, \\cdots, h_n)$\n",
    "\n",
    "Each input and hidden unit pair is passed to an activation function $f_W()$, whose output is the next hidden unit. Generally:\n",
    "\n",
    "$$ f_W(x_{t}, h_{t-1}) = h_{t} $$\n",
    "\n",
    "Loss: $L = \\sum_{t=1}^{n} L_t$, where $L_t = y_t - h_t, \\forall t$ \n",
    "\n",
    "**Sequence to Sequence** netowrk is a a) Many-to-One RNN + b) One-to-Many RNN, where the last hidden state from a) is the input sequence for b).\n",
    "\n",
    "**Truncated** Backprop Through Time: break the dataset into pieces and do forward/backward passes for each piece separate. This is quite similiar to minibatch SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "Gates: $i, f, o, g$, cell states $c_t$, hidden state $h_t$\n",
    "\n",
    "RNN models typically not deep, 2-4 layers normally.\n",
    "\n",
    "Vanilla RNNs suffer from exploding / vanishing gradient problem. Backpropagation through multiplication gates results in multiplying $W^T$ for computing gradients of $h_t$. This means the gradient of $h_0$ involves many factors of $W^T$. \n",
    "\n",
    "Whether it explodes or vanishes depends on the largest eigenvalue of the hidden state weight matrix $W$. If larger than 1 it explodes, vanishes if less than 1.\n",
    "\n",
    "Hack for this: **gradient clipping**.\n",
    "\n",
    "Forget gates can **turn off** gradient flow. At start we typically initialize forget gates to positive numbers so **not** to turn off gradient, the network then learns how to forget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension of weights\n",
    "\n",
    "$x_t$, $h_{t-1}$ are vectors of length $h$, stack them together to form a matrix of $2h \\times 1$.\n",
    "\n",
    "weights are combined into $W$ for $i,f,o,g$, shape $4h \\times 2h$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DL Libraries\n",
    "\n",
    "Lots of examples in [slides](http://cs231n.stanford.edu/slides/2016/winter1516_lecture12.pdf).\n",
    "\n",
    "Use cases:\n",
    "\n",
    "Feature extraction / finetuning existing models: Use Caffe\n",
    "\n",
    "Complex uses of pre-trained models: Use Lasagne or Torch\n",
    "\n",
    "Write you own layers: Use Torch\n",
    "\n",
    "Crazy RNN: use Theano or Tensorflow\n",
    "\n",
    "Huge models, need model parallelism: use Tensorflow\n",
    "\n",
    "Visualization during training: Tensorflow w/ TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
