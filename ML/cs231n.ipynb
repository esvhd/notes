{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford cs231n Notes\n",
    "\n",
    "Winter 2016 series by Li Fei-Fei, Andrej Karpathy and Justin Johnson\n",
    "\n",
    "[videos](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)\n",
    "\n",
    "[material](http://cs231n.stanford.edu/2016/syllabus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3\n",
    "\n",
    "## SVM\n",
    "\n",
    "Given a classifier in the form of $f(x, W)=Wx$ that assigns class scores. \n",
    "\n",
    "Given an example $(x_i, y_i)$, where $x_i$ is the image and $y_i$ is the integer label, and the score vector is $s = f(x_i, W)$.\n",
    "\n",
    "The **Multi-class SVM Loss (Hinge Loss)** for an example is given by: \n",
    "\n",
    "$$ L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1) $$\n",
    "\n",
    "1 above is a **safty margin** (typical SVM hyperparameter), detailed notes showed that it does not matter as weights can also scale to compensate for it.\n",
    "\n",
    "The loss is essentially summing the error for all the incorrect classes.\n",
    "\n",
    "The **full training losss** is the mean over all examples in the training data:\n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_{i=1}^{N} L_i $$\n",
    "\n",
    "Note: **Squared Hinge Loss**, sometimes works better but meaning is different. \n",
    "\n",
    "$$ L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem with the above loss is that the weight matrix $W$ is not unique.** It can be scaled to still achieve the same score. Therefore we need regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implementation\n",
    "def L_i_vectorized(x, y, W):\n",
    "    scores = np.dot(W, x)\n",
    "    margins = np.maximum(0, scores - scores[y] + 1)\n",
    "    # zero out the correct class\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Hinge Loss Gradient\n",
    "\n",
    "See course [notes](http://cs231n.github.io/optimization-1/#gradcompute) for details. This is written for assigment 1 SVM problems.\n",
    "\n",
    "This SVM has the following definition, where $j$ and $y_i$ are **class labels (indexes into $S$)** and **true class labels**, for $i$th training example in total of $N$ examples:\n",
    "\n",
    "Dimensions:\n",
    "\n",
    "```\n",
    "# D - data dimension\n",
    "# C - no. of classes\n",
    "# N - no. of training examples\n",
    "\n",
    "W.shape == (D, C)\n",
    "X.shape == (N, D)\n",
    "scores.shape = (N, C)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s_i &= X_i W \\\\\n",
    "Margin_{j, y_i} &= s_j - s_{y_i} + 1 = \\text{incorrect class score - true class score + 1}\\\\\n",
    "L(S) &= \\sum_{j \\neq y_i} \\max(0, Margin_{j, y_i}) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For a single training example $i$, where $y_i$ is the correct class label for $i$:\n",
    "\n",
    "$W_j$ is the column vector for the weights for class $j$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s_{y_i} =& W_{y_i}^T X_i  \\\\\n",
    "s_{j} =& W_j^T X_j & j \\neq y_i\\\\\n",
    "Margin_{j, y_i} =& s_j - s_{y_i} + 1 = \\text{incorrect class score - true class score + 1}\\\\\n",
    "L_i(S) =& \\sum_{j \\neq y_i} \\max(0, Margin_{j, y_i}) \\\\\n",
    "=& \\sum_{j \\neq y_i} \\max\\big(0, W_j ^T X_i - W_{y_i}^T X_i + \\delta\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Backprop\n",
    "\n",
    "Dimension:\n",
    "\n",
    "```\n",
    "W_j - (D, 1)\n",
    "W_yi - (D, 1)\n",
    "X_i - (D, 1)\n",
    "z - (1, D) * (D, 1) = (1, 1)\n",
    "L_i - sum((1, C)) == (1, 1)\n",
    "dz - sum((1, C)) == (1, 1)\n",
    "dWj - (D, 1) * (1, 1) = (D, 1)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_j =& W_j ^T X_j - W_{y_i}^T X_i + \\delta \\\\\n",
    "L_i(z_j) =& \\sum_{j \\neq y_i} \\max\\big(0, z_j\\big) \\\\\n",
    "dz =& \\sum_{j \\neq y_i} \\mathbb{1}\\big( W_j ^T X_i - W_{y_i}^T X_i + \\delta > 0 \\big) \\\\\n",
    "=& \\mathbb{1}\\big( W_1 ^T X_i - W_{y_i}^T X_i + \\delta > 0 \\big) + \\\\\n",
    "& \\mathbb{1}\\big( W_2 ^T X_i - W_{y_i}^T X_i + \\delta > 0 \\big) + \\\\\n",
    "& \\cdots \\\\\n",
    "& \\mathbb{1}\\big( W_C ^T X_i - W_{y_i}^T X_i + \\delta > 0 \\big) \\\\\n",
    "dW_{y_i} =& -X_i \\cdot dz \\\\\n",
    "=& -X_i \\cdot \\sum_{j \\neq y_i} \\mathbb{1}\\big( W_j ^T X_i - W_{y_i}^T X_i + \\delta > 0 \\big) & \n",
    "\\text{because } W_{y_i} \\text{exists in every time in dz} \\\\\n",
    "dW_j =& X_i \\cdot dz_j \\\\\n",
    "=& X_i \\cdot \\mathbb{1}\\big( W_j ^T X_i - W_{y_i}^T X_i + \\delta > 0 \\big) & \\text{because other } W_j\\text{ terms disappear}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $\\mathbb{1}()$ is an indicator function.\n",
    "\n",
    "**Essentially, the loss function is a count of the number of incorrect classes.**\n",
    "\n",
    "Vectorized:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "S &= X W & (N, D) \\times (D, C) = (N, C)\\\\\n",
    "Margin_{j, y} &= S_j - S_{y} + 1 & (N, C)\\\\\n",
    "L(S) &= \\sum_{j \\neq y_i} \\bigg(\\max\\big(0, Margin_{j, y_i}\\big)\\bigg) & (1,)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Derivatives are hence:\n",
    "\n",
    "The key here is we need to capture the gradient from the **correct** class here, which is missing from $dS_j$ because $j \\neq y_i$, it is coming from $dS_{y}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dMargin &= \\begin{cases}\n",
    "1 & Margin_{j,y_i} > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases} & (N, C)\\\\\n",
    "ds_{j \\neq y_i} &= 1^{N\\times C} \\odot dMargin & (N, C)\\\\\n",
    "ds_{j = y_i} &= -1^{N \\times C} \\odot \\sum_{j \\neq y_i} dMargin & (N, 1) \\\\\n",
    "dW_{j \\neq y_i} &= X^T \\cdot ds_{j \\neq y_i} & (N, C)\\\\\n",
    "dW_{j = y_i} &= X^T \\cdot ds_{j = y_i} & (N, 1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In code form:\n",
    "\n",
    "```\n",
    "dmargin = loss > 0\n",
    "ds = dmargin\n",
    "dW += np.dot(X.T, dmargin)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Loss function\n",
    "\n",
    "$$ L_i = -\\log \\big( softmax(s_i) \\big) $$\n",
    "\n",
    "**Tip**: Initially, the value of this loss function should be close to $-\\log(1/C)$ where $C$ is the number of classes. This can be used as a sanity check for running optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4 Backprop\n",
    "\n",
    "In a backprop computational graph:\n",
    "\n",
    "**Add** gate: gradient distributor\n",
    "\n",
    "**Max** gate: gradient router, gradient goes to the larger input, smaller input has zero gradient.\n",
    "\n",
    "**Multiply** gate: gradient \"switcher\"?\n",
    "\n",
    "If a node's output is used by multiple consumer nodes then in backprop, gradients coming from the consumer nodes should be **added**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5\n",
    "\n",
    "Good example in video that implemented a check after initalizing a network, feed data through the network without regularization and check the loss, on CIFAR-10 the expected loss is $-log(1/10) \\approx 2.3$.\n",
    "\n",
    "**Tip**: make sure that you can **overfit** very small portion of the training data. If you can't overfit a small portion of the data, then things are broken.\n",
    "\n",
    "### Hyperparameter Search\n",
    "\n",
    "Run coarse search for 5 epochs, then run finer searches. \n",
    "\n",
    "Example showed a good result near the boundaries of the learning rate search space. Thsis may be **problematic**, because it may indicate that the optimal parameter may be **outside** the search space.\n",
    "\n",
    "**Track the ratio of weight updates / weight magnitudes**. This should be `-1e-3`. If too high, maybe decrease learning rate, vice verse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 6\n",
    "\n",
    "### Second Order Optimization\n",
    "\n",
    "Uses **Hessian** matrix, this will have faster convergence, and **no hyperparameter**!\n",
    "\n",
    "However, when training large networks, the Hessian matrix ends up being quite huge, inversing it becomes impractically expensive to evaluate. \n",
    "\n",
    "**L-BFGS** is usually used for second order optimization, however, it works for **deterministic** functions. This means it **does not work in minibatch settings**. \n",
    "\n",
    "Adam is usually a good choice, if you can afford to do full batch updates, then try L-BFGS. \n",
    "\n",
    "### Ensemble Trick/Tips\n",
    "\n",
    "Can get a small boost from averaging multiple model checkpoints of a single model.\n",
    "\n",
    "Keep track of a **running average of parameter vector**, to use at test time. \n",
    "```\n",
    "loss = nn.forward()\n",
    "dx = nn.backward()\n",
    "x += - learning_rate * dx\n",
    "x_test = 0.995 * x_test + 0.005 * x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 7 CNN\n",
    "\n",
    "Input images are `width * height * depth`, e.g. 64 * 64 * 3, for 64x64 images with RGB colours.\n",
    "\n",
    "**Filters**, aka kernels, are used to slide through the images, filter size can be 3x3x3, depth is **always** the same as input images. Filters are dot product operators.\n",
    "\n",
    "**Stride** is the step size to slide the filter. \n",
    "\n",
    "**Dimension**: NxN images, FxF filter size, **output size** is given by: \n",
    "\n",
    "$$(N-F) / stride + 1$$\n",
    "\n",
    "**Multiple** filters can be used in a layer to generate multiple **activation maps**, e.g. 6 filters would give 6 maps in a layer, which then are fed to ReLU for example.\n",
    "\n",
    "3D Convolution is just 2D convolution applied at the same time to the 3rd dimension. \n",
    "\n",
    "\n",
    "### Padding\n",
    "\n",
    "Padding of **zeros** are added to the broders of the input images to **preserve size spatially** for the activation maps after convolution operations. \n",
    "\n",
    "If filter size is FxF, then zero-padding size = $(F-1)/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**, image input 32x32x3, 10 filters with size 5x5x3 and stride 1, padding 2, then after convolution size is 32x32x10, e.g. (32 + 2 * 2 - 5)/1 + 1 = 32.\n",
    "\n",
    "Number of parameters = 5 * 5 * 3 * 10 + 10 = 760, 10 * (filter size + bias). \n",
    "\n",
    "### Dimension Summary\n",
    "\n",
    "For a Conv Layer, \n",
    "\n",
    "Input volume size $W_1 \\times H_1 \\times D_1$\n",
    "\n",
    "Requires 4 hyperparameters:\n",
    "\n",
    "* Number of filters, $K$\n",
    "* filter spaitial extend, $F$,\n",
    "* stride, $S$\n",
    "* the amoutn of zero padding, $P$\n",
    "    \n",
    "Output volume size $W_2 \\times H_2 \\times D_2$, where:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_2 &= (W_1 - F + 2P) / S + 1 \\\\\n",
    "H_2 &= (H_1 - F + 2P) / S + 1 \\\\\n",
    "D_2 &= K\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Wither **parameter sharing**, this introduces $F \\times F \\times D_1$ weights per filter, total parameters size $(F \\times F \\times D_1) \\times K + K$.\n",
    "\n",
    "$K$ is usually chosen as powers of 2.\n",
    "\n",
    "\n",
    "\n",
    "In each activation map, neurons share **weights** (from filters) and **local connectivity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "**Max pooling**: use a filter, e.g. 2x2, then take the max of each 2x2 area. **Average pooling**, performs as well as max pooling. \n",
    "\n",
    "Input volume size: $W_1 \\times H_1 \\times D_1$\n",
    "\n",
    "Requires 2 hyperparameters:\n",
    "\n",
    "* their spatial extend, $F$\n",
    "* stride, $S$\n",
    "\n",
    "Output volume size: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_2 &= (W_1 - F) / S + 1 \\\\\n",
    "H_2 &= (H_1 - F) / S + 1 \\\\\n",
    "D_2 &= D_1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Not common to use zero-padding for pooling layers.\n",
    "\n",
    "Pooling **shrinks** the input volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet Example\n",
    "\n",
    "Input: 227x227x3 images, (paper says 224, confused everyone...)\n",
    "\n",
    "First conv layer (CONV1): 96 11x11x3 filters applied at stride 4, output volume size: 55x55x96, (227 - 11)/4 + 1 = 55. \n",
    "\n",
    "Total number of parmas = 11 * 11 * 3 * 96 + 96 = 35k\n",
    "\n",
    "After pooling layer, 3x3x3 filter applied at stride 2, output size 27x27x96, e.g. (55 - 3) / 2 + 1, 0 params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Example\n",
    "\n",
    "Total memory ~93MB / image for forward pass, ~2x for backward pass.\n",
    "\n",
    "Total Params 138mm.\n",
    "\n",
    "### ResNet\n",
    "\n",
    "A lot deeper, 152 layers, ~5mm parameters. Relying on skip connections, each layer is trained to be a **delta** that is added to the original input. This allows gradients to feed back to the first layer easily. \n",
    "\n",
    "Very repaid spatial reduction, but relying on many layers.\n",
    "\n",
    "8 GPUs trained for 2 weeks....\n",
    "\n",
    "As of 2017, Inception-V4 (ResNet + Inception) has the best top-1 accuracy of ~80%. VGG has the highest memory and compute usage. See this [video](https://www.youtube.com/watch?v=DAOcjicFr1Y&index=9&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)\n",
    "\n",
    "ResNeXT was published later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet\n",
    "\n",
    "Dense blocks where each layer is connected to every other layer in feedforward fashion. \n",
    "\n",
    "Alleviates vanishing gradient, strengthens feature propagation, encourages feature reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Implementation\n",
    "\n",
    "Replace large covolutions with stacks of 3x3 convolutions\n",
    "\n",
    "1x1 bottleneck convolutions are very efficient\n",
    "\n",
    "Can factor NxN convolution into 1xN and Nx1\n",
    "\n",
    "All of the above give fewer parameters, less compute, more non-linearity.\n",
    "\n",
    "## Convolution \n",
    "\n",
    "### im2col\n",
    "\n",
    "Input H x W x C, conv weights: D filters, each K x K x C\n",
    "\n",
    "Reshape K x K x C (dimension of receptive fields in the input) into matrix columns of $K^2 C$ elements. Repeat for all columns to get a $K^2C \\times N$ matrix (N receptive field locations).\n",
    "\n",
    "This uses a lot of memory but in practice ok. \n",
    "\n",
    "Reshape each filter to $K^2C$ length rows, making a $D \\times K^2C$ matrix.\n",
    "\n",
    "For mini-batches, concat each sample to the matrices above to form the batch.\n",
    "\n",
    "Use matrix multiply to get results in dimension of $D \\times N$, reshape to output tensor.\n",
    "\n",
    "### FFT\n",
    "\n",
    "Convolution Theorem: the convolution of $f$ and $g$ is equal to the element-wise product of their Fourier Transforms: \n",
    "\n",
    "$$ \\mathcal{F}(f * g) = \\mathcal{F}(f) \\odot \\mathcal{F}(g) $$\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. compute FFT of weights: F(W)\n",
    "2. compute FFT of image (activation map): F(X)\n",
    "3. compute element-wise product of F(W) * F(X)\n",
    "4. Compute inverse FFT: Y = inv_F[F(W) * F(X)]\n",
    "\n",
    "Big speed up but only for large filters. **Not much speedup for 3x3 filters!**\n",
    "\n",
    "### Strassen's Algorithm \n",
    "\n",
    "Navie matrix multiplication takes $O(N^3)$ operations. Strassen's algo reduces complexity to $O(N^{log_2(7)})$.\n",
    "\n",
    "Lavin and Gray 2015 paper: Fast Algorithms for Convolution Neural Networks. Improved VGG speed by factor of 2.\n",
    "\n",
    "## Other Tips\n",
    "\n",
    "Use lower precision floating point such as 16-bit. \n",
    "\n",
    "Store data in SSD drives. Write data into HDF5 file to reduce disk seek time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='rnn'></a>\n",
    "# RNN / LSTM Lecture 10\n",
    "\n",
    "[char-rnn code](https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "\n",
    "[2017 video](https://www.youtube.com/watch?v=6niqTuYFZLQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initial hidden state is usually initialized to zero.\n",
    "\n",
    "Because of the recurrence, the same weight matrix $W$ is used many times, in a computational graph, this is essentially reusing the same node many times. In the backprop, this means you end up summing the gradients for $dW$.\n",
    "\n",
    "Input: $X = (x_1, x_2, \\cdots, x_n)$\n",
    "Hidden units: $H = (h_1, h_2, \\cdots, h_n)$\n",
    "\n",
    "Each input and hidden unit pair is passed to an activation function $f_W()$, whose output is the next hidden unit. Generally:\n",
    "\n",
    "$$ f_W(x_{t}, h_{t-1}) = h_{t} $$\n",
    "\n",
    "Loss: $L = \\sum_{t=1}^{n} L_t$, where $L_t = y_t - h_t, \\forall t$ \n",
    "\n",
    "**Sequence to Sequence** netowrk is a a) Many-to-One RNN + b) One-to-Many RNN, where the last hidden state from a) is the input sequence for b).\n",
    "\n",
    "**Truncated** Backprop Through Time: break the dataset into pieces and do forward/backward passes for each piece separate. This is quite similiar to minibatch SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "**Gates**: $i, f, o, g$, aka **ifog**. **cell state** $c_t$, **hidden state** $h_t$\n",
    "\n",
    "Input is $x$ or $h_{t-1}$, outputs are $c_t$ and $h_t$.\n",
    "\n",
    "RNN models typically not deep, 2-4 layers normally.\n",
    "\n",
    "### Formula\n",
    "\n",
    "Here when $l=1$, $h^{l-1}_t = x_t$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "i\\\\\n",
    "f\\\\\n",
    "o\\\\\n",
    "g\n",
    "\\end{array} \\right)\n",
    " &= \n",
    "\\left( \n",
    "\\begin{array}{cc}\n",
    "sigmoid\\\\\n",
    "sigmoid\\\\\n",
    "sigmoid\\\\\n",
    "sigmoid\n",
    "\\end{array} \\right) W^{l} \\left( \\begin{array}{cc}\n",
    "h_t^{l-1} \\\\\n",
    "h_{t-1}^{l}\n",
    "\\end{array} \\right)\\\\\n",
    "c^l_t &= f \\odot c^l_{t-1} + i \\odot g \\\\\n",
    "h^l_t &= o \\odot \\tanh(c^l_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Vanilla RNNs suffer from exploding / vanishing gradient problem. Backpropagation through multiplication gates results in multiplying $W^T$ for computing gradients of $h_t$. This means the gradient of $h_0$ involves many factors of $W^T$. \n",
    "\n",
    "Whether it explodes or vanishes depends on the **largest eigenvalue of the hidden state weight matrix $W$**. If larger than 1 it explodes, vanishes if less than 1.\n",
    "\n",
    "Hack for this: **gradient clipping**.\n",
    "\n",
    "Forget gates can **turn off** gradient flow. At start we typically initialize forget gates to positive numbers so **not** to turn off gradient, the network then learns how to forget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension of weights\n",
    "\n",
    "$x_t$, $h_{t-1}$ are vectors of length $h$, stack them together to form a matrix of $2h \\times 1$.\n",
    "\n",
    "weights are combined into $W$ for $i,f,o,g$, shape $4h \\times 2h$. \n",
    "\n",
    "\n",
    "### Backprop\n",
    "\n",
    "Gradients need to be accumlated (summed) as they flow back in the back pass. \n",
    "\n",
    "For each cell, there will be gradients coming back from the output as well as the **next** hidden state. This results in the hidden state weight $W_hh$ being applied repeatedly, resulting in the exploding/vanishing gradient problem. \n",
    "\n",
    "Look at Andrej's char-rnn model and description [here](http://cs231n.github.io/neural-networks-case-study/#grad). The derivative derivation shows how gradients are accumulated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DL Libraries\n",
    "\n",
    "Lots of examples in [slides](http://cs231n.stanford.edu/slides/2016/winter1516_lecture12.pdf).\n",
    "\n",
    "Use cases:\n",
    "\n",
    "**Caffe**\n",
    "\n",
    "Feature extraction / fine-tuning existing models.\n",
    "\n",
    "Not good for RNNs. Cumbersome for big networks such as GooLeNet, ResNet. \n",
    "\n",
    "**Torch / Lasagne**\n",
    "\n",
    "Complex uses of pre-trained models.\n",
    "\n",
    "Write you own layers.\n",
    "\n",
    "Not great for RNNs. \n",
    "\n",
    "Lua easy to read.\n",
    "\n",
    "**Theano or Tensorflow**\n",
    "\n",
    "For crazy RNNs\n",
    "\n",
    "**TensorFlow**\n",
    "\n",
    "Huge models, need model parallelism.\n",
    "\n",
    "Visualization during training: **TensorBoard**\n",
    "\n",
    "Data and model parallelism, best of all frameworks\n",
    "\n",
    "Computational graph abstraction, great for RNNs.\n",
    "\n",
    "Best multi-GPU support.\n",
    "\n",
    "Distributed models. \n",
    "\n",
    "Slower in other models right now. \n",
    "\n",
    "Not many pretrained models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
