{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford cs231n Notes\n",
    "\n",
    "Winter 2016 series by Li Fei-Fei, Andrej Karpathy and Justin Johnson\n",
    "\n",
    "[videos](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)\n",
    "\n",
    "[material](http://cs231n.stanford.edu/2016/syllabus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5\n",
    "\n",
    "Good example in video that implemented a check after initalizing a network, feed data through the network without regularization and check the loss, on CIFAR-10 the expected loss is $-log(1/10) \\approx 2.3$.\n",
    "\n",
    "**Tip**: make sure that you can **overfit** very small portion of the training data. If you can't overfit a small portion of the data, then things are broken.\n",
    "\n",
    "### Hyperparameter Search\n",
    "\n",
    "Run coarse search for 5 epochs, then run finer searches. \n",
    "\n",
    "Example showed a good result near the boundaries of the learning rate search space. Thsis may be **problematic**, because it may indicate that the optimal parameter may be **outside** the search space.\n",
    "\n",
    "**Track the ratio of weight updates / weight magnitudes**. This should be `-1e-3`. If too high, maybe decrease learning rate, vice verse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 6\n",
    "\n",
    "### Second Order Optimization\n",
    "\n",
    "Uses **Hessian** matrix, this will have faster convergence, and **no hyperparameter**!\n",
    "\n",
    "However, when training large networks, the Hessian matrix ends up being quite huge, inversing it becomes impractically expensive to evaluate. \n",
    "\n",
    "**L-BFGS** is usually used for second order optimization, however, it works for **deterministic** functions. This means it **does not work in minibatch settings**. \n",
    "\n",
    "Adam is usually a good choice, if you can afford to do full batch updates, then try L-BFGS. \n",
    "\n",
    "### Ensemble Trick/Tips\n",
    "\n",
    "Can get a small boost from averaging multiple model checkpoints of a single model.\n",
    "\n",
    "Keep track of a **running average of parameter vector**, to use at test time. \n",
    "```\n",
    "loss = nn.forward()\n",
    "dx = nn.backward()\n",
    "x += - learning_rate * dx\n",
    "x_test = 0.995 * x_test + 0.005 * x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 7 CNN\n",
    "\n",
    "Input images are `width * height * depth`, e.g. 64 * 64 * 3, for 64x64 images with RGB colours.\n",
    "\n",
    "**Filters**, aka kernels, are used to slide through the images, filter size can be 3x3x3, depth is **always** the same as input images. Filters are dot product operators.\n",
    "\n",
    "**Stride** is the step size to slide the filter. \n",
    "\n",
    "**Dimension**: NxN images, FxF filter size, **output size** is given by: \n",
    "\n",
    "$$(N-F) / stride + 1$$\n",
    "\n",
    "**Multiple** filters can be used in a layer to generate multiple **activation maps**, e.g. 6 filters would give 6 maps in a layer, which then are fed to ReLU for example.\n",
    "\n",
    "3D Convolution is just 2D convolution applied at the same time to the 3rd dimension. \n",
    "\n",
    "\n",
    "### Padding\n",
    "\n",
    "Padding of **zeros** are added to the broders of the input images to **preserve size spatially** for the activation maps after convolution operations. \n",
    "\n",
    "If filter size is FxF, then zero-padding size = $(F-1)/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**, image input 32x32x3, 10 filters with size 5x5x3 and stride 1, padding 2, then after convolution size is 32x32x10, e.g. (32 + 2 * 2 - 5)/1 + 1 = 32.\n",
    "\n",
    "Number of parameters = 5 * 5 * 3 * 10 + 10 = 760, 10 * (filter size + bias). \n",
    "\n",
    "### Dimension Summary\n",
    "\n",
    "For a Conv Layer, \n",
    "\n",
    "Input volume size $W_1 \\times H_1 \\times D_1$\n",
    "\n",
    "Requires 4 hyperparameters:\n",
    "\n",
    "* Number of filters, $K$\n",
    "* filter spaitial extend, $F$,\n",
    "* stride, $S$\n",
    "* the amoutn of zero padding, $P$\n",
    "    \n",
    "Output volume size $W_2 \\times H_2 \\times D_2$, where:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_2 &= (W_1 - F + 2P) / S + 1 \\\\\n",
    "H_2 &= (H_1 - F + 2P) / S + 1 \\\\\n",
    "D_2 &= K\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Wither **parameter sharing**, this introduces $F \\times F \\times D_1$ weights per filter, total parameters size $(F \\times F \\times D_1) \\times K + K$.\n",
    "\n",
    "$K$ is usually chosen as powers of 2.\n",
    "\n",
    "\n",
    "\n",
    "In each activation map, neurons share **weights** (from filters) and **local connectivity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "**Max pooling**: use a filter, e.g. 2x2, then take the max of each 2x2 area. **Average pooling**, performs as well as max pooling. \n",
    "\n",
    "Input volume size: $W_1 \\times H_1 \\times D_1$\n",
    "\n",
    "Requires 2 hyperparameters:\n",
    "\n",
    "* their spatial extend, $F$\n",
    "* stride, $S$\n",
    "\n",
    "Output volume size: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_2 &= (W_1 - F) / S + 1 \\\\\n",
    "H_2 &= (H_1 - F) / S + 1 \\\\\n",
    "D_2 &= D_1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Not common to use zero-padding for pooling layers.\n",
    "\n",
    "Pooling **shrinks** the input volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet Example\n",
    "\n",
    "Input: 227x227x3 images, (paper says 224, confused everyone...)\n",
    "\n",
    "First conv layer (CONV1): 96 11x11x3 filters applied at stride 4, output volume size: 55x55x96, (227 - 11)/4 + 1 = 55. \n",
    "\n",
    "Total number of parmas = 11 * 11 * 3 * 96 + 96 = 35k\n",
    "\n",
    "After pooling layer, 3x3x3 filter applied at stride 2, output size 27x27x96, e.g. (55 - 3) / 2 + 1, 0 params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Example\n",
    "\n",
    "Total memory ~93MB / image for forward pass, ~2x for backward pass.\n",
    "\n",
    "Total Params 138mm.\n",
    "\n",
    "### ResNet\n",
    "\n",
    "A lot deeper, 152 layers, ~5mm parameters. Relying on skip connections, each layer is trained to be a **delta** that is added to the original input. This allows gradients to feed back to the first layer easily. \n",
    "\n",
    "Very repaid spatial reduction, but relying on many layers.\n",
    "\n",
    "8 GPUs trained for 2 weeks....\n",
    "\n",
    "As of 2017, Inception-V4 (ResNet + Inception) has the best top-1 accuracy of ~80%. VGG has the highest memory and compute usage. See this [video](https://www.youtube.com/watch?v=DAOcjicFr1Y&index=9&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)\n",
    "\n",
    "ResNeXT was published later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet\n",
    "\n",
    "Dense blocks where each layer is connected to every other layer in feedforward fashion. \n",
    "\n",
    "Alleviates vanishing gradient, strengthens feature propagation, encourages feature reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
