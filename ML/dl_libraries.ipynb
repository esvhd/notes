{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "This is a set of notes on `keras` based on either Prof. Andrew Ng's Deeplearning.ai course, or Francois Chollet's book Deep Learning with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs** in `keras` are also represented as layers: `keras.layers.Input()`.\n",
    "\n",
    "The **first** layer needs an `input_shape=` parameters, the shape here should be the input data shape **without** the batch dimension. \n",
    "\n",
    "**Activations** can also be standalone layers, `keras.layers.Activations()`. Some layers have optional parameters to build in activations, such as in `Dense()` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways for multi-class classification:\n",
    "1. one-hot labels, use `categorical_crossentropy` loss.\n",
    "2. integer labels, use `sparse_categorical_crossentropy` loss. \n",
    "\n",
    "**Avoid** shrinking layer dimension smaller than input dimension too quickly to avoid loss of information early in the chain.\n",
    "\n",
    "When calling `model.compile()`, you can specify a loss function with parameter `loss=`, as well as a metric to monitor with `metrics=` param.\n",
    "\n",
    "Validation: `model.evaluate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN \n",
    "\n",
    "`keras` RNN layers take input in the shape of `(batch_size, timesteps, input_features)`.\n",
    "\n",
    "Recurrent layers all have **dropout** related params: \n",
    "* `dropout=` floating dropout rate for layer inputs\n",
    "* `recurrent_dropout=` dropout rate for the recurrent unit\n",
    "\n",
    "Yarin Gal 2015 PhD thesis: recurrent layer dropout should use the **same** dropout mask for every timestep.\n",
    "\n",
    "`keras.layers.LSTM()` has boolean parameter `return_sequences=` to either return sequences, or the last element of the returned sequence. \n",
    "\n",
    "Parameter `implementation=` (either 1 or 2) controls how computations are done. Looks like mode 2 is vectorized for batch processing. See code [here](https://github.com/keras-team/keras/blob/d9f26a92f4fdc1f1e170a4203a7c13abf3af86e8/keras/layers/recurrent.py#L1821)\n",
    "\n",
    "`keras.layers.Bidirectional()` for Bidirectional RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Weights\n",
    "\n",
    "Once you build a model, you can use `model.load_weights()` to load previously saved weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Weights\n",
    "\n",
    "To **freeze** layer weights, set `trainable=False` when instantiating the layer. \n",
    "\n",
    "Use `set_weights()` to set layer weights to pre-trained values. Example below, thanks to Andrew Ng's Deeplearning.ai Coursera course:\n",
    "\n",
    "\n",
    "```\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=False)\n",
    "# or set embedding_layer.trainable = False\n",
    "\n",
    "# Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "# Do not modify the \"None\".\n",
    "embedding_layer.build((None,))\n",
    "\n",
    "# Set the weights of the embedding layer to the embedding matrix. \n",
    "# Your layer is now pretrained.\n",
    "embedding_layer.set_weights([emb_matrix])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Use `keras.regularizers.*`. Instances can be passed to layers using param `kernel_regularizer=`.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "* **Balanced**-classification: ROC AUC\n",
    "* **Imbalanced**-classification: precision and recall, F1 score\n",
    "* **Ranking/Multi-label classification**: mean average precision. \n",
    "\n",
    "**TODO**: data generators, `model.fit_generator()`\n",
    "\n",
    "### Multiple Inputs\n",
    "\n",
    "\n",
    "### Multiple Outputs / Loss functions\n",
    "\n",
    "### Callbacks\n",
    "\n",
    "### Tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Always **remove** redundancy in your data. \n",
    "\n",
    "**One Hot Encoding**: `keras.utils.np_utils.to_categorical()`\n",
    "\n",
    "**Sequence Padding**: `keras.preprocessing.sequence.pad_sequence()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Tips\n",
    "\n",
    "Be aware of **nonstationary** problems. Because such problems change over time, the right move is:\n",
    "* constantly training on recent data, or\n",
    "* gather data at a timescale where the problem is stationary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
