{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "This is a set of notes on `keras` based on either Prof. Andrew Ng's Deeplearning.ai course, or Francois Chollet's book Deep Learning with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs** in `keras` are also represented as layers: `keras.layers.Input()`.\n",
    "\n",
    "The **first** layer needs an `input_shape=` parameters, the shape here should be the input data shape **without** the batch dimension. \n",
    "\n",
    "**Activations** can also be standalone layers, `keras.layers.Activations()`. Some layers have optional parameters to build in activations, such as in `Dense()` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways for multi-class classification:\n",
    "1. one-hot labels, use `categorical_crossentropy` loss.\n",
    "2. integer labels, use `sparse_categorical_crossentropy` loss. \n",
    "\n",
    "**Avoid** shrinking layer dimension smaller than input dimension too quickly to avoid loss of information early in the chain.\n",
    "\n",
    "When calling `model.compile()`, you can specify a loss function with parameter `loss=`, as well as a metric to monitor with `metrics=` param.\n",
    "\n",
    "Validation: `model.evaluate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers\n",
    "\n",
    "Extend from `keras.layers.Layer`, must implement `call(self, inputs)` which should always return a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Always **remove** redundancy in your data. \n",
    "\n",
    "**One Hot Encoding**: `keras.utils.np_utils.to_categorical()`\n",
    "\n",
    "**Sequence Padding**: `keras.preprocessing.sequence.pad_sequence()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Tips\n",
    "\n",
    "Be aware of **nonstationary** problems. Because such problems change over time, the right move is:\n",
    "* constantly training on recent data, or\n",
    "* gather data at a timescale where the problem is stationary.\n",
    "\n",
    "\n",
    "\n",
    "## Emsembles \n",
    "\n",
    "One style that has had recent success is the **wide and deep** category of models. Such models consist of jointly training a deep neural network with a large linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN \n",
    "\n",
    "`keras` RNN layers take input in the shape of `(batch_size, timesteps, input_features)`.\n",
    "\n",
    "Recurrent layers all have **dropout** related params: \n",
    "* `dropout=` floating dropout rate for layer inputs\n",
    "* `recurrent_dropout=` dropout rate for the recurrent unit\n",
    "\n",
    "Yarin Gal 2015 PhD thesis: recurrent layer dropout should use the **same** dropout mask for every timestep.\n",
    "\n",
    "`keras.layers.LSTM()` has boolean parameter `return_sequences=` to either return sequences, or the last element of the returned sequence. \n",
    "\n",
    "Parameter `implementation=` (either 1 or 2) controls how computations are done. Looks like mode 2 is vectorized for batch processing. See code [here](https://github.com/keras-team/keras/blob/d9f26a92f4fdc1f1e170a4203a7c13abf3af86e8/keras/layers/recurrent.py#L1821)\n",
    "\n",
    "`keras.layers.Bidirectional()` for Bidirectional RNN.\n",
    "\n",
    "### Load Model Weights\n",
    "\n",
    "Once you build a model, you can use `model.load_weights()` to load previously saved weights.\n",
    "\n",
    "### Layer Weights\n",
    "\n",
    "To **freeze** layer weights, set `trainable=False` when instantiating the layer. \n",
    "\n",
    "Use `set_weights()` to set layer weights to pre-trained values. Example below, thanks to Andrew Ng's Deeplearning.ai Coursera course:\n",
    "\n",
    "\n",
    "```\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=False)\n",
    "# or set embedding_layer.trainable = False\n",
    "\n",
    "# Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "# Do not modify the \"None\".\n",
    "embedding_layer.build((None,))\n",
    "\n",
    "# Set the weights of the embedding layer to the embedding matrix. \n",
    "# Your layer is now pretrained.\n",
    "embedding_layer.set_weights([emb_matrix])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Use `keras.regularizers.*`. Instances can be passed to layers using param `kernel_regularizer=`.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "* **Balanced**-classification: ROC AUC\n",
    "* **Imbalanced**-classification: precision and recall, F1 score\n",
    "* **Ranking/Multi-label classification**: mean average precision. \n",
    "\n",
    "### `model.fit_generator()`\n",
    "\n",
    "[docs](https://keras.io/models/model/#fit_generator)\n",
    "\n",
    "First argument is expected to be a python generator that will yield **batches** of inputs and targets **indefinitely**, i.e. returns `(samples, target)`, where `len(samples) == batch_size`. \n",
    "\n",
    "How many samples are drawn for each epoch is defined by param `steps_per_epoch=`.\n",
    "\n",
    "`validation_data=` can be either a generator or numpy arrays. `validate_steps=` should be specified if a generator is given.\n",
    "\n",
    "Example: `keras.preprocessing.image.ImageDataGenerator`\n",
    "\n",
    "### Saving Trained Model\n",
    "\n",
    "Models can be saved by calling `model.save('path.h5')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Inputs\n",
    "\n",
    "Jointly train multiple networks with a combined loss function. Keras `functional` API provides flexible ways to achieve this. \n",
    "\n",
    "Model object can be created with multiple inputs, specified as a **list** of inputs. When calling `fit()`, input can either be: \n",
    "* a list, or \n",
    "* a dict with keys matching `Input` layer names.\n",
    "\n",
    "Output from different sources are combined with `keras.layers.concatenate()` beforing feeding to the next layer. \n",
    "\n",
    "### Multiple Outputs / Loss functions\n",
    "\n",
    "Multiple loss functions can be used but in the end there needs to be a single loss. `keras` provides a way to simply sum the losses to produce a single loss. This is done by specifying in the `model.compile()` call with either: \n",
    "\n",
    "* a list of loss functions, e.g. `['mse', 'categorical_crossentropy', 'binary_crossentropy']`,\n",
    "* a dictionary with keys matching the output layer names, e.g. `{'age': 'mse', 'income': 'categorical_crossentropy'}`. \n",
    "* loss weights can be specified by `loss_weights=` for weighted sums.\n",
    "\n",
    "### Layer Weight Sharing\n",
    "\n",
    "A layer instance can be used multiple times, the weights in this case would be shared across all calls. Example use is a **Siamese** network. \n",
    "\n",
    "Models can also be used as layers. Example: dual camera inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Models\n",
    "\n",
    "Use `keras.utils.plot_model()`. If `show_shapes=True` then layer shapes are also shown. [docs](https://keras.io/utils/#plot_model)\n",
    "\n",
    "### Callbacks\n",
    "\n",
    "Built-in callbacks: `keras.callbacks`, [docs](https://keras.io/callbacks/)\n",
    "\n",
    "Examples of callbacks:\n",
    "\n",
    "* Model checkpointing\n",
    "* Early stopping\n",
    "* Dynamically adjust certain parameters during training, such as learning rate\n",
    "* logging training and validation metrics\n",
    "\n",
    "### Tensorboard\n",
    "\n",
    "Use `keras.callbacks.TensorBoard`, [docs](https://keras.io/callbacks/#tensorboard).\n",
    "\n",
    "Steps:\n",
    "1. create logging directory `z`\n",
    "2. create TensorBoard callback, provide `log_dir=z`, see docs for other params such as `histogram_freq=` and `embeddings_freq=`.\n",
    "3. pass the callback to fit() with `callbacks=` param.\n",
    "4. at command prompt, call: `tensorboard --logdir=my_log_dir`\n",
    "5. connect to host `http://localhost:6006`\n",
    "\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "\n",
    "Hyperparameter space is usually made of discrete decisions and thus isn't continuous or differentiable. Hence gradient descent doesn't work. Need gradient free methods, which is far less efficient. \n",
    "\n",
    "* Hyperopt\n",
    "* Hyperas (integrates Hyperopt with Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "Hyperparameter space is usually made of discrete decisions and thus isn't continuous or differentiable. Hence gradient descent doesn't work. Need gradient free methods, which is far less efficient. \n",
    "\n",
    "* Hyperopt\n",
    "* Hyperas (integrates Hyperopt with Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
