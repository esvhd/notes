{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution Stratgies\n",
    "\n",
    "This is a set of notes based on [@hardmaru](https://twitter.com/hardmaru?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)'s blog posts that can be found [here](http://blog.otoro.net/2017/10/29/visual-evolution-strategies/) and [here](http://blog.otoro.net/2017/11/12/evolving-stable-strategies/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two simple toy problems for testing continuous black-box optimization algorithms:\n",
    "\n",
    "* Schaffer function\n",
    "* Rastrigin function\n",
    "\n",
    "General outline of an evolution strategy:\n",
    "\n",
    "* **Objective function** that takes a given **solution** and returns a single **fitness** score.\n",
    "* Based on the current fitness score, the algorith produces the next generation of candidate soutions that is likely to produce even better results.\n",
    "* Iterate the above steps until a satisfactory solution is found.\n",
    "\n",
    "## Simple Evolution Strategy\n",
    "\n",
    "We draw solutions from a Normal distribution with mean and standard deviation of $\\mu$ and $\\sigma$. \n",
    "\n",
    "Run through the solutions and produce a fitness score for each. Keep the best solution and use them as the new $(\\mu, \\sigma)$ of a Normal distribution from which the next generation of solutions will be drawn. \n",
    "\n",
    "This algorithm is **greedy**, so it can be prone to be stuck at a local optimum for more complicated problems. We need a more **diverse** set of ideas!\n",
    "\n",
    "## Simple Genetic Algorithm\n",
    "\n",
    "Instead of keeping only the best solution, in GA we keep the top 10% of the solutions in the current generation. Let the rest of the population die. \n",
    "\n",
    "For the next generation, randomly select two solutions from the survivors, recombine their parameters to form a new solution. This **crossover** recombination process uses a coin toss to determine which parent to take each paramter from. \n",
    "\n",
    "Gaussian noise and a fixed standard deviation will be injected into each new solution after this recombination process.\n",
    "\n",
    "GA helps to diversify. However, in practice, most of the solutions in the elite surviving population tend to convere to a **local optimum** over time. \n",
    "\n",
    "Other sophisticated GA out there: CoSyNe, ESP, and NEAT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance-Matrix Adaptation Evolution (CMA-ES)\n",
    "\n",
    "Drawback of simple ES and simple GA: standard deviation noise paramter is fixed. But there are times when we want to explore more and increase the stdev of the search space, and vice versa. \n",
    "\n",
    "CMA-ES can adaptively increase or decrease the search space for the next generation. It will calculate the entire **covariance matrix** of the parameter space. At each generation, CMA-ES provides the parameters of a multi-variate normal distribution to sample solutions from. \n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "1. Calculate the fitness scores for all candidate solutions in generation (g)\n",
    "2. Take the top 25% of generation (g),\n",
    "3. Calculate the means for next generation (g+1) $\\mu^{(g+1)}$ in the population using the top 25% of current generation (g).\n",
    "4. Calculate the 2D covariance matrix $C^{(g+1)}$ for generate (g+1), but using the current generation's $\\mu^{(g)}$ (top 25% still).\n",
    "5. Sample new set of solutions using $(\\mu^{(g+1)}, C^{(g+1)})$. \n",
    "\n",
    "Complexity is $O(N^2)$, approximations can get to $O(N)$ recently. \n",
    "\n",
    "**Ok to use when search space is less than 1k parameters, up to 10k if patient.**\n",
    "\n",
    "Detail see [this](https://arxiv.org/abs/1604.00772)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mu(x):\n",
    "    '''\n",
    "    Parameters:\n",
    "    x:\n",
    "        parameter matrix for current generation.\n",
    "        \n",
    "    Returns:\n",
    "    mean for next generation.\n",
    "    '''\n",
    "    return np.mean(x, axis=0)\n",
    "\n",
    "\n",
    "def vcov(x, mu_prev):\n",
    "    '''\n",
    "    Parameters:\n",
    "    x:\n",
    "        parameter matrix for current generation\n",
    "    mu_prev:\n",
    "        mean of parameters from previous generation\n",
    "        \n",
    "    returns:\n",
    "    covariance matrix for next generation.\n",
    "    '''\n",
    "    _, D = x.shape\n",
    "    L = mu_prev.shape\n",
    "    assert(D == L)\n",
    "    \n",
    "    # populate diagnal\n",
    "    vcov = np.diag(np.mean(np.power(x - mu_prev, 2), axis=0))\n",
    "    \n",
    "    # populate off-diagnal items.\n",
    "    for i in range(D):\n",
    "        for j in range(D):\n",
    "            if i == j:\n",
    "                continue\n",
    "            vcov[i, j] = np.mean((x[:, i] - mu_prev[i]) * (x[:, j] - mu_prev[j]))\n",
    "            vcov[j, i] = vcov[i, j]\n",
    "            \n",
    "            \n",
    "def compute_next(x, mu_prev):\n",
    "    mu_next = mu(x)\n",
    "    vcov_next = vcov(x, mu_prev)\n",
    "    return (mu_next, vcov_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Evolution Strategy\n",
    "\n",
    "[paper](http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf)\n",
    "\n",
    "Weakness of CMA-ES and other simple strategies so far: Weak solutions that contain info on what **not** to do is discarded.\n",
    "\n",
    "**REINFORCE-ES** Idea: maximize the **expected value** of the fitness score of a sampled solution. This is **almost** the same as maximizing the total fitness score of the entire population.\n",
    "\n",
    "Can use gradient descent methods, such as momentum SGD, RMSProp or Adam.\n",
    "\n",
    "Unlike CMA-ES, there is no correlation structure in this implementation. Complexity is $O(N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI ES\n",
    "\n",
    "In their [paper](https://blog.openai.com/evolution-strategies/) they implemented a special case of REINFORCE-ES algorithm: \n",
    "\n",
    "* keep $\\sigma$ constant, only update $\\mu$ at each generation.\n",
    "* modified update rule suitable for parallel computation across multiple machines.\n",
    "\n",
    "Paper discussed lots of practial aspects of deploying ES / RL-sytle tasks, worth a read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Shaping\n",
    "\n",
    "**Fitness shaping** allow us to avoid outliers in the population dominating the approximate gradient calculation mentioned above.\n",
    "\n",
    "Idea is to apply a **rank transformation** of the raw fitness scores, normalizes to [-.5, .5]. Similar to batch norm. \n",
    "\n",
    "## Discussion\n",
    "\n",
    "ES is good at problems where it is difficult to calculation accurate gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution Strategies for Reinforcement Learning\n",
    "\n",
    "General steps in RL:\n",
    "\n",
    "```\n",
    "env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    a = agent.get_action(obs)\n",
    "    obs, reward, done = env.step(a)\n",
    "    total_reward += reward\n",
    "```\n",
    "\n",
    "## Deterministic Policies\n",
    "\n",
    "The agent can be modelled with many things, hard coded rules, decision trees, linear functions, or RNN. \n",
    "\n",
    "Example uses a 2-layer FC network with `tanh` activation. Weights: $W_1$ and $W_2$.\n",
    "\n",
    "## Stochastic Policies\n",
    "\n",
    "###  Bayesian Neural Networks\n",
    "\n",
    "Instead of having weights $W$ explicitly, we have $N(\\mu, \\sigma)$, during each forward pass, a new $W$ is sampled from $N(\\mu, \\sigma)$. \n",
    "\n",
    "Stochastic policy network / **Proximal Policy Optimizaton (PPO)** samples from $N(\\mu, \\sigma)$ for the final layer. \n",
    "\n",
    "**Adding noise to parameters** are also known to encourage the agent to explore the environment and **escape from local optima**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
