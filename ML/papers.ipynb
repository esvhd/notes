{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Papers\n",
    "\n",
    "[Highway Networks](#highway)\n",
    "\n",
    "[DenseNet](#densenet)\n",
    "\n",
    "[Bootstrap](#bootstrap)\n",
    "\n",
    "[LSTM/RHN on Natural Language Modeling](#1707.05589)\n",
    "\n",
    "[Clustering Financial Time Series](#clustering_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Normalization\n",
    "\n",
    "[paper](https://arxiv.org/abs/1602.07868)\n",
    "\n",
    "Speeds up SGD convergence by regularizing the weight norm. For a given network:\n",
    "\n",
    "$$ y = \\phi(w \\cdot x + b) $$\n",
    "\n",
    "where $w \\in \\mathcal{R}^k$, $b$ is a scalar bias, $x \\in \\mathcal{R}^k$, we reparameterize the $w$ as:\n",
    "\n",
    "$$ w = \\frac{g}{\\| v \\|} v $$\n",
    "\n",
    "where $g$ is a scaler, $\\| v \\|$ denotes the Euclidean norm of vector $v$. We now have $\\| w \\| = g$.\n",
    "\n",
    "Forthermore, we can reparameterize $g$ as $g = e^s$, where $s$ is a log-scale parameter to learn by SGD. However, empirically, the authors **did not** find this to be an advantage, and optimization was slightly slower.\n",
    "\n",
    "### Comparison to Batch Norm\n",
    "\n",
    "In special cases weight norm is the same as batch norm, see paper section 2.2 for detail.\n",
    "\n",
    "For CNNs, weight normalization is often much faster computationally, it is also non-stochastic, not affected by batch size. It can be viewed as a cheaper and less noisy approximation to batch norm. Equivalence does not hold for deeper architectures.\n",
    "\n",
    "### Data-Dependent Initialization of Parameters\n",
    "\n",
    "Important to properly initialize our parameters. Authors proposed to sample the elements of $v$ from a simple distribution with fixed scale, such as a normal distribution with zero mean and standard deviation of 0.05.\n",
    "\n",
    "This only works where batch norm is applicable, for RNNs and LSTMs, need to resort to standard initilization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='highway'></a>\n",
    "## Highway Networks\n",
    "\n",
    "[Summary Paper](https://arxiv.org/abs/1505.00387)\n",
    "\n",
    "[Full Paper](https://arxiv.org/abs/1507.06228)\n",
    "\n",
    "\n",
    "Highway networks enables the optimization of the networks with virtually arbitary depth. This is accomplished through the use of a **learned gating machanism** for regulating information flow wihch is inspired by LSTM.\n",
    "\n",
    "The paper shows that the optimization of highway network is virtually independent of depth. Used to train a 900-layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plain feedforward network of $L$ layers with $H$ as a non-linear transform function, ignoring layer index:\n",
    "\n",
    "$$ y = H(x, W_H) $$\n",
    "\n",
    "For a highway network, the paper adds two non-linear transform: \n",
    "* Tranform gate, $T(x W_T)$\n",
    "* Carry gate, $C(x, W_C)$\n",
    "\n",
    "$$ y = H(x, W_H) \\cdot T(x, W_T) + x \\cdot C(x, W_C) $$\n",
    "\n",
    "The paper sets $C = 1 - T$\n",
    "\n",
    "The **dimensionality** of $x$, $y$, $H(x, W_H)$ and $T(x, W_T)$ must be the same for the equation above to hold.\n",
    "\n",
    "If size of the representation needs to be changed, two ways:\n",
    "\n",
    "1. replace $x$ with $\\tilde{x}$ obtained by suitably sub-sampling or zero-padding $x$\n",
    "2. Use a plain layer without highway to change dimensionality and then continue with stacking highway layers. This is what the paper used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform gate** is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "T(x) &= \\sigma(W_T^T x + b_T) \\\\\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}, x \\in \\mathbb{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$b_T$ can be initialized with a negative number (e.g. -1, -3, etc) such that the network is biased initially towards **carry** behaviour. The paper found that during training, $b_T$ actually got further negative, this behavour suggests that the strong negative biases at low depths are not used to shut down the gates, but to make them more selective. \n",
    "\n",
    "$W_H$ can be initialized with various zero mean distributins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "<a id='densenet'></a>\n",
    "## Densely Connected Convolutional Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bootstrap'></a>\n",
    "# Bootstrap\n",
    "\n",
    "**My current thinking** is that perhaps it’s best to look at multiple measures, large discrepancies would point to something odd. Otherwise, the differences between the methods are mostly minor for practical use. Particularly comparing to other market uncertainties.\n",
    "\n",
    "\n",
    "Efron: **Accelerated Bootstrap intervals (BCa)** has better estimates of confidence intervals with some assumptions. makes certain assumptions, see below\n",
    " \n",
    "[Paper](https://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1032280214)\n",
    "\n",
    "[Slides](https://faculty.washington.edu/heagerty/Courses/b572/public/GregImholte-1.pdf)\n",
    "\n",
    "[Stack Exchange Answer](https://stats.stackexchange.com/questions/19340/bootstrap-based-confidence-interval)\n",
    "\n",
    "## `arch`\n",
    "Python package `arch` has studentized/bias-corrected intervals for bootstrap results.See [doc](http://arch.readthedocs.io/en/latest/bootstrap/confidence-intervals.html#bias-corrected-and-accelerated-bca)\n",
    "\n",
    "Discovered a minor bug in `conf_int(method=’bca’)`, see [issue](https://github.com/bashtage/arch/issues/193)\n",
    "\n",
    "\n",
    "## `scikits-bootstrap`\n",
    "\n",
    "Another implementation, see comments in [code](https://github.com/cgevans/scikits-bootstrap/blob/master/scikits/bootstrap/bootstrap.py)\n",
    "\n",
    "\n",
    "## Block Bootstrap\n",
    "\n",
    "**Politis, White, 2004. Automatic Block-Length Selection for the Dependent Bootstrap**\n",
    "\n",
    "Code in R and Matlab can be found on this [page](http://public.econ.duke.edu/~ap172/) by Prof. Andrew Patton, as well as a correction in 2009 to the original paper.\n",
    "\n",
    "Main points from the paper:\n",
    "\n",
    "**Stationary Bootstrap (SB)** is less accurate than **Circular Block Bootstrap (CB)** for estimating $\\sigma^2_{\\infty}$. Although they have similar bias the SB has higher variance due to the additional randomization involved in drawing the random block size.\n",
    "\n",
    "SB is **less sensitive** to block size mis-specification compared to CB and/or the moving block bootstrap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1707.05589'></a>\n",
    "\n",
    "# On the State of the Art of Evaluation in Neural Language Models\n",
    "\n",
    "[paper](https://arxiv.org/abs/1707.05589)\n",
    "\n",
    "This paper tuned LSTM and Recurrent Highway Networks to beat many more complex models in natural language modeling. A lot of dropout techniques are used here. \n",
    "\n",
    "Instead of comparing models based on the number of hidden units, comparison is done based on **total number of trainable parameters.**\n",
    "\n",
    "Dropout:\n",
    "* input dropout\n",
    "* intra-layer dropout\n",
    "* down-projected outputs / output dropout\n",
    "\n",
    "Things to learn about:\n",
    "\n",
    "**Variational Dropout**: Gal and Ghahramani 2016, [link](https://arxiv.org/abs/1512.05287)\n",
    "\n",
    "**Recurrent Dropout**: Semeniuta et al. 2016. [link](https://arxiv.org/abs/1603.05118)\n",
    "\n",
    "**Using mean-field approximation for dropout at test time**. \n",
    "\n",
    "**Truncated backpropagation**: Training used Adam/batch size 64/truncated backprop performed with 50 time steps.\n",
    "\n",
    "Hyperparameter tuning was performed using a black-box tuner based on **batched GP bandits** (Desautels et al. 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering_ts'></a>\n",
    "## Clustering Financial Time Series: how long is enough?\n",
    "\n",
    "[paper](https://arxiv.org/abs/1603.04017)\n",
    "\n",
    "Looked at Hierarchical Correlation Block Model (HCBM) with single-linkage, complete-linkage, average-linkage, Ward, McQuitty, Median, Centroid algos.\n",
    "\n",
    "Spearman rank correlation is more robust than Pearson correlation when there is:\n",
    "* noise\n",
    "* variables have infinite second moment.\n",
    "\n",
    "Their conclusion was **Ward** method converges faster than others such as single/average-linkage, converges around 250 observations for 256 assets with correlation matrix simiar to Figure 2 in this paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Temporal Convolutional Networks (TCN)\n",
    "\n",
    "[paper](https://arxiv.org/abs/1803.01271), code available on [github](https://github.com/locuslab/TCN). Here is a description of its basic architecure. \n",
    "\n",
    "Two principles: \n",
    "\n",
    "* network produces the same length as the input\n",
    "* there can be no leakage from the future into the past\n",
    "\n",
    "TCN uses **1-D fully-convolution**:\n",
    "\n",
    "* hidden layer is the same length as the input layer.\n",
    "* zero padding of length (k-1) where $k$ is the kernel size, keeping subsequent layers the same size as the previous ones.\n",
    "\n",
    "TCN uses **causal convolution**, meaning an output at time t is only convolved with elements from time t and earlier in the previous layers. \n",
    "\n",
    "**Dilated convolution** is used here, starting with the input layer with dilation $d = 1$, i.e. no dilation and increase dilation **exponentially** with the depth of the network (i.e. $d = \\mathcal{O}(2^i)$ where $i$ is the layer index).\n",
    "\n",
    "Ensures:\n",
    "\n",
    "1. some filter hits each input within the effective history,\n",
    "2. allowing for an extremely large effective history using deep network. \n",
    "\n",
    "**Residual Connection** like ResNet, with weight normalization. Unlike ResNet, the input and output of TCN can have different length, therefore authors use a 1x1 convolution to ensure that **element-wise** addition receives tensors of the same shape (figure b, c below).\n",
    "\n",
    "Authors demostrated TCN has longer memory than LSTM and GRU in 2 tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/tcn.png' width='800'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
