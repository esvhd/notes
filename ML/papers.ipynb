{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Papers\n",
    "\n",
    "[Weight Normalization](#weight_norm)\n",
    "\n",
    "[Highway Networks](#highway)\n",
    "\n",
    "[DenseNet](#densenet)\n",
    "\n",
    "[Bootstrap](#bootstrap)\n",
    "\n",
    "[LSTM/RHN on Natural Language Modeling](#1707.05589)\n",
    "\n",
    "[Clustering Financial Time Series](#clustering_ts)\n",
    "\n",
    "[Temporal Convolutional Networks](#tcn)\n",
    "\n",
    "[Layer Normalization](#layer_norm)\n",
    "\n",
    "[DropConnect](#dropconn)\n",
    "\n",
    "[awd-lstm-lm](#awd-lstm-lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='weight_norm'></a>\n",
    "## Weight Normalization\n",
    "\n",
    "[paper](https://arxiv.org/abs/1602.07868)\n",
    "\n",
    "Speeds up SGD convergence by regularizing the weight norm. For a given network:\n",
    "\n",
    "$$ y = \\phi(w \\cdot x + b) $$\n",
    "\n",
    "where $w \\in \\mathcal{R}^k$, $b$ is a scalar bias, $x \\in \\mathcal{R}^k$, we reparameterize the $w$ as:\n",
    "\n",
    "$$ w = \\frac{g}{\\| v \\|} v $$\n",
    "\n",
    "where $g$ is a scaler, $\\| v \\|$ denotes the Euclidean norm of vector $v$. We now have $\\| w \\| = g$.\n",
    "\n",
    "Forthermore, we can reparameterize $g$ as $g = e^s$, where $s$ is a log-scale parameter to learn by SGD. However, empirically, the authors **did not** find this to be an advantage, and optimization was slightly slower.\n",
    "\n",
    "### Comparison to Batch Norm\n",
    "\n",
    "In special cases weight norm is the same as batch norm, see paper section 2.2 for detail.\n",
    "\n",
    "For CNNs, weight normalization is often much faster computationally, it is also non-stochastic, not affected by batch size. It can be viewed as a cheaper and less noisy approximation to batch norm. Equivalence does not hold for deeper architectures.\n",
    "\n",
    "### Data-Dependent Initialization of Parameters\n",
    "\n",
    "Important to properly initialize our parameters. Authors proposed to sample the elements of $v$ from a simple distribution with fixed scale, such as a normal distribution with zero mean and standard deviation of 0.05.\n",
    "\n",
    "This only works where batch norm is applicable, for RNNs and LSTMs, need to resort to standard initilization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='highway'></a>\n",
    "## Highway Networks\n",
    "\n",
    "[Summary Paper](https://arxiv.org/abs/1505.00387)\n",
    "\n",
    "[Full Paper](https://arxiv.org/abs/1507.06228)\n",
    "\n",
    "\n",
    "Highway networks enables the optimization of the networks with virtually arbitary depth. This is accomplished through the use of a **learned gating machanism** for regulating information flow wihch is inspired by LSTM.\n",
    "\n",
    "The paper shows that the optimization of highway network is virtually independent of depth. Used to train a 900-layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plain feedforward network of $L$ layers with $H$ as a non-linear transform function, ignoring layer index:\n",
    "\n",
    "$$ y = H(x, W_H) $$\n",
    "\n",
    "For a highway network, the paper adds two non-linear transform: \n",
    "* Tranform gate, $T(x W_T)$\n",
    "* Carry gate, $C(x, W_C)$\n",
    "\n",
    "$$ y = H(x, W_H) \\cdot T(x, W_T) + x \\cdot C(x, W_C) $$\n",
    "\n",
    "The paper sets $C = 1 - T$\n",
    "\n",
    "The **dimensionality** of $x$, $y$, $H(x, W_H)$ and $T(x, W_T)$ must be the same for the equation above to hold.\n",
    "\n",
    "If size of the representation needs to be changed, two ways:\n",
    "\n",
    "1. replace $x$ with $\\tilde{x}$ obtained by suitably sub-sampling or zero-padding $x$\n",
    "2. Use a plain layer without highway to change dimensionality and then continue with stacking highway layers. This is what the paper used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform gate** is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "T(x) &= \\sigma(W_T^T x + b_T) \\\\\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}, x \\in \\mathbb{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$b_T$ can be initialized with a negative number (e.g. -1, -3, etc) such that the network is biased initially towards **carry** behaviour. The paper found that during training, $b_T$ actually got further negative, this behavour suggests that the strong negative biases at low depths are not used to shut down the gates, but to make them more selective. \n",
    "\n",
    "$W_H$ can be initialized with various zero mean distributins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "<a id='densenet'></a>\n",
    "## Densely Connected Convolutional Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bootstrap'></a>\n",
    "# Bootstrap\n",
    "\n",
    "**My current thinking** is that perhaps it’s best to look at multiple measures, large discrepancies would point to something odd. Otherwise, the differences between the methods are mostly minor for practical use. Particularly comparing to other market uncertainties.\n",
    "\n",
    "\n",
    "Efron: **Accelerated Bootstrap intervals (BCa)** has better estimates of confidence intervals with some assumptions. makes certain assumptions, see below\n",
    " \n",
    "[Paper](https://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1032280214)\n",
    "\n",
    "[Slides](https://faculty.washington.edu/heagerty/Courses/b572/public/GregImholte-1.pdf)\n",
    "\n",
    "[Stack Exchange Answer](https://stats.stackexchange.com/questions/19340/bootstrap-based-confidence-interval)\n",
    "\n",
    "## `arch`\n",
    "Python package `arch` has studentized/bias-corrected intervals for bootstrap results.See [doc](http://arch.readthedocs.io/en/latest/bootstrap/confidence-intervals.html#bias-corrected-and-accelerated-bca)\n",
    "\n",
    "Discovered a minor bug in `conf_int(method=’bca’)`, see [issue](https://github.com/bashtage/arch/issues/193)\n",
    "\n",
    "\n",
    "## `scikits-bootstrap`\n",
    "\n",
    "Another implementation, see comments in [code](https://github.com/cgevans/scikits-bootstrap/blob/master/scikits/bootstrap/bootstrap.py)\n",
    "\n",
    "\n",
    "## Block Bootstrap\n",
    "\n",
    "**Politis, White, 2004. Automatic Block-Length Selection for the Dependent Bootstrap**\n",
    "\n",
    "Code in R and Matlab can be found on this [page](http://public.econ.duke.edu/~ap172/) by Prof. Andrew Patton, as well as a correction in 2009 to the original paper.\n",
    "\n",
    "Main points from the paper:\n",
    "\n",
    "**Stationary Bootstrap (SB)** is less accurate than **Circular Block Bootstrap (CB)** for estimating $\\sigma^2_{\\infty}$. Although they have similar bias the SB has higher variance due to the additional randomization involved in drawing the random block size.\n",
    "\n",
    "SB is **less sensitive** to block size mis-specification compared to CB and/or the moving block bootstrap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1707.05589'></a>\n",
    "\n",
    "# On the State of the Art of Evaluation in Neural Language Models\n",
    "\n",
    "[paper](https://arxiv.org/abs/1707.05589)\n",
    "\n",
    "This paper tuned LSTM and Recurrent Highway Networks to beat many more complex models in natural language modeling. A lot of dropout techniques are used here. \n",
    "\n",
    "Instead of comparing models based on the number of hidden units, comparison is done based on **total number of trainable parameters.**\n",
    "\n",
    "Dropout:\n",
    "* input dropout\n",
    "* intra-layer dropout\n",
    "* down-projected outputs / output dropout\n",
    "\n",
    "Things to learn about:\n",
    "\n",
    "**Variational Dropout**: Gal and Ghahramani 2016, [link](https://arxiv.org/abs/1512.05287)\n",
    "\n",
    "**Recurrent Dropout**: Semeniuta et al. 2016. [link](https://arxiv.org/abs/1603.05118)\n",
    "\n",
    "**Using mean-field approximation for dropout at test time**. \n",
    "\n",
    "**Truncated backpropagation**: Training used Adam/batch size 64/truncated backprop performed with 50 time steps.\n",
    "\n",
    "Hyperparameter tuning was performed using a black-box tuner based on **batched GP bandits** (Desautels et al. 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering_ts'></a>\n",
    "## Clustering Financial Time Series: how long is enough?\n",
    "\n",
    "[paper](https://arxiv.org/abs/1603.04017)\n",
    "\n",
    "Looked at Hierarchical Correlation Block Model (HCBM) with single-linkage, complete-linkage, average-linkage, Ward, McQuitty, Median, Centroid algos.\n",
    "\n",
    "Spearman rank correlation is more robust than Pearson correlation when there is:\n",
    "* noise\n",
    "* variables have infinite second moment.\n",
    "\n",
    "Their conclusion was **Ward** method converges faster than others such as single/average-linkage, converges around 250 observations for 256 assets with correlation matrix simiar to Figure 2 in this paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='tcn'></a>\n",
    "\n",
    "## Temporal Convolutional Networks (TCN)\n",
    "\n",
    "[paper](https://arxiv.org/abs/1803.01271), code available on [github](https://github.com/locuslab/TCN). ICLR 2018 reviews [here](https://openreview.net/forum?id=rk8wKk-R-). \n",
    "\n",
    "\n",
    "Here is a description of its basic architecure. \n",
    "\n",
    "Two principles: \n",
    "\n",
    "* network produces the same length as the input\n",
    "* there can be no leakage from the future into the past\n",
    "\n",
    "TCN uses **1-D full convolution + causal convolution**, meaning an output at time `t` is only convolved with elements from time `t` and earlier in the previous layers. \n",
    "\n",
    "* Hidden layer is the **same length as the input layer**.\n",
    "* zero padding of length `k-1` where `k` is the kernel size, keeping subsequent layers the same size as the previous ones.\n",
    "\n",
    "In the `tcn.py`, `Chomp1d()` is essentially a layer that chops off the extra padding added to the end of the inputs to ensure **causal convolution**. \n",
    "\n",
    "**Dilated convolution** is used here, starting with the input layer with dilation $d = 1$, i.e. no dilation and increase dilation **exponentially** with the depth of the network (i.e. $d = \\mathcal{O}(2^i)$ where $i$ is the layer index).\n",
    "\n",
    "* **Same padding** size for layer `i`: `padding = (k - 1) * dilation_size`.\n",
    "* **Larger dilation** enables an output at the top level to represent a **wider** range of inputs, thus increase the **receptive field** of the conv net. \n",
    "* 3 ways to **increase** receptive field: \n",
    "    1. larger filter size `k`\n",
    "    2. larger dilation, set `d=2**i` for layer `i`\n",
    "    3. more of layers, in the code the number of layers is set to the number of channels.\n",
    "\n",
    "Ensures:\n",
    "\n",
    "1. some filter hits each input within the effective history,\n",
    "2. allowing for an extremely large effective history using deep network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/tcn.png' width='800'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TemporalBlock` in code\n",
    "\n",
    "**Residual Connection** like ResNet, with weight normalization. Unlike ResNet, the input and output of TCN can have different length, therefore authors use a 1x1 convolution to ensure that **element-wise** addition receives tensors of the same shape (figure b, c above).\n",
    "\n",
    "**Weight normalization** [above](#weight_norm) from [Salimans & Kingma 2016](https://arxiv.org/abs/1602.07868). Pytorch: `torch.nn.utils.weight_norm()` [docs](http://pytorch.org/docs/master/nn.html#weight-norm)\n",
    "\n",
    "**Spatial dropout** from [Srivastava et al 2014](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf). Pytorch: `torch.nn.Dropout2d()` [docs](http://pytorch.org/docs/master/nn.html#torch.nn.Dropout2d)\n",
    "\n",
    "Authors demostrated TCN has longer memory than LSTM and GRU in 2 tasks. \n",
    "\n",
    "#### Construction\n",
    "\n",
    "Input shape is `(N, C, L)`, where:\n",
    "* `N` is number of samples\n",
    "* `C` is number of channels, or number of feature dimensions\n",
    "* `L` is sequence length of all channels/features.\n",
    "\n",
    "For the `Conv1d` layers, **input** and **output** sizes are set to the **number of channels** for input and output. \n",
    "\n",
    "In the paper, padding is done such as input and output sequence lengths are matched.\n",
    "\n",
    "`Chomp1d` applied after `Conv1d` to ensure causal convolution, `chomp_size = padding`.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "Forward flow is as follows: \n",
    "\n",
    "```\n",
    "z = conv1d -> chomp1d(padding) -> relu -> dropout -> conv1d -> chomp1d(padding) -> relu -> dropout\n",
    "\n",
    "if input_channel != output_channel:\n",
    "    # map z to output_channels\n",
    "    r = conv1d(z, input_channels, output_channels, kernel_size=1)\n",
    "    residual = r(z)\n",
    "    out = relu(z + residual)\n",
    "else:\n",
    "    out = z\n",
    "\n",
    "return out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape is (N, in_channels, L)\n",
    "# N: number of samples\n",
    "# in_channels: or number of features\n",
    "# L: feature length\n",
    "x = torch.randn(2, 1, 5)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.2089 -0.2274  0.0779 -0.8448 -0.0247\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.3559 -0.5786 -0.2676 -1.2133  2.0995\n",
       "[torch.FloatTensor of size 2x1x5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m(Variable(x, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.3220  0.2502  0.4168  0.3306  0.6797  0.3565  0.3297\n",
       "  0.2908  0.3632  0.1106  0.3824 -0.0922  0.6123  0.2945\n",
       "  0.0077 -0.2472  0.0814 -0.5939  0.1302 -0.4114 -0.1000\n",
       " -0.1645 -0.4203 -0.2044 -0.4503  0.1377  0.2141 -0.2240\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.3170  0.2012  0.5718  0.4940  0.7666 -0.5190  0.2869\n",
       "  0.2952  0.4129 -0.1018  0.3707 -0.0417  1.6270 -0.5545\n",
       "  0.0768 -0.4474  0.0599 -0.8202  1.0825 -1.0621  0.7218\n",
       " -0.1136 -0.6156 -0.2242 -0.2223  1.2371 -0.6594 -1.3268\n",
       "[torch.FloatTensor of size 2x4x7]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y is padded by 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "(0 ,.,.) = \n",
       " -0.0202 -0.4157 -0.0345\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.3997  0.4084  0.0296\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.3869 -0.2392  0.4698\n",
       "\n",
       "(3 ,.,.) = \n",
       " -0.5192 -0.5013  0.3462\n",
       "[torch.FloatTensor of size 4x1x3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.034489214420318604"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight.data[0,0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       " 0.3292\n",
       " 0.2847\n",
       "-0.0904\n",
       "-0.2368\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3220\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.mul(x[0, 0, :1], m.weight.data[0, 0, 2])) + m.bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3220287561416626"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.data[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chomp1d()` in the TCN code is to make sure that we chop off the padding on the right to ensure causual convolution. \n",
    "\n",
    "See example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chomp = Chomp1d(padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.3220  0.2502  0.4168  0.3306  0.6797\n",
       "  0.2908  0.3632  0.1106  0.3824 -0.0922\n",
       "  0.0077 -0.2472  0.0814 -0.5939  0.1302\n",
       " -0.1645 -0.4203 -0.2044 -0.4503  0.1377\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.3170  0.2012  0.5718  0.4940  0.7666\n",
       "  0.2952  0.4129 -0.1018  0.3707 -0.0417\n",
       "  0.0768 -0.4474  0.0599 -0.8202  1.0825\n",
       " -0.1136 -0.6156 -0.2242 -0.2223  1.2371\n",
       "[torch.FloatTensor of size 2x4x5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last two columns of y chopped off.\n",
    "chomp(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='layer_norm'></a>\n",
    "\n",
    "# Layer Normalization\n",
    "\n",
    "[2016 Paper](https://arxiv.org/abs/1607.06450)\n",
    "\n",
    "## Notations\n",
    "\n",
    "Given the $l^{th}$ hidden layer: \n",
    "\n",
    "* $w^l$ be the weight vector, $w^l_i$ is the wieght for the $i^{th}$ hidden unit.\n",
    "* $h^l$ is the bottom-up input, \n",
    "* $b^l$ is the bias,\n",
    "* let $a^l$ be the vector of the summed inputs to the neuron in that layer:\n",
    "\n",
    "$$ a^l_i = w^l_i h^l $$\n",
    "$$ h^{l+1} = f(a^l_i + b^l_i) $$\n",
    "\n",
    "Where $f()$ is an element-wise non-linear function. \n",
    "\n",
    "For [Batch Norm](./andrew_ng/DeepLearning.ai.ipynb#batchnorm), the normalization is done over the **entire** training batch due to computational reasons. This puts constraints on the size of a minibatch and it is hard to apply to RNN.\n",
    "\n",
    "## Layer Norm\n",
    "\n",
    "Becaue the changes in the output of one layer will tend to cause highly correlated changes in the summed inputs to the next layer, especially with ReLU units whose output can change by a lot, ***Layer norm** statistics are computed over all the hidden units in the same layer as follows:\n",
    "\n",
    "* $H$ number of hidden units in a layer\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu^l &= \\frac{1}{H} \\sum^H_{i=1} a^l_i \\\\\n",
    "\\sigma^l &= \\sqrt{\\frac{1}{H}\\sum^H_{i=1}\\big( a^l_i - \\mu^l\\big)^2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Normalization happens within the hidden layer, all hidden units in the same layer share the same $mu$ and $sigma$, but different training cases have different normalization terms.\n",
    "\n",
    "**Unlike** batch norm, layer norm is:\n",
    "\n",
    "* does not impose any constraint on the size of minibatch\n",
    "* can be used in a pure online regime with batch size 1\n",
    "\n",
    "For RNN, layer norm works as follows\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^t &= W_{hh} h^{t-1} + W_{xh} x^t \\\\\n",
    "h^t &= f \\bigg[ \\frac{g}{\\sigma^t} \\odot \\big( a^t - \\mu^t \\big) + b \\bigg] \\\\\n",
    "\\mu^t &= \\frac{1}{H} \\sum^H_{i=1} a^t_i \\\\\n",
    "\\sigma^t &= \\sqrt{\\frac{1}{H}\\sum^H_{i=1}\\big( a^t_i - \\mu^t\\big)^2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $b$ is bias parameter, same dimension as $h^t$\n",
    "* $g$ is gain parameter, same dimension as $h^t$\n",
    "\n",
    "Layer norm for RNN results in much more stable hidden-to-hidden gradient dynamics (vs. exploding/vanishing gradients).\n",
    "\n",
    "Section 4 of the paper discusses related work including weight norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/layer_norm_comp.png' width=800/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropconn'></a>\n",
    "# DropConnect\n",
    "\n",
    "[Li Wan, et al. 2013](https://cs.nyu.edu/~wanli/dropc/dropc.pdf) and a nice web [page](https://cs.nyu.edu/~wanli/dropc/) that explains it.\n",
    "\n",
    "## Summary \n",
    "\n",
    "Unlike **dropout** which drops output of a neural cell with probability `1-p`, DropConnect drops the **weights of a layer** with probability `1-p`. \n",
    "\n",
    "A Bernoulli mask matrix of the same dimension as the weight matrix is generated for **each** training sample, **not** for a minibatch.\n",
    "\n",
    "Therefore, memory requirement increases as the size of minibatch increases.\n",
    "\n",
    "Inference is based on drawing samples from a Gaussian distribution with mean and variance estimated with the dropout rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "For a network layer:\n",
    "\n",
    "* Input: $v = [ v_1, v_2, \\dots, v_n ]^T$\n",
    "* Weight matrix: $W \\in R^{d \\times n}$, bias terms are included here.\n",
    "* Activation function: `a()`\n",
    "* Output: $r = [r_1, r_2, \\dots, r_d]^T$\n",
    "\n",
    "A simple fully connected layer is:\n",
    "\n",
    "$$ r = a(Wv) $$\n",
    "\n",
    "Dimension here:\n",
    "\n",
    "`(Wv).shape = (d, n) * (n, 1) = (d, 1)`, therefore `r.shape` is `(d, 1)`\n",
    "\n",
    "\n",
    "## Details\n",
    "\n",
    "### Dropout\n",
    "\n",
    "A binary mask **vector** is drawn, $m \\in R^d$, where each element of $m$ is drawn from $m_j \\sim Bernoulli(p)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as ss\n",
    "\n",
    "# d is the dimension of output vector v\n",
    "d = 10\n",
    "m = ss.bernoulli.rvs(.8, size=d)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **forward layer** is then defined as:\n",
    "\n",
    "$$ r = m \\odot a(Wv) $$\n",
    "\n",
    "For many activation functions such as **tanh**, **centered sigmoid**, and **relu**, that have the property of $a(0) = 0$, dropout is applied at the inputs to the activation function, i.e.\n",
    "\n",
    "$$ r = a(m \\odot Wv) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropConnect\n",
    "\n",
    "Similar to dropout as it introduces dynamic sparsity within the model, but the difference is that the sparsity is on the **weights** $W$, rather than on the output vectors.\n",
    "\n",
    "A binary mask **matrix** is drawn, $M \\in R^{d\\times n}$ (same dimension as the weight matrix $W$), where each element of $M$ is drawn from $M_{ij} \\sim Bernoulli(p)$. The output of this layer is given by:\n",
    "\n",
    "$$ r = a ((M \\odot W) v) $$\n",
    "\n",
    "Note that the biases are also included in the masking here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape:  (3, 5) M.shape:  (3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.        , -0.67000094, -0.        ,  2.15240859,  0.78178666],\n",
       "       [ 0.80938994,  0.73258852,  0.        , -1.07328596, -0.11110874],\n",
       "       [ 0.48647332, -0.        ,  0.        , -0.8445224 ,  0.        ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W is the weight matrix\n",
    "W = np.random.randn(3, 5)\n",
    "M = ss.bernoulli.rvs(.8, size=W.shape)\n",
    "\n",
    "print('W.shape: ', W.shape, 'M.shape: ', M.shape)\n",
    "\n",
    "# masking is done by element-wise product\n",
    "M * W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropConnect Training\n",
    "\n",
    "This mask is generated **for each training example independently**. Therefore, a different connection is used for each example seen. This is a **key component of successful training with DropConnect**. The paper states that \"Selecting a single mask for a subset of training examples, such as a mini-batch of 128 examples, **does not** regularize the model enough in practice.\"\n",
    "\n",
    "This means that the memory requirement for $M$ grows as the size of the mini-batch grows. \n",
    "\n",
    "The mask $M$ is applied to the gradient to update only those elements that were active in the forward pass. When passing gradient down, the **masked weight matrix** ($M \\odot W$) is used.\n",
    "\n",
    "<img src='img/dropconnect_training.png' width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropConnect Inference\n",
    "\n",
    "For inference, computing $2^{\\mid M \\mid}$ different masks is **infeasible**.\n",
    "\n",
    "Dropout made the approximation $\\sum_M a((M \\odot M)v) \\approx a(\\sum_M (M \\odot M)v)$, works in practice but not justified mathematically. particularly for **relu**. \n",
    "\n",
    "For DropConnect, Gaussian approximation via moment matching is used. The mean and variance is defined **before activation** as \n",
    "\n",
    "$$u = (M \\odot W) v$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_M [u] &= p W v \\\\\n",
    "V_M [u] &= p(1 - p)(W \\odot W) (v \\odot v) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Samples are drawn from this distribution and then passed through the activation function, before averaging them and passed to the next layer.\n",
    "\n",
    "<img src='img/dropconnect_inference.png' width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='awd-lstm-lm'></a>\n",
    "\n",
    "## Regularizing and Optimizing LSTM Language Models\n",
    "\n",
    "AvSGD Weight-Dropped LSTM (AWD-LSTM)\n",
    "\n",
    "S. Merity, et al. ICLR Feb 2018, [paper](https://openreview.net/forum?id=SyyGPP0TZ)\n",
    "\n",
    "Experiment Data Perplexity\n",
    "* Penn Treebank, 52.8\n",
    "* WikiText-2, 52.0\n",
    "\n",
    "Github [issue](https://github.com/salesforce/awd-lstm-lm/pull/43) to make the code work in `pytorch 0.4`.\n",
    "\n",
    "### RNN Regularization\n",
    "\n",
    "Some techniques cited:\n",
    "\n",
    "* Dropout by Gal & Ghahramani, 2016\n",
    "* Limiting updates to RNN's hidden state, Semeniuta et al. 2016 / Zone-out Krueger et al. 2016\n",
    "* Restrictions on the recurrent matrices\n",
    "* Batch norm / Layer norm - paper argues both introduce additional training parameters and can complicate the training process while increasing the sensitivity of the model.\n",
    "* This paper introduces **Weight-Dropped LSTM**, applying recurrent regularization through a [DropConnect](#dropconn) mask on the **hidden-to-hidden** recurrent weights only. Code: `weight_drop.py`, class `WeightDrop`.\n",
    "\n",
    "Specifically, given the following notation for LSTM, weight-drop with drop connect is applyed only on $[U^i, U^f, U^o, U^c]$.\n",
    "\n",
    "<img src='img/lstm_merity_2017.png' width=200/>\n",
    "\n",
    "Question: how is this done in code? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Regularization Techniques\n",
    "\n",
    "* **Randomized-length BPTT**. BPTT fixed window results in inefficient use of data. (Where is this in the code?) Solution: \n",
    "    *  randomly select the sequence length for the formard and backward pass in two steps. \n",
    "    * Learning rate is also rescaled depending on the length of the resulting sequence compared to the originally specified sequence length. (How)\n",
    "\n",
    "\n",
    "* **Variational dropout** [(Gal & Ghahramani, 2016)](https://arxiv.org/abs/1512.05287), samples a binary dropout mask **only once** upon the first call and then to repeatedly use that **locked** dropout mask for all repeated connections within the forward and backward pass.\n",
    "    * Used here for **all dropout operations other than hidden-to-hidden**. \n",
    "    * specifically, applied to all inputs and outputs of the LSTM.\n",
    "    * A different mask is used for each example within the minibatch. Similar to dropconnect here.\n",
    "\n",
    "\n",
    "* **Embedding dropout** (Gal & Ghahramani, 2016), equivalent to performing dropout on the embedding matrix at word level. \n",
    "    * Since dropout is at the embedding level, it implies that **all** occurrences of a specific word will disappear within that pass.\n",
    "    * Equiavalent to performing variational dropout on the connection between the one-hot embedding and the embedding lookup.\n",
    "\n",
    "\n",
    "* **Weight tying** - shares the weights between the embedding and softmax layer, substantionally reduceing the total number of parameters in the model.\n",
    "\n",
    "\n",
    "\n",
    "* **Independent embedding size and hidden size**\n",
    "    * first and last LSTM layers are mofidied such that the **input of the first layer** and **output of the last layer's dimensionality are equal** to the reduced embedding size.\n",
    "\n",
    "\n",
    "* **Activation regularization (AR)**, applied only to the output of the **final RNN layer**.\n",
    "    * $L_2$ decay used on individual unit actionvations and on the difference in outputs of an RNN at different time steps.\n",
    "    * AR penalizes activations that are significantly larger than 0 as a means of regularizing the network.\n",
    "    * **Defined as**: $\\alpha \\mathrm{L}_2 (m \\odot h_t)$, $m$ is the dropout mask, $\\mathrm{L}_2(\\cdot) = \\|\\cdot\\|_2$, $h_t$ is the output of the RNN at timestep $t$, $\\alpha$ is a scaling coefficient.\n",
    "    * See [here](#ar_tar)\n",
    "\n",
    "\n",
    "* **Temporal activation regularization (TAR)**, applied only to the output of the final RNN layer\n",
    "    * slowness regularizer\n",
    "    * penalize the model from producing large changes in the hidden state.\n",
    "    * **Defined as**: $\\beta \\mathrm{L}_2(h_t - h_{t+1})$, same notation as above, $\\beta$ is a scaling coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Introduces **NT-AvSGD**. A few things that the paper mentioned:\n",
    "\n",
    "* SGD shown to outperform other adaptive algos in word-level language modeling (see paper for citations)\n",
    "* NT-AvSGD based on AvSGD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Code\n",
    "\n",
    "#### `model.py`\n",
    "\n",
    "Model is defined in `model.py` as `RNNModel`. \n",
    "\n",
    "##### Initialization\n",
    "\n",
    "Lots of dropout and other initalization parameters, comments here.\n",
    "\n",
    "```\n",
    "ntoken:\n",
    "    number of embeddings\n",
    "ninp:\n",
    "    embedding dimension\n",
    "nhid:\n",
    "    hidden layer size\n",
    "wdrop: \n",
    "    weight dropout for hidden-to-hidden, passed to WeightDrop()\n",
    "dropout:\n",
    "    torch.nn.Dropout() layer, stored in self.drop (torch.nn.Drouput), \n",
    "    for last rnn layer output\n",
    "dropouth:\n",
    "    torch.nn.Dropout() layer, stored in self.hdrop (torch.nn.Dropout), \n",
    "    for rnn layers except the last layer\n",
    "dropouti\n",
    "    torch.nn.Dropout() layer, stored in self.idrop (torch.nn.Dropout), \n",
    "    for embedding droput?\n",
    "dropoute:\n",
    "    used in embedded_drop() for model input dropout.\n",
    "```\n",
    "\n",
    "Summary of dropout layers used, and their associated dropout rate.\n",
    "\n",
    "```\n",
    "WeightDrop:\n",
    "    w/ wdrop, applied to all RNN layers, (LSTM, GRU)\n",
    "\n",
    "self.lockdrop:\n",
    "    LockedDropout(), used to drop:\n",
    "    1. embeddings, w/ dropoute if training, else 0\n",
    "    2. output of RNNs for all layers except the last one, w/ dropouth\n",
    "    3. output of last RNN layer, w/ dropout\n",
    "    \n",
    "self.idrop:\n",
    "    w/ dropouti. This is actually not used, instead embedding_dropout() is used.\n",
    "    \n",
    "self.hdrop:\n",
    "    w/ dropouth. Not used, self.lockdrop is used instead.\n",
    "    \n",
    "self.drop:\n",
    "    w/ dropout. Not used\n",
    "```\n",
    "\n",
    "### Model Forward Pass\n",
    "\n",
    "Flow of forward pass:\n",
    "\n",
    "1. embedded droput on input, `embedded_drop()` with `dropoute` if in training model, otherwise 0.\n",
    "2. locked dropout on embedding, `self.lockdrop` with `dropouti`\n",
    "3. pass to rnn layers (**weight drop** also happens here)\n",
    "4. For rnn layers except the last one,  apply `self.lockdrop` with `dropouth` (variational dropout)\n",
    "5. for **last** rnn layer output, apply `self.lockdrop` with `dropout` (AR / TAR, see above)\n",
    "\n",
    "A few `nn.Dropout()` layers were created but never used: `idrop`, `hdrop`, `drop`.\n",
    "\n",
    "##### Tied-Weights\n",
    "\n",
    "This is done in `RNNModel.__init__()`.\n",
    "\n",
    "`tie_weights` boolean parameter in `RNNModel` class. If set to `True`, the `hidden_size` (i.e. output size) for **last** RNN (LSTM, GRU) layer is set to `ninp` (here it is the **embedding dimension**), otherwise set to `nhid`.\n",
    "\n",
    "The goal of this is to allow the embedding layer and the final softmax layer to **share weights**. See code snippet below. \n",
    "\n",
    "```\n",
    "self.encoder = torch.nn.Embedding(num_embeddings=ntoken, embedding_dim=ninp)\n",
    "self.decoder = torch.nn.Linear(nhid, ntoken)\n",
    "if tie_weights:\n",
    "    self.decoder.weight = self.encoder.weight\n",
    "```\n",
    "\n",
    "`nhid` is the RNN hidden layer size.\n",
    "\n",
    "[This github issues](https://github.com/salesforce/awd-lstm-lm/issues/48) flagged that `self.decoder` was **never used**. New: the decoder is used in `main.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Independent embedding size and hidden size\n",
    "\n",
    "This is done when initilizing the RNN layers, if `tie_weights==True` then the **first layer's input** and the **last layer's output** are set to `ninp`, which is the same as embedding dimension.\n",
    "\n",
    "The paper mentioned that most previous LSTM languamge models tie the dimensionality of the **word vectors** to the dimensionality of the LSTM's **hidden state**. The **easiest reduction in total parameters** is to reduce the word vector size (for preventing overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `embedded_dropout()` in `embed_regularize.py`\n",
    "\n",
    "Dropout mask is generated similiar to `LockedDropout`, applied to embedding weights (`embed.weight`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LockedDropout` class. \n",
    "\n",
    "A blog [post](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307) mentioned the implementation here.\n",
    "\n",
    "Defined in `loked_dropout.py`. This is the layer that implements RNN **variantional dropout**. \n",
    "\n",
    "I changed the code to adapt `pytorch 0.4`. In this version `torch.Tensor.new()` method is gone from the docs, so I used a different way to create the mask.\n",
    "\n",
    "This layer essentially generates a bernoulli mask with probability `(1 - dropout)`, normalize by divide this mask by `(1 - dropout)`, then applied to inputs: `mask * x`. This is the **inverse dropout** technique mentioned in Andrew Ng's course.\n",
    "\n",
    "Note that when generating the mask, we create a mask with first dimension of 1, then the same dimensions as the input. Then the mask is expanded with `expand_as()` to have the same shape as the input.\n",
    "\n",
    "This is **different** to drawing Bernoulli masks the same shape as the input: it means the **same mask** is applied to all elements along the first dimension of $x$, which is the full sequence output of a LSTM layer, which means the same mask is applied to all timesteps for this layer, achieving variational dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `WeightDrop` class\n",
    "\n",
    "Takes in an RNN layer, extracts `weight_hh_l0` from both `LSTM` and `GRU` for applying dropout.\n",
    "\n",
    "See this [issue](https://github.com/salesforce/awd-lstm-lm/issues/51) I filed on `_setup()`. \n",
    "\n",
    "At the moment there is a hack to disable `RNNBase.flatten_parameters()`, see code comment for this, and [this github issue](https://github.com/salesforce/awd-lstm-lm/issues/7)\n",
    "\n",
    "Weight dropout is applied in `self._setweights()`.\n",
    "\n",
    "If using variational dropout, a new mask is generated each time `self.forward()` is called, and applied to `weight_hh_l0` during training mode. Note that the mask here is also normalized by dividing by `(1 - dropout)`. Training mode is **always** set to `True`. **Why?** Inverse dropout used here, i.e. divide by `(1 - dropout)`.\n",
    "\n",
    "Otherwise, `DropConnect` is used, by simplying dropping some weights in `weight_hh_l0`. \n",
    "\n",
    "In the code `_setupweights()`, `self.training` is referenced and passed to `torch.nn.functional.dropout()`. This variable is inherited from the parent `nn.Module` class and by default set to `True`. \n",
    "\n",
    "When `RNNModel` is set to `train()` or `eval()` mode, the `WeightDrop` layers will also change according. I.e. if `RNNModel` is set to `eval()` mode, its `WeightDrop` layers will also be set to `eval()` mode w/ `self.training` set to False.\n",
    "\n",
    "##### Forward Pass\n",
    "\n",
    "In the forward pass, dropout is applied first to the hidden-to-hidden layer weights of the actual RNN module by calling `self._setweights()`, then the RNN module's `forward()` function is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `main.py`\n",
    "\n",
    "Note how model total parameters are counted here.\n",
    "\n",
    "##### Training\n",
    "\n",
    "1. calls `model.init_hidden(args.batch_size)`, which returns the hidden states of the model.\n",
    "\n",
    "\n",
    "2. iterate through training data\n",
    "\n",
    "\n",
    "3. BPTT length is drawn from one of two normal distributions $\\mathcal{N}(bptt, 5)$ and $\\mathcal{N}(bptt/2, 5)$. Min `seq_len=5`. To efficiently use all of the data. Also here is a [github issue](https://github.com/salesforce/awd-lstm-lm/issues/33) that talked about this, in short, the use of 2 possible normal distributions, is to **ensure different starting point for batches and still maintain efficient use of GPU** (smaller `seq_len` doesn't come too often, e.g. 5% of the time).\n",
    "\n",
    "    ```\n",
    "    # 95% chance that mean of normal distribution is the provided bptt in \n",
    "    # command line args\n",
    "    bptt = args.bptt if np.random.random() < .95 else args.bptt / 2.\n",
    "    \n",
    "    # draw from normal distributino (mean=bptt, stdev=5)\n",
    "    seq_len = max(5, int(np.random.normal(bptt, 5))\n",
    "    \n",
    "    # risk of having a very high seq_len is ignored in production code\n",
    "    # further looking into this, this is mitigated in utils.get_batch()\n",
    "    # where the seq_len is maxed out at len(train_data) - 1 - batch_count\n",
    "    # the line essentially bounds bptt with 2x stdev\n",
    "    # seq_len = min(seq_len, args.bptt + 10)\n",
    "    ```\n",
    "    \n",
    "    \n",
    "4. Adjust learning rate `lr = lr * seq_len / args.bptt`. Scale `lr` based on the ratio of the drawn `seq_len` and input `args.bptt`. Paper states that this is necessary as fixed learning rate **favours shorter sequences** over longer ones. \n",
    "\n",
    "\n",
    "5. batch data returned by `get_batch(train_data, i, arg, seq_len=seq_len)`. From training data, return batches of sizes around `min(seq_len, len(train_data) - 1 - i)`.\n",
    "\n",
    "\n",
    "6. detach hidden state from previous iterations, by calling `repackage_hidden()`, then `optimizer.zero_grad()`\n",
    "\n",
    "\n",
    "7. Computes loss, apply Activation Regularization (AR) and Temporal Activation Regularization (TAR)\n",
    "\n",
    "\n",
    "8. backprop\n",
    "\n",
    "\n",
    "9. `clip_grad_norm_` for all parameters\n",
    "\n",
    "\n",
    "10. `optimizer.step()`\n",
    "\n",
    "\n",
    "11. `batch += 1`, `i += seq_len`\n",
    "\n",
    "\n",
    "#### `SplitCrossEntropyLoss`\n",
    "\n",
    "Calculates an approximate softmax.\n",
    "\n",
    "A few code changes for `pytorch 0.4`\n",
    "\n",
    "* `clip_grad_norm()` to `clip_grad_norm_()` in `main.py` and `finetune.py`\n",
    "* 0-tensor access should use `tensor.item()` in `evaluate()` and `train()`\n",
    "* `torch.nn.functional.log_softmax()` and `torch.nn.functional.softmax()` calls should be used with explicit `dim=-1` (i.e. for 2D tensors this should be dim=1, the dimension represent all classes). See [issue fix here](#https://github.com/pytorch/pytorch/issues/1020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO \n",
    "\n",
    "Run this model with multiple GPUs. see examples [here](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further developemnts on Language Models\n",
    "\n",
    "As of May 2018, a few things claim to have surpassed `awd-lstm-lm`. See claimed results below.\n",
    "\n",
    "Techniques worth looking into:\n",
    "\n",
    "* Dynamic evaluation for sequence models, [paper](https://arxiv.org/abs/1709.07432)\n",
    "* Mixtures of Softmax, [paper](https://arxiv.org/abs/1711.03953)\n",
    "* Noisin, [paper](https://arxiv.org/abs/1805.01500). Improvement here seems very small as the MoS paper perplexity is 47.69...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/penn_treebank_noisin.png' width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ar_tar'></a>\n",
    "## Activation Regularization & Temporal Activation Regularization\n",
    "\n",
    "[paper](https://arxiv.org/abs/1708.01009)\n",
    "\n",
    "The paper finds it is more effective to apply AR to $m \\odot h_t$ than applying it to neurons not updated during the current optimization step ($h_t$).\n",
    "\n",
    "When using AR and TAR together, the authors found that the best results was achieved by decreasing $\\alpha$ and $\\beta$, likely as the model was over-regularized otherwise.\n",
    "\n",
    "The authors also tested a few different values of $\\alpha$ and $\\beta$, from 0 to 9 each, showing that the validation set results were relative insensitive for values larger than or equal to 3. \n",
    "\n",
    "Two dropout layers are used here:\n",
    "\n",
    "* `dp` - dropout rate used on the word vectors and the final RNN output.\n",
    "* `dp_h` - dropout rate used on the eonnections between RNN layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
